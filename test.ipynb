{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import farasa\n",
    "from farasa.segmenter import FarasaSegmenter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Specify the file path\n",
    "file_path = \"./dataset/train.txt\"\n",
    "\n",
    "# Read the contents of the file located at file_path \n",
    "# and append each line to the list data_before_preprocessing\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data_before_preprocessing = file.readlines()\n",
    "    # remove '\\n' from each line\n",
    "    data_before_preprocessing = [line.strip() for line in data_before_preprocessing]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove diacritics\n",
    "def remove_diacritics(text):\n",
    "    text = araby.strip_tashkeel(text)\n",
    "    return text\n",
    "\n",
    "# Remove any non-Arabic letters\n",
    "def remove_non_arabic(text):\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]|،|؛', '', text)\n",
    "    return text\n",
    "\n",
    "def input_preprocessing_text(text):\n",
    "    # Correct most common errors on word like repetetion of harakats, or tanween before alef\n",
    "    text = araby.autocorrect(text)\n",
    "\n",
    "    # Remove any non-Arabic letters\n",
    "    text = remove_non_arabic(text)\n",
    "\n",
    "    # Remove diacritics\n",
    "    text = remove_diacritics(text)\n",
    "\n",
    "    # Tokenize\n",
    "    text = araby.tokenize(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def save_tokenized_input(text):\n",
    "    words = input_preprocessing_text(text)\n",
    "    # Write and append on the tokenized input to a file\n",
    "    with open('./generatedFiles/tokenized_input.txt', 'a', encoding='utf-8') as file:\n",
    "        for word in words:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "def save_gold_output(text):\n",
    "    # Remove any non-Arabic letters and extra spaces\n",
    "    text = remove_non_arabic(text)\n",
    "\n",
    "    # Tokenize\n",
    "    text = araby.tokenize(text)\n",
    "\n",
    "    # Write and append on the gold output to a file\n",
    "    with open('./generatedFiles/gold_output.txt', 'a', encoding='utf-8') as file:\n",
    "        for word in text:\n",
    "            # if last word in the text don't add '\\n'\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "# Extract diacritics by returning a list containing a tuple of 3 elements: (letter, tashkeel, shadda)\n",
    "def extract_arabic_diacritics(tokenized_text):\n",
    "    diacritics_list = []\n",
    "    for word in tokenized_text:\n",
    "        extracted_word, tashkeel, shadda = araby.separate(word, extract_shadda=True)\n",
    "        for i in range(len(extracted_word)):\n",
    "            diacritics_list.append((extracted_word[i], araby.name(tashkeel[i]), araby.name(shadda[i])))\n",
    "    return diacritics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN ONE TIME ONLY THIS CODE AGAIN \n",
    "# # Generate Gold Input file\n",
    "# for i in range(len(data_before_preprocessing)):\n",
    "#     save_tokenized_input(data_before_preprocessing[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RUN ONE TIME ONLY THIS CODE AGAIN\n",
    "# # Generate Gold Output file\n",
    "# for i in range(len(data_before_preprocessing)):\n",
    "#     test = data_before_preprocessing[i]\n",
    "#     text1 = save_gold_output(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قال', 'ابن', 'القاسم', 'قال', 'مالك', 'في', 'مكي', 'أحرم', 'بحجة', 'من', 'الحرم', 'ثم', 'أحصر', 'أنه', 'يخرج', 'إلى', 'الحل', 'فيلبي', 'من', 'هناك', 'لأنه', 'أمر', 'من', 'فاته', 'الحج', 'وقد', 'أحرم', 'من', 'مكة', 'أن', 'يخرج', 'إلى', 'الحل', 'فيعمل', 'فيما', 'بقي', 'عليه', 'ما', 'يعمل', 'المعتمر', 'ويحل']\n",
      "[('ق', 'فتحة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ل', 'فتحة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ب', 'سكون', 'تطويل'), ('ن', 'ضمة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ل', 'سكون', 'تطويل'), ('ق', 'فتحة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('س', 'كسرة', 'تطويل'), ('م', 'كسرة', 'تطويل'), ('ق', 'فتحة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ل', 'فتحة', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ل', 'كسرة', 'تطويل'), ('ك', 'ضمتان', 'تطويل'), ('ف', 'كسرة', 'تطويل'), ('ي', 'تطويل', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ك', 'سكون', 'شدة'), ('ي', 'كسرة', 'شدة'), ('أ', 'فتحة', 'تطويل'), ('ح', 'سكون', 'تطويل'), ('ر', 'فتحة', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ب', 'كسرة', 'تطويل'), ('ح', 'فتحة', 'تطويل'), ('ج', 'سكون', 'شدة'), ('ة', 'فتحة', 'تطويل'), ('م', 'كسرة', 'تطويل'), ('ن', 'سكون', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ل', 'سكون', 'تطويل'), ('ح', 'فتحة', 'تطويل'), ('ر', 'فتحة', 'تطويل'), ('م', 'كسرة', 'تطويل'), ('ث', 'ضمة', 'تطويل'), ('م', 'سكون', 'شدة'), ('أ', 'ضمة', 'تطويل'), ('ح', 'سكون', 'تطويل'), ('ص', 'كسرة', 'تطويل'), ('ر', 'فتحة', 'تطويل'), ('أ', 'فتحة', 'تطويل'), ('ن', 'سكون', 'شدة'), ('ه', 'فتحة', 'تطويل'), ('ي', 'فتحة', 'تطويل'), ('خ', 'سكون', 'تطويل'), ('ر', 'ضمة', 'تطويل'), ('ج', 'ضمة', 'تطويل'), ('إ', 'تطويل', 'تطويل'), ('ل', 'فتحة', 'تطويل'), ('ى', 'تطويل', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ل', 'سكون', 'تطويل'), ('ح', 'كسرة', 'تطويل'), ('ل', 'سكون', 'شدة'), ('ف', 'فتحة', 'تطويل'), ('ي', 'ضمة', 'تطويل'), ('ل', 'فتحة', 'تطويل'), ('ب', 'سكون', 'شدة'), ('ي', 'كسرة', 'تطويل'), ('م', 'كسرة', 'تطويل'), ('ن', 'سكون', 'تطويل'), ('ه', 'ضمة', 'تطويل'), ('ن', 'فتحة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ك', 'فتحة', 'تطويل'), ('ل', 'كسرة', 'تطويل'), ('أ', 'فتحة', 'تطويل'), ('ن', 'سكون', 'شدة'), ('ه', 'فتحة', 'تطويل'), ('أ', 'فتحة', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ر', 'فتحة', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ن', 'سكون', 'تطويل'), ('ف', 'فتحة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ت', 'فتحة', 'تطويل'), ('ه', 'ضمة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ل', 'سكون', 'تطويل'), ('ح', 'فتحة', 'تطويل'), ('ج', 'سكون', 'شدة'), ('و', 'فتحة', 'تطويل'), ('ق', 'فتحة', 'تطويل'), ('د', 'سكون', 'تطويل'), ('أ', 'فتحة', 'تطويل'), ('ح', 'سكون', 'تطويل'), ('ر', 'فتحة', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('م', 'كسرة', 'تطويل'), ('ن', 'سكون', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ك', 'سكون', 'شدة'), ('ة', 'فتحة', 'تطويل'), ('أ', 'فتحة', 'تطويل'), ('ن', 'سكون', 'تطويل'), ('ي', 'فتحة', 'تطويل'), ('خ', 'سكون', 'تطويل'), ('ر', 'ضمة', 'تطويل'), ('ج', 'فتحة', 'تطويل'), ('إ', 'تطويل', 'تطويل'), ('ل', 'فتحة', 'تطويل'), ('ى', 'تطويل', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ل', 'سكون', 'تطويل'), ('ح', 'كسرة', 'تطويل'), ('ل', 'سكون', 'شدة'), ('ف', 'فتحة', 'تطويل'), ('ي', 'فتحة', 'تطويل'), ('ع', 'سكون', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ل', 'فتحة', 'تطويل'), ('ف', 'كسرة', 'تطويل'), ('ي', 'تطويل', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ب', 'فتحة', 'تطويل'), ('ق', 'كسرة', 'تطويل'), ('ي', 'فتحة', 'تطويل'), ('ع', 'فتحة', 'تطويل'), ('ل', 'فتحة', 'تطويل'), ('ي', 'سكون', 'تطويل'), ('ه', 'كسرة', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ي', 'فتحة', 'تطويل'), ('ع', 'سكون', 'تطويل'), ('م', 'فتحة', 'تطويل'), ('ل', 'ضمة', 'تطويل'), ('ا', 'تطويل', 'تطويل'), ('ل', 'سكون', 'تطويل'), ('م', 'ضمة', 'تطويل'), ('ع', 'سكون', 'تطويل'), ('ت', 'فتحة', 'تطويل'), ('م', 'كسرة', 'تطويل'), ('ر', 'ضمة', 'تطويل'), ('و', 'فتحة', 'تطويل'), ('ي', 'ضمة', 'تطويل'), ('ح', 'كسرة', 'تطويل'), ('ل', 'سكون', 'شدة')]\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "\n",
    "test = \"قَالَ ابْنُ الْقَاسِمِ : قَالَ مَالِكٌ فِي مَكِّيٍّ أَحْرَمَ بِحَجَّةٍ مِنْ الْحَرَمِ ثُمَّ أُحْصِرَ ، أَنَّهُ يَخْرُجُ إلَى الْحِلِّ فَيُلَبِّي مِنْ هُنَاكَ لِأَنَّهُ أَمَرَ مَنْ فَاتَهُ الْحَجُّ وَقَدْ أَحْرَمَ مِنْ مَكَّةَ ، أَنْ يَخْرُجَ إلَى الْحِلِّ فَيَعْمَلَ فِيمَا بَقِيَ عَلَيْهِ مَا يَعْمَلُ الْمُعْتَمِرُ وَيُحِلُّ .( 2 / 437 ) \"\n",
    "text2 = input_preprocessing_text(test)\n",
    "print(text2)\n",
    "text3 = remove_non_arabic(test)\n",
    "text3 = araby.tokenize(text3)\n",
    "diacritics_list = extract_arabic_diacritics(text3)\n",
    "print(diacritics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important functions in PyArabic\n",
    "\n",
    "# araby.tokenize(text) # Tokenize the sentence text into words\n",
    "# araby.is_arabicrange(text) # Check if the text is Arabic\n",
    "# araby.sentence_tokenize(text) # Tokenize the text into sentences\n",
    "# araby.strip_tashkeel(text) # Remove diacritics (FATHA, DAMMA, KASRA, SUKUN, SHADDA, FATHATAN, DAMMATAN, KASRATAN)\n",
    "# araby.strip_diacritics(text) # Remove diacritics (Small Alef الألف الخنجرية, Harakat + Shadda, Quranic marks)\n",
    "# araby.strip_tatweel(text) # Remove tatweel\n",
    "# araby.strip_shadda(text) # Remove shadda\n",
    "# araby.autocorrect(text) # Correct most common errors on word like repetetion of harakats,or tanwin befor alef\n",
    "# araby.arabicrange() # Return a list of Arabic characters\n",
    "\n",
    "# New Functions in PyArabic\n",
    "# araby.vocalized_similarity(word1, word2) # if the two words has the same letters and the same harakats, this function return True. \n",
    "# The two words can be full vocalized, or partial vocalized\n",
    "\n",
    "# araby.vocalizedlike(word1, word2) Same as vocalized_similarity but return True and False\n",
    "\n",
    "# araby.joint(word1, word2) # joint the letters with the marks the length ot letters and marks must be equal return word\n",
    "\n",
    "\n",
    "\n",
    "# Return the text, its tashkeel and shadda if extract_shadda is True\n",
    "# text, marks, shada = araby.separate(text,extract_shadda=True) # Separate diacritics from the text\n",
    "# print (text)\n",
    "# for m in marks:\n",
    "#     print (araby.name(m))\n",
    "\n",
    "# for s in shada:\n",
    "#     print (araby.name(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n"
     ]
    }
   ],
   "source": [
    "# read the tokenized input file\n",
    "with open('./generatedFiles/tokenized_input.txt', 'r', encoding='utf-8') as file:\n",
    "    tokenized_input = file.readlines()\n",
    "    print(len(tokenized_input))\n",
    "    # remove '\\n' from each line\n",
    "    tokenized_input = [line.strip() for line in tokenized_input]\n",
    "    # put in tokenized_input list the tokenized input of length 1\n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "    # get the inde\n",
    "\n",
    "print(tokenized_input[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Core Word (CW) Diacritization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Characters: here we extract each character from all tokenized words and create a vector of size 50 for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_char = Tokenizer(char_level=True)\n",
    "tokenizer_char.fit_on_texts(tokenized_input)\n",
    "sequences_char = tokenizer_char.texts_to_sequences(tokenized_input)\n",
    "char_features = pad_sequences(sequences_char)   # padding the sequences to have the same length as the longest sequence (word)\n",
    "char_embeddings = np.random.rand(len(tokenizer_char.word_index) + 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2104308, 13)\n",
      "(38, 50)\n"
     ]
    }
   ],
   "source": [
    "print(char_features.shape) # (number of words, max length of word in the dataset)\n",
    "\n",
    "\n",
    "print(char_embeddings.shape)\n",
    "\n",
    "# 38 rows: 37 unique characters identified by the tokenizer, 1 row for handling characters not seen in the training data\n",
    "# 50 columns: Each character is encoded as a 50-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0 13  5  1  7]\n"
     ]
    }
   ],
   "source": [
    "print(char_features[0]) \n",
    "# the number of non zero elements corresponds to the length of the word \n",
    "# and the value of each element corresponds to the index of the character in the tokenizer\n",
    "# which means that every character now is encoded as a number and this number is the index of the character in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      " 0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      " 0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      " 0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      " 0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      " 0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      " 0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      " 0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      " 0.4219707  0.02121006]\n"
     ]
    }
   ],
   "source": [
    "print(char_embeddings[0])\n",
    "# this is the embedding of each character in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 50)\n",
      "[[0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      "  0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      "  0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      "  0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      "  0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      "  0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      "  0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      "  0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      "  0.4219707  0.02121006]\n",
      " [0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      "  0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      "  0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      "  0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      "  0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      "  0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      "  0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      "  0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      "  0.4219707  0.02121006]\n",
      " [0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      "  0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      "  0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      "  0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      "  0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      "  0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      "  0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      "  0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      "  0.4219707  0.02121006]\n",
      " [0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      "  0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      "  0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      "  0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      "  0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      "  0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      "  0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      "  0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      "  0.4219707  0.02121006]\n",
      " [0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      "  0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      "  0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      "  0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      "  0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      "  0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      "  0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      "  0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      "  0.4219707  0.02121006]\n",
      " [0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      "  0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      "  0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      "  0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      "  0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      "  0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      "  0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      "  0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      "  0.4219707  0.02121006]\n",
      " [0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      "  0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      "  0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      "  0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      "  0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      "  0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      "  0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      "  0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      "  0.4219707  0.02121006]\n",
      " [0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      "  0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      "  0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      "  0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      "  0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      "  0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      "  0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      "  0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      "  0.4219707  0.02121006]\n",
      " [0.98932928 0.643462   0.77532937 0.1227929  0.73089188 0.21288858\n",
      "  0.99034029 0.15313401 0.44557446 0.12192174 0.44356161 0.12061557\n",
      "  0.8478695  0.76412183 0.7398288  0.04048527 0.08625787 0.03671214\n",
      "  0.3239838  0.75342392 0.92016011 0.11164682 0.36365469 0.55828169\n",
      "  0.47184493 0.41743149 0.58200443 0.81894359 0.52881037 0.70788089\n",
      "  0.74585946 0.9119129  0.72889559 0.41261597 0.81483764 0.03791911\n",
      "  0.7136551  0.4430647  0.53942118 0.49148931 0.47247786 0.39238231\n",
      "  0.49273873 0.51496099 0.08505062 0.17991849 0.14699755 0.07638961\n",
      "  0.4219707  0.02121006]\n",
      " [0.69596443 0.46327086 0.04989315 0.57319556 0.46321828 0.86333981\n",
      "  0.43915603 0.52755565 0.84240889 0.08997341 0.28604612 0.2557372\n",
      "  0.2274677  0.39363135 0.99414667 0.07585322 0.92199385 0.04373313\n",
      "  0.70990919 0.4360616  0.48644933 0.5592223  0.13257888 0.41911949\n",
      "  0.93462526 0.04008053 0.78447575 0.60025567 0.73424243 0.8009054\n",
      "  0.76289675 0.75943543 0.20147904 0.03967676 0.01061337 0.54743578\n",
      "  0.01343758 0.914436   0.17061287 0.37194049 0.98512124 0.51581151\n",
      "  0.79665558 0.40242291 0.98111754 0.81320089 0.25234302 0.62143175\n",
      "  0.37036881 0.76360891]\n",
      " [0.75828487 0.18725984 0.77544523 0.96665039 0.26634178 0.04673267\n",
      "  0.2035453  0.10443905 0.90281793 0.97386434 0.60966731 0.18398623\n",
      "  0.66316616 0.98211016 0.25651331 0.40446289 0.5397675  0.27763438\n",
      "  0.15194684 0.05008994 0.0968067  0.35993132 0.48179914 0.74931552\n",
      "  0.94449436 0.60193978 0.97742258 0.56877287 0.02786724 0.58177\n",
      "  0.87367533 0.33755043 0.87581992 0.44299588 0.11532714 0.16113008\n",
      "  0.68916134 0.76445859 0.31706133 0.28058034 0.0924761  0.55233655\n",
      "  0.57335522 0.18306024 0.47658718 0.97920136 0.88973796 0.28852299\n",
      "  0.40878621 0.83318453]\n",
      " [0.27084663 0.82418806 0.20869779 0.52130615 0.34751411 0.69064329\n",
      "  0.7611207  0.88231269 0.88763604 0.51683382 0.35530201 0.21827887\n",
      "  0.3081564  0.06606289 0.99537605 0.34877159 0.13117483 0.98930072\n",
      "  0.2887127  0.12137867 0.9527321  0.80668942 0.55865011 0.64360559\n",
      "  0.20960731 0.78839936 0.50473106 0.14450542 0.63885848 0.24883165\n",
      "  0.46513788 0.20067816 0.86475882 0.14354683 0.20344593 0.86684683\n",
      "  0.20121387 0.02867965 0.84756443 0.12376093 0.56876083 0.90673881\n",
      "  0.81987158 0.24028892 0.32697312 0.33136911 0.07246366 0.59771462\n",
      "  0.5268663  0.44060051]\n",
      " [0.15002946 0.56607117 0.29317611 0.08474136 0.03575483 0.74793715\n",
      "  0.93810805 0.31711085 0.69334481 0.51652757 0.21226474 0.4637196\n",
      "  0.40192919 0.25837979 0.49822805 0.34552077 0.32692806 0.82205729\n",
      "  0.2921887  0.0835154  0.25224465 0.50645422 0.99296524 0.27057298\n",
      "  0.61986517 0.75244655 0.41246832 0.49909171 0.78946029 0.61000667\n",
      "  0.74985498 0.09222884 0.88896973 0.90990745 0.44728063 0.5245421\n",
      "  0.56072625 0.63487387 0.88830762 0.23410441 0.00666079 0.6167816\n",
      "  0.88848687 0.65838866 0.09098265 0.8240682  0.72398586 0.20263552\n",
      "  0.26554221 0.66796996]]\n"
     ]
    }
   ],
   "source": [
    "print(char_embeddings[char_features[0]].shape)\n",
    "# 13 is the word of characters and 50 is the embedding size of each character\n",
    "\n",
    "print(char_embeddings[char_features[0]])\n",
    "# this is the embedding of each character in the first tokenized word, this is the 1st feature and the input of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - The position of the character in a word segment. For example, given the word “wAlktAb” , which is composed of three segments “w+Al+ktAb”. Letters were marked as “B” if they begin a segment, “M” if they are in the middle of a segment, “E” if they end a segment, and “S” if they are single letter segments. So for “w+Al+ktAb”, the corresponding character positions are “S+BE+BMME.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'farasa-api.qcri.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241M/241M [05:22<00:00, 749kiB/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-26 12:58:39,880 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "segmenter = FarasaSegmenter(interactive=True) # The default behaviour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented word: ['ك', 'قلم', 'ه']\n",
      "SEG tags: ['S', 'B', 'M', 'E', 'S']\n"
     ]
    }
   ],
   "source": [
    "def get_seg_tags(word):                 # word = \"wAlktAb\"\n",
    "    segments = segmenter.segment(word)  # segments will be a list: [\"w\", \"Al\", \"ktAb\"]\n",
    "    segments = segments.split('+')\n",
    "    seg_tags = []\n",
    "    for segment in segments:\n",
    "        if len(segment) == 1:\n",
    "            seg_tags.append(\"S\")\n",
    "        else:\n",
    "            seg_tags.append(\"B\")  # First letter\n",
    "            seg_tags.extend(\"M\" * (len(segment) - 2))  # Middle letters\n",
    "            seg_tags.append(\"E\")  # Last letter\n",
    "    return segments, seg_tags\n",
    "\n",
    "word = \"كقلمه\"\n",
    "segments, seg_tags = get_seg_tags(word)\n",
    "print(\"Segmented word:\", segments)\n",
    "print(\"SEG tags:\", seg_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DON'T RUN THIS CODE AGAIN, THIS CELL TOOK 25m 4.5s TO RUN\n",
    "# # The Output of this code is the input_segments.txt file\n",
    "\n",
    "# for i in range(len(tokenized_input)):\n",
    "#     segments, seg_tags = get_seg_tags(tokenized_input[i])\n",
    "#     # Write and append on the tokenized input to a file\n",
    "#     with open('./generatedFiles/input_segments.txt', 'a', encoding='utf-8') as file:\n",
    "#         for tag in seg_tags:\n",
    "#             file.write(tag)\n",
    "#         file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['BMES', 'BE', 'BME', 'BEBME', 'BES', 'BME', 'BME', 'BEBMMME', 'BME', 'BMES']\n"
     ]
    }
   ],
   "source": [
    "# read the input_segments file\n",
    "with open('./generatedFiles/input_segments.txt', 'r', encoding='utf-8') as file:\n",
    "    input_segments = file.readlines()\n",
    "    print(len(input_segments))\n",
    "    # remove '\\n' from each line\n",
    "    input_segments = [line.strip() for line in input_segments]\n",
    "    # put in tokenized_input list the tokenized input of length 1\n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "    # get the inde\n",
    "\n",
    "print(input_segments[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tags = Tokenizer(char_level=True)\n",
    "tokenizer_tags.fit_on_texts(input_segments)\n",
    "sequences_tags = tokenizer_tags.texts_to_sequences(input_segments)\n",
    "tags_features = pad_sequences(sequences_tags)   \n",
    "tags_embeddings = np.random.rand(len(tokenizer_tags.word_index) + 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2104308, 13)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "print(tags_features.shape) \n",
    "print(tags_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - PRIOR: diacritics seen in the training set per segment. Since we used a character-level model, this feature informed the model with word-level information. For example, the word “ktAb”  was observed to have two diacritized forms in the training set, namely “kitaAb” ( – book) and “kut∼aAb” ( – writers). The first letter in the word (“k”) accepted the diacritics “i” and “u.” Thus, given a binary vector representing whether a character is allowed to assume any of the eight primitive Arabic diacritic marks (a, i, u, o, K, N, F, and ∼ in order), the first letter would be given the following vector “01100000.” If a word segment was never observed during training, then the vector for all letters therein would be set to 11111111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قَوْلُهُ', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', 'قَالَ', 'الزَّرْكَشِيُّ', 'ابْنُ', 'عَرَفَةَ']\n"
     ]
    }
   ],
   "source": [
    "# read the gold_output file\n",
    "with open('./generatedFiles/gold_output.txt', 'r', encoding='utf-8') as file:\n",
    "    gold_output = file.readlines()\n",
    "    print(len(gold_output))\n",
    "    # remove '\\n' from each line\n",
    "    gold_output = [line.strip() for line in gold_output]\n",
    "    # put in tokenized_input list the tokenized input of length 1\n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "    # get the inde\n",
    "\n",
    "print(gold_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قَوْلُهُ', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', 'قَالَ', 'الزَّرْكَشِيُّ', 'ابْنُ', 'عَرَفَةَ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(gold_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_input))\n",
    "print(tokenized_input[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each diacrtics to its unicode\n",
    "diacritics_mapping = {\n",
    "    'FATHA': '\\u064E',\n",
    "    'DAMMA': '\\u064F',\n",
    "    'KASRA': '\\u0650',\n",
    "    'SUKUN': '\\u0652',\n",
    "    'SHADDA': '\\u0651',\n",
    "    'FATHATAN': '\\u064B',\n",
    "    'DAMMATAN': '\\u064C',\n",
    "    'KASRATAN': '\\u064D',\n",
    "    'TANWEEN': '\\u0640'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstly, initialize an empty dictionary for the 'prior' feature\n",
    "# the value will be the 8 arabic marks (FATHA, DAMMA, KASRA, SUKUN, SHADDA with FATHA, SHADDA with DAMMA, SHADDA with KASRA, NULL) as a binary vector\n",
    "\n",
    "# then, loop over the tokenized input and check if the each character and word pair is not in the dictionary, get the indices of this word and its duplicates in the tokenized input array\n",
    "def get_prior(tokenized_input, gold_output):\n",
    "    prior = {} # this dictionary will hold a key of tuple of 2 elements (word, character) and \n",
    "    for i in range(len(tokenized_input)):\n",
    "        if (tokenized_input[i], tokenized_input[i][0], 0) not in prior:\n",
    "            # get the indices of the word in the tokenized input array\n",
    "            indices = [j for j, x in enumerate(tokenized_input) if x == tokenized_input[i]]\n",
    "            print(indices)\n",
    "            # get the words in the gold_output array with the same indices\n",
    "            words = [gold_output[j] for j in indices]\n",
    "            for indx, charac in enumerate(tokenized_input[i]):\n",
    "                for word in words:\n",
    "                    # extract the diacritics of word[indx]\n",
    "                    prior[(tokenized_input[i], charac, indx)] = [0, 0, 0, 0, 0, 0, 0, 0] # initialize the value of the key with zeros\n",
    "                    if diacritics_mapping['SHADDA'] in word[indx]:\n",
    "                        prior[(tokenized_input[i], charac, indx)][5] = 1 if diacritics_mapping['DAMMA'] in word[indx: indx+2] else 0\n",
    "                        prior[(tokenized_input[i], charac, indx)][4] = 1 if diacritics_mapping['FATHA'] in word[indx: indx+2] else 0\n",
    "                        prior[(tokenized_input[i], charac, indx)][6] = 1 if diacritics_mapping['KASRA'] in word[indx: indx+2] else 0\n",
    "                        prior[(tokenized_input[i], charac, indx)][7] = 1 if not  diacritics_mapping['FATHA'] in word[indx: indx+2] and not diacritics_mapping['DAMMA'] in word[indx: indx+2]  and not diacritics_mapping['KASRA'] in word[indx: indx+2] else 0\n",
    "                    else:\n",
    "                        prior[(tokenized_input[i], charac,indx)][0] = 1 if diacritics_mapping['FATHA'] in word[indx: indx+2] else 0\n",
    "                        prior[(tokenized_input[i], charac,indx)][1] = 1 if diacritics_mapping['DAMMA'] in word[indx: indx+2] else 0\n",
    "                        prior[(tokenized_input[i], charac,indx)][2] = 1 if diacritics_mapping['KASRA'] in word[indx: indx+2] else 0\n",
    "                        prior[(tokenized_input[i], charac,indx)][3] = 1 if diacritics_mapping['SUKUN'] in word[indx: indx+2] else 0\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[2, 3]\n",
      "[4, 5]\n",
      "{('كإنكار', 'ك', 0): [1, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'إ', 1): [1, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'ن', 2): [0, 0, 1, 0, 0, 0, 0, 0], ('كإنكار', 'ك', 3): [0, 0, 1, 0, 0, 0, 0, 0], ('كإنكار', 'ا', 4): [0, 0, 0, 1, 0, 0, 0, 0], ('كإنكار', 'ر', 5): [0, 0, 0, 1, 0, 0, 0, 0], ('بقذر', 'ب', 0): [0, 0, 1, 0, 0, 0, 0, 0], ('بقذر', 'ق', 1): [0, 0, 1, 0, 0, 0, 0, 0], ('بقذر', 'ذ', 2): [1, 0, 0, 0, 0, 0, 0, 0], ('بقذر', 'ر', 3): [1, 0, 0, 0, 0, 0, 0, 0], ('أكثر', 'أ', 0): [1, 0, 0, 0, 0, 0, 0, 0], ('أكثر', 'ك', 1): [1, 0, 0, 0, 0, 0, 0, 0], ('أكثر', 'ث', 2): [0, 0, 0, 1, 0, 0, 0, 0], ('أكثر', 'ر', 3): [0, 0, 0, 1, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "test_tokenized_input = ['كإنكار', 'كإنكار', 'بقذر','بقذر', 'أكثر', 'أكثر']\n",
    "test_gold_output = ['كَإِنْكَارِ','كَإِنْكَارٍ', 'بِقَذَر', 'بِقَذَرٍ','أكْثَرَ', 'أَكْثَرُ']\n",
    "print (get_prior(test_tokenized_input, test_gold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نْ\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "letter, tashkeel, shadda = araby.separate('زَّ', extract_shadda=True)\n",
    "enkar = 'كَإِنْكَارِ'\n",
    "print(enkar[4:6])\n",
    "print( diacritics_mapping['FATHA'] in enkar[0:1])\n",
    "print( diacritics_mapping['SHADDA'] in 'زَّ')\n",
    "print( diacritics_mapping['DAMMA'] in 'زَّ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
