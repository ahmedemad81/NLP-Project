{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVqrsVWh0kiC"
   },
   "source": [
    "# Named Entity Recognition Assignment\n",
    "NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. In this assignment, you will train a named entity recognition system and test it on a test data. \\\n",
    "Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WR6a6DkN0d-3"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils import get_params, get_vocab\n",
    "import random as rnd\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab, tag_map, sentences_file, labels_file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(sentences_file, encoding=\"utf-8\") as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each token by its index if it is in vocab\n",
    "            # else use index of UNK_WORD\n",
    "            s = [vocab[token] if token in vocab \n",
    "                 else vocab['UNK']\n",
    "                 for token in sentence.split(' ')]\n",
    "            sentences.append(s)\n",
    "\n",
    "    with open(labels_file, encoding=\"utf-8\") as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each label by its index\n",
    "            l = [tag_map[label] for label in sentence.split(' ')] # I added plus 1 here\n",
    "            labels.append(l) \n",
    "    return sentences, labels, len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_vocab(vocab_path, tags_path):\n",
    "#     vocab = {}\n",
    "#     with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "#         for i, l in enumerate(f.read().splitlines()):\n",
    "#             vocab[l] = i  # to avoid the 0\n",
    "#         # loading tags (we require this to map tags to their indices)\n",
    "#     vocab['<PAD>'] = len(vocab) # 35180\n",
    "#     tag_map = {}\n",
    "#     with open(tags_path, encoding=\"utf-8\") as f:\n",
    "#         for i, t in enumerate(f.read().splitlines()):\n",
    "#             tag_map[t] = i \n",
    "    \n",
    "#     return vocab, tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the contents of the pickle file located at file_path \n",
    "    and append each line to the list data\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "diacritics = set(read_pickle_file(\"../Delivery/diacritics.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_letters = set(read_pickle_file(\"../Delivery/arabic_letters.pickle\"))\n",
    "\n",
    "# save this arabic_letters as a dict with index starting from 0\n",
    "vocab = dict()\n",
    "for i, char in enumerate(arabic_letters):\n",
    "    vocab[i] = char\n",
    "# vocab[36] = '<PAD>'\n",
    "vocab[36] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'أ': 0, 'ص': 1, 'ا': 2, 'ى': 3, 'ذ': 4, 'ف': 5, 'ظ': 6, 'ؤ': 7, 'ت': 8, 'ء': 9, 'ه': 10, 'ي': 11, 'ز': 12, 'ط': 13, 'ل': 14, 'إ': 15, 'د': 16, 'ئ': 17, 'و': 18, 'ج': 19, 'ن': 20, 'ح': 21, 'ر': 22, 'س': 23, 'ش': 24, 'ك': 25, 'ع': 26, 'خ': 27, 'ق': 28, 'غ': 29, 'آ': 30, 'م': 31, 'ب': 32, 'ة': 33, 'ث': 34, 'ض': 35, ' ': 36}\n"
     ]
    }
   ],
   "source": [
    "reverse_vocab = dict()\n",
    "for key, value in vocab.items():\n",
    "    reverse_vocab[value] = key\n",
    "\n",
    "print(reverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tokenized_input and then append to tokenized_input_char as list of lists for each line, each list contains the characters of the line encoded as the dict vocab\n",
    "def read_tokenized_input(reverse_vocab, input_file):\n",
    "    with open(input_file, 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "    tokenized_input_char = []\n",
    "    for line in lines:\n",
    "        line = list(line)\n",
    "        line = [reverse_vocab[char] for char in line]\n",
    "        tokenized_input_char.append(line)\n",
    "    return tokenized_input_char\n",
    "\n",
    "tokenized_input_char = read_tokenized_input(reverse_vocab, '../generatedFiles/training/new_input_sentence.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 18, 14, 10, 36, 0, 18, 36, 28, 13, 26, 36, 2, 14, 0, 18, 14, 36, 11, 16, 10, 36, 15, 14, 27, 36, 28, 2, 14, 36, 2, 14, 12, 22, 25, 24, 11]\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input_char[0])\n",
    "print(len(tokenized_input_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7095\n"
     ]
    }
   ],
   "source": [
    "# get the length of the max array in tokenized_input_char\n",
    "max_len = max([len(line) for line in tokenized_input_char])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 23:38:28.485707: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-01 23:38:28.637266: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib\n",
      "2024-01-01 23:38:28.637307: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-01-01 23:38:29.555897: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib\n",
      "2024-01-01 23:38:29.556052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib\n",
      "2024-01-01 23:38:29.556070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_sequences_padded = pad_sequences(tokenized_input_char, padding='post', value=37, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28 18 14 ... 37 37 37]\n"
     ]
    }
   ],
   "source": [
    "print(word_sequences_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_44BK5K82YwF"
   },
   "source": [
    "# Importing and discovering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each diacrtics to its unicode\n",
    "diacritics_mapping = {\n",
    "    'FATHA': '\\u064E',\n",
    "    'DAMMA': '\\u064F',\n",
    "    'KASRA': '\\u0650',\n",
    "    'SHADDA': '\\u0651',\n",
    "    'SUKUN': '\\u0652',\n",
    "    'FATHATAN': '\\u064B',\n",
    "    'DAMMATAN': '\\u064C',\n",
    "    'KASRATAN': '\\u064D'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'َ', 1: 'ً', 2: 'ِ', 3: 'ٍ', 4: 'ُ', 5: 'ٌ', 6: 'ْ', 7: 'ّ', 8: 'َّ', 9: 'ًّ', 10: 'ِّ', 11: 'ٍّ', 12: 'ُّ', 13: 'ٌّ', 14: ''}\n"
     ]
    }
   ],
   "source": [
    "predictions_map = {\n",
    "    0 : diacritics_mapping['FATHA'],\n",
    "    1 : diacritics_mapping['FATHATAN'],\n",
    "    2 : diacritics_mapping['KASRA'],\n",
    "    3 : diacritics_mapping['KASRATAN'],\n",
    "    4 : diacritics_mapping['DAMMA'],\n",
    "    5 : diacritics_mapping['DAMMATAN'],\n",
    "    6 : diacritics_mapping['SUKUN'],\n",
    "    7 : diacritics_mapping['SHADDA'],\n",
    "    8 : diacritics_mapping['SHADDA'] + diacritics_mapping['FATHA'],\n",
    "    9 : diacritics_mapping['SHADDA'] + diacritics_mapping['FATHATAN'],\n",
    "    10 : diacritics_mapping['SHADDA'] + diacritics_mapping['KASRA'],\n",
    "    11 : diacritics_mapping['SHADDA'] + diacritics_mapping['KASRATAN'],\n",
    "    12 : diacritics_mapping['SHADDA'] + diacritics_mapping['DAMMA'],\n",
    "    13 : diacritics_mapping['SHADDA'] + diacritics_mapping['DAMMATAN'],\n",
    "    14 : ''\n",
    "}\n",
    "\n",
    "# reverse predictions_map\n",
    "# predictions_map = {v: k for k, v in predictions_map.items()}\n",
    "\n",
    "print(predictions_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "{'ً', 'َ', 'ّ', 'ِ', 'ٌ', 'ُ', 'ْ', 'ٍ'}\n",
      "36\n",
      "{'أ', 'ص', 'ا', 'ى', 'ذ', 'ف', 'ظ', 'ؤ', 'ت', 'ء', 'ه', 'ي', 'ز', 'ط', 'ل', 'إ', 'د', 'ئ', 'و', 'ج', 'ن', 'ح', 'ر', 'س', 'ش', 'ك', 'ع', 'خ', 'ق', 'غ', 'آ', 'م', 'ب', 'ة', 'ث', 'ض'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(diacritics))\n",
    "print(diacritics)\n",
    "\n",
    "print(len(arabic_letters))\n",
    "print(arabic_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import re\n",
    "\n",
    "# Remove diacritics\n",
    "def remove_diacritics(text):\n",
    "    text = araby.strip_tashkeel(text)\n",
    "    return text\n",
    "\n",
    "# Remove any letters not found in set arabic_letters and not found in set diacritics\n",
    "def remove_non_arabic(text):\n",
    "    text = re.sub(r'[^\\s' + ''.join(arabic_letters) + ''.join(diacritics) + ']', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def save_sentence_in_file(path, words, permission='w'):\n",
    "    \"\"\"\n",
    "    Save the words in the file located at path \n",
    "    \"\"\"\n",
    "    with open(path, permission, encoding='utf-8') as file:\n",
    "            file.write(words + '\\n')\n",
    "\n",
    "def save_new_input_sentence(text,path=\"./OutputFiles/new_input_sentence.txt\", permission='w'):\n",
    "    # Remove any non-Arabic letters and extra spaces\n",
    "    text = remove_non_arabic(text)\n",
    "\n",
    "    #remove extra spaces between words\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    save_sentence_in_file(path, text, permission)\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\"\n",
    "    Read the contents of the file located at file_path \n",
    "    and append each line to the list data\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "        # remove '\\n' from each line\n",
    "        data = [line.strip() for line in data]\n",
    "    return data\n",
    "\n",
    "data_before_preprocessing = read_data(\"./dataset/train.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(data_before_preprocessing)):\n",
    "    save_new_input_sentence(data_before_preprocessing[i], path=\"./OutputFiles/gold_input_sentence.txt\", permission='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tashkeel_list = read_pickle_file(\"./tashkeel_list.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['َ', 'ْ', 'ُ', 'ُ', '', 'َ', 'ْ', '', 'َ', 'َ', 'َ', '', '', 'ْ', 'َ', '٤', 'ُ', '', 'َ', 'َ', 'ُ', '', '', 'َ', 'ْ', '', 'َ', '', 'َ', '', '', '', '٤', 'ْ', 'َ', 'ِ', '٥']\n",
      "{'َ': 0, 'ً': 1, 'ِ': 2, 'ٍ': 3, 'ُ': 4, 'ٌ': 5, 'ْ': 6, 'ّ': 7, 'َّ': 8, 'ًّ': 9, 'ِّ': 10, 'ٍّ': 11, 'ُّ': 12, 'ٌّ': 13, '': 14}\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list[0])\n",
    "\n",
    "# reverse predictions_map\n",
    "predictions_map_reversed = {v: k for k, v in predictions_map.items()}\n",
    "\n",
    "print(predictions_map_reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_map = {\n",
    "    diacritics_mapping['FATHA'] : 0,\n",
    "    diacritics_mapping['FATHATAN'] : 1,\n",
    "    diacritics_mapping['KASRA'] : 2,\n",
    "    diacritics_mapping['KASRATAN'] : 3,\n",
    "    diacritics_mapping['DAMMA'] : 4,\n",
    "    diacritics_mapping['DAMMATAN'] : 5,\n",
    "    diacritics_mapping['SUKUN'] : 6,\n",
    "    diacritics_mapping['SHADDA'] : 7,\n",
    "    '٤' : 8,\n",
    "    '١' : 9,\n",
    "   '٦' : 10,\n",
    "   '٣' : 11,\n",
    "    '٥' : 12,\n",
    "    '٢' : 13,\n",
    "    '' : 14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over tashkeel_list and replace each tashkeel with predictions_map_reversed\n",
    "tashkeel_list_updated = [[]]\n",
    "for i in range(len(tashkeel_list)):\n",
    "    tashkeel_list_updated.append([])\n",
    "    for j in range(len(tashkeel_list[i])):\n",
    "        tashkeel_list_updated[i].append(new_map[tashkeel_list[i][j]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 4, 4, 14, 0, 6, 14, 0, 0, 0, 14, 14, 6, 0, 8, 4, 14, 0, 0, 4, 14, 14, 0, 6, 14, 0, 14, 0, 14, 14, 14, 8, 6, 0, 2, 12]\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list_updated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tashkeel_list_sequences_padded = pad_sequences(tashkeel_list_updated, padding='post', value=14, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7095\n",
      "7095\n"
     ]
    }
   ],
   "source": [
    "print(len(tashkeel_list_sequences_padded[2]))\n",
    "print(len(word_sequences_padded[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tokenized_input_char.txt to index using reverse_vocab and save it in tokenized_input_char_id list\n",
    "tokenized_input_char_id = []\n",
    "with open('./generatedFiles/training/tokenized_input_char.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    for line in lines:\n",
    "        tokenized_input_char_id.append(reverse_vocab[line])\n",
    "\n",
    "t_sentences = tokenized_input_char_id\n",
    "t_size = len(t_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-PkKTK22xc6"
   },
   "source": [
    "`vocab` is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a `<PAD>` token. \n",
    "\n",
    "When training an LSTM using batches, all your input sentences must be the same size. To accomplish this, you set the length of your sentences to a certain number and add the generic `<PAD>` token to fill all the empty spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IB_1MhtP1rLL",
    "outputId": "2b7bf8c4-ff02-422e-b926-e39573f1efed"
   },
   "outputs": [],
   "source": [
    "# vocab translates from a word to a unique number\n",
    "# print('padded token:', reverse_vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PagjN4rl22Fr",
    "outputId": "f65e741a-74e1-45f6-8361-58a0ce4cc818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'َ', 1: 'ً', 2: 'ِ', 3: 'ٍ', 4: 'ُ', 5: 'ٌ', 6: 'ْ', 7: 'ّ', 8: 'َّ', 9: 'ًّ', 10: 'ِّ', 11: 'ٍّ', 12: 'ُّ', 13: 'ٌّ', 14: ''}\n"
     ]
    }
   ],
   "source": [
    "# The possible tags\n",
    "print(predictions_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt3e4nxjFT3O"
   },
   "source": [
    "# NERDataset\n",
    "The class that impelements the dataset for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "29iM0u4-4YOV"
   },
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y):\n",
    "    \"\"\"\n",
    "    This is the constructor of the NERDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    ##################### TODO: create two tensors one for x and the other for labels ###############################\n",
    "\n",
    "    # Convert to tensors\n",
    "    self.x = torch.tensor(x)\n",
    "    self.y = torch.tensor(y)   \n",
    "\n",
    "    \n",
    "    #################################################################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    This function should return the length of the dataset (the number of sentences)\n",
    "    \"\"\"\n",
    "    ###################### TODO: return the length of the dataset #############################\n",
    "    return len(self.x)\n",
    "    ###########################################################################################\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    This function returns a subset of the whole dataset\n",
    "    \"\"\"\n",
    "    ###################### TODO: return a tuple of x and y ###################################\n",
    "    return self.x[idx], self.y[idx]\n",
    "    ##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(word_sequences_padded[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sz-saCtRs7Pz",
    "outputId": "03e6cdf1-7785-4725-d4ad-fb085c70e1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7095]) torch.Size([5, 7095]) torch.Size([5, 7095]) torch.Size([5, 7095])\n",
      "tensor([28, 18, 14,  ..., 37, 37, 37], dtype=torch.int32) \n",
      " tensor([ 0,  6,  4,  ..., 14, 14, 14], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "mini_dataset = NERDataset(word_sequences_padded, tashkeel_list_sequences_padded)\n",
    "dummy_dataloader = torch.utils.data.DataLoader(mini_dataset, batch_size=5)\n",
    "dg = iter(dummy_dataloader)\n",
    "X1, Y1 = next(dg)\n",
    "X2, Y2 = next(dg)\n",
    "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
    "print(X1[0][:], \"\\n\", Y1[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfw9pEd1wdMr"
   },
   "source": [
    "#### Expected output\n",
    "torch.Size([5, 30]) torch.Size([5, 30]) torch.Size([3, 30]) torch.Size([3, 30])\\\n",
    "tensor([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n",
    "           10,    11,    12,    13,    14,     9,    15,     1,    16,    17,\n",
    "           18,    19,    20,    21, 35180, 35180, 35180, 35180, 35180, 35180]) \\\n",
    "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQB6O7I7FbUh"
   },
   "source": [
    "# NER\n",
    "The class that implementss the pytorch model for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xHeJcz1JuhYa"
   },
   "outputs": [],
   "source": [
    "class NER(nn.Module):\n",
    "  def __init__(self, vocab_size=39, embedding_dim=50, hidden_size=50, n_classes=16):\n",
    "    \"\"\"\n",
    "    The constructor of our NER model\n",
    "    Inputs:\n",
    "    - vacab_size: the number of unique words\n",
    "    - embedding_dim: the embedding dimension\n",
    "    - n_classes: the number of final classes (tags)\n",
    "    \"\"\"\n",
    "    super(NER, self).__init__()\n",
    "    ####################### TODO: Create the layers of your model #######################################\n",
    "    # (1) Create the embedding layer\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "    # (3) Create a linear layer with number of neorons = n_classes\n",
    "    self.linear = nn.Linear(hidden_size, n_classes)\n",
    "    #####################################################################################################\n",
    "\n",
    "  def forward(self, sentences):\n",
    "    \"\"\"\n",
    "    This function does the forward pass of our model\n",
    "    Inputs:\n",
    "    - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "    Returns:\n",
    "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "    \"\"\"\n",
    "\n",
    "    final_output = None\n",
    "    ######################### TODO: implement the forward pass ####################################\n",
    "    output = self.embedding(sentences)\n",
    "    output, _ = self.lstm(output)\n",
    "    final_output = self.linear(output)\n",
    "    \n",
    "    ###############################################################################################\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJJJF-qQA_wk",
    "outputId": "c78037fc-d3a7-4743-d82d-aabb1d469a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER(\n",
      "  (embedding): Embedding(39, 50)\n",
      "  (lstm): LSTM(50, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NER()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCI_6UCRBk6N"
   },
   "source": [
    "#### Expected output\n",
    "NER( \\\n",
    "  (embedding): Embedding(35181, 50) \\\n",
    "  (lstm): LSTM(50, 50, batch_first=True) \\\n",
    "  (linear): Linear(in_features=50, out_features=17, bias=True) \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLHx_oHpFlSX"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-yvaq8i2CCLD"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size=256, epochs=5, learning_rate=0.01):\n",
    "  \"\"\"\n",
    "  This function implements the training logic\n",
    "  Inputs:\n",
    "  - model: the model ot be trained\n",
    "  - train_dataset: the training set of type NERDataset\n",
    "  - batch_size: integer represents the number of examples per step\n",
    "  - epochs: integer represents the total number of epochs (full training pass)\n",
    "  - learning_rate: the learning rate to be used by the optimizer\n",
    "  \"\"\"\n",
    "\n",
    "  ############################## TODO: replace the Nones in the following code ##################################\n",
    "  \n",
    "  # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  # (2) make the criterion cross entropy loss\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  # (3) create the optimizer (Adam)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      # (4) move the train input to the device\n",
    "      # train_label = train_label.to(device)\n",
    "      train_label = train_label.long().to(device)\n",
    "\n",
    "      # (5) move the train label to the device\n",
    "      train_input = train_input.long().to(device)\n",
    "\n",
    "\n",
    "      # (6) do the forward pass\n",
    "      # print(train_input.shape)\n",
    "      output = model(train_input)\n",
    "      \n",
    "      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "      # batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n",
    "      batch_loss = criterion(output.reshape(-1, 16), train_label.reshape(-1))\n",
    "\n",
    "      # (8) append the batch loss to the total_loss_train\n",
    "      total_loss_train += batch_loss.item()\n",
    "      \n",
    "      # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "      acc = (output.argmax(dim=2) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      # (10) zero your gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # (11) do the backward pass\n",
    "      batch_loss.backward()\n",
    "      # To avoid the exploding gradient problem, we clip the gradients\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "      # (12) update the weights with your optimizer\n",
    "      optimizer.step()\n",
    "      \n",
    "    # epoch loss\n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    # (13) calculate the accuracy\n",
    "    epoch_acc = total_acc_train / (len(train_dataset) * train_dataset[0][0].shape[0])\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')\n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "3BI7_ANkLf7G"
   },
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(word_sequences_padded, tashkeel_list_sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMXjDv51LU6k",
    "outputId": "92dbd51b-4732-48cb-f4ad-7ae7b09b1283"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██████████████████████████████████▎                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 7/196 [01:17<34:43, 11.03s/it]"
     ]
    }
   ],
   "source": [
    "train(model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# torch.save(model.state_dict(), \"./model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_before_preprocessing = read_data(\"../dataset/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_before_preprocessing)):\n",
    "    save_new_input_sentence(test_before_preprocessing[i], permission='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input_char_test = read_tokenized_input(reverse_vocab, './OutputFiles/test_input_sentence.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_test = max([len(line) for line in tokenized_input_char_test])\n",
    "print(max_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_sequences_padded_test = pad_sequences(tokenized_input_char_test, padding='post', value=37, maxlen=max_len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWJNO6mUXPRI"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz5mxUAJM1xS"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, batch_size=512):\n",
    "  \"\"\"\n",
    "  This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
    "  Inputs:\n",
    "  - model: a NER model\n",
    "  - test_dataset: dataset of type NERDataset\n",
    "  \"\"\"\n",
    "  ########################### TODO: Replace the Nones in the following code ##########################\n",
    "\n",
    "  # (1) create the test data loader\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  # GPU Configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  total_acc_test = 0\n",
    "  \n",
    "  # (2) disable gradients\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for test_input, test_label in tqdm(test_dataloader):\n",
    "      # (3) move the test input to the device\n",
    "      test_label = test_label.to(device)\n",
    "\n",
    "      # (4) move the test label to the device\n",
    "      test_input = test_input.to(device)\n",
    "\n",
    "      # (5) do the forward pass\n",
    "      output = model(test_input)\n",
    "      \n",
    "      # accuracy calculation (just add the correct predicted items to total_acc_test)\n",
    "      acc = (output.argmax(dim=2) == test_label).sum().item()\n",
    "      total_acc_test += acc\n",
    "    \n",
    "    # (6) calculate the over all accuracy\n",
    "    total_acc_test /= (len(test_dataset) * test_dataset[0][0].shape[0])\n",
    "  ##################################################################################################\n",
    "\n",
    "  \n",
    "  print(f'\\nTest Accuracy: {total_acc_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FD8JNcHWmMY",
    "outputId": "b4916766-dd57-4716-db7f-90c6d46655fa"
   },
   "outputs": [],
   "source": [
    "# evaluate(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.tensor(word_sequences_padded_test))\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0][0])\n",
    "\n",
    "softmax_output = nn.functional.softmax(output, dim=-1)\n",
    "print(softmax_output[0][0])\n",
    "\n",
    "# Find the index of the maximum value along the last axis\n",
    "max_arg = torch.argmax(softmax_output, dim=-1)\n",
    "\n",
    "# Add a new dimension at the end to make the shape (2000, 1904, 1)\n",
    "new_tensor = torch.unsqueeze(max_arg, dim=-1)\n",
    "\n",
    "print(new_tensor.shape)\n",
    "print(new_tensor[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate csv file\n",
    "with open('./OutputFiles/test_input_sentence.txt', 'r', encoding='utf-8') as file:\n",
    "    test_txt = file.readlines()\n",
    "        \n",
    "list_of_sentences = []\n",
    "for sentence in test_txt:\n",
    "    list_of_sentences.append(sentence.strip())\n",
    "\n",
    "# Create a list of lists with an added ID column and a single label column\n",
    "csv_data = [['id', 'label']]\n",
    "\n",
    "row = 0\n",
    "column = 0\n",
    "id = 0\n",
    "for sentence in list_of_sentences:\n",
    "    for char in sentence:\n",
    "        if char == \" \" or char == \".\":\n",
    "            column += 1\n",
    "            continue\n",
    "\n",
    "        csv_data.append([id, new_tensor[row][column].item()])\n",
    "\n",
    "        id += 1\n",
    "        column += 1\n",
    "\n",
    "    row += 1\n",
    "    column = 0\n",
    "\n",
    "with open(\"./Answer.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(f'CSV file has been created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhbNZ1HVaLO_"
   },
   "source": [
    "# Thank you"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
