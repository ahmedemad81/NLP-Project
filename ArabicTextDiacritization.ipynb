{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import farasa\n",
    "from farasa.segmenter import FarasaSegmenter \n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Specify the file path\n",
    "file_path = \"./dataset/train.txt\"\n",
    "\n",
    "# Read the contents of the file located at file_path \n",
    "# and append each line to the list data_before_preprocessing\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data_before_preprocessing = file.readlines()\n",
    "    # remove '\\n' from each line\n",
    "    data_before_preprocessing = [line.strip() for line in data_before_preprocessing]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove diacritics\n",
    "def remove_diacritics(text):\n",
    "    text = araby.strip_tashkeel(text)\n",
    "    return text\n",
    "\n",
    "# Remove any non-Arabic letters\n",
    "def remove_non_arabic(text):\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]|،|؛', '', text)\n",
    "    return text\n",
    "\n",
    "def input_preprocessing_text(text):\n",
    "    # Correct most common errors on word like repetetion of harakats, or tanween before alef\n",
    "    text = araby.autocorrect(text)\n",
    "\n",
    "    # Remove any non-Arabic letters\n",
    "    text = remove_non_arabic(text)\n",
    "\n",
    "    # Remove diacritics\n",
    "    text = remove_diacritics(text)\n",
    "\n",
    "    # Tokenize\n",
    "    text = araby.tokenize(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def save_tokenized_input(text):\n",
    "    words = input_preprocessing_text(text)\n",
    "    # Write and append on the tokenized input to a file\n",
    "    with open('./generatedFiles/tokenized_input.txt', 'a', encoding='utf-8') as file:\n",
    "        for word in words:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "def save_gold_output(text):\n",
    "    # Remove any non-Arabic letters and extra spaces\n",
    "    text = remove_non_arabic(text)\n",
    "\n",
    "    # Tokenize\n",
    "    text = araby.tokenize(text)\n",
    "\n",
    "    # Write and append on the gold output to a file\n",
    "    with open('./generatedFiles/gold_output.txt', 'a', encoding='utf-8') as file:\n",
    "        for word in text:\n",
    "            # if last word in the text don't add '\\n'\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "\n",
    "def is_not_arabic_diacritic(char):\n",
    "   category = unicodedata.category(char)\n",
    "   return not (category == 'Mn' or category == 'Mc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character is an Arabic diacritic.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "character = 'ذْ'\n",
    "if is_not_arabic_diacritic(character[1]):\n",
    "   print(\"The character is not an Arabic diacritic.\")\n",
    "else:\n",
    "   print(\"The character is an Arabic diacritic.\")\n",
    "\n",
    "\n",
    "# Testing of is_not_arabic_diacritic() function with gettting the index of the first non diacritic character in the word\n",
    "word = 'زَّراع'\n",
    " \n",
    "for i in range(1, len(word)): # start from 1 because the first character is not a diacritic\n",
    "    if is_not_arabic_diacritic(word[i]):\n",
    "        print(i)\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN ONE TIME ONLY THIS CODE AGAIN \n",
    "# # Generate Gold Input file\n",
    "# for i in range(len(data_before_preprocessing)):\n",
    "#     save_tokenized_input(data_before_preprocessing[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RUN ONE TIME ONLY THIS CODE AGAIN\n",
    "# # Generate Gold Output file\n",
    "# for i in range(len(data_before_preprocessing)):\n",
    "#     test = data_before_preprocessing[i]\n",
    "#     text1 = save_gold_output(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قال', 'ابن', 'القاسم', 'قال', 'مالك', 'في', 'مكي', 'أحرم', 'بحجة', 'من', 'الحرم', 'ثم', 'أحصر', 'أنه', 'يخرج', 'إلى', 'الحل', 'فيلبي', 'من', 'هناك', 'لأنه', 'أمر', 'من', 'فاته', 'الحج', 'وقد', 'أحرم', 'من', 'مكة', 'أن', 'يخرج', 'إلى', 'الحل', 'فيعمل', 'فيما', 'بقي', 'عليه', 'ما', 'يعمل', 'المعتمر', 'ويحل']\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "test = \"قَالَ ابْنُ الْقَاسِمِ : قَالَ مَالِكٌ فِي مَكِّيٍّ أَحْرَمَ بِحَجَّةٍ مِنْ الْحَرَمِ ثُمَّ أُحْصِرَ ، أَنَّهُ يَخْرُجُ إلَى الْحِلِّ فَيُلَبِّي مِنْ هُنَاكَ لِأَنَّهُ أَمَرَ مَنْ فَاتَهُ الْحَجُّ وَقَدْ أَحْرَمَ مِنْ مَكَّةَ ، أَنْ يَخْرُجَ إلَى الْحِلِّ فَيَعْمَلَ فِيمَا بَقِيَ عَلَيْهِ مَا يَعْمَلُ الْمُعْتَمِرُ وَيُحِلُّ .( 2 / 437 ) \"\n",
    "text2 = input_preprocessing_text(test)\n",
    "print(text2)\n",
    "text3 = remove_non_arabic(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important functions in PyArabic\n",
    "\n",
    "# araby.tokenize(text) # Tokenize the sentence text into words\n",
    "# araby.is_arabicrange(text) # Check if the text is Arabic\n",
    "# araby.sentence_tokenize(text) # Tokenize the text into sentences\n",
    "# araby.strip_tashkeel(text) # Remove diacritics (FATHA, DAMMA, KASRA, SUKUN, SHADDA, FATHATAN, DAMMATAN, KASRATAN)\n",
    "# araby.strip_diacritics(text) # Remove diacritics (Small Alef الألف الخنجرية, Harakat + Shadda, Quranic marks)\n",
    "# araby.strip_tatweel(text) # Remove tatweel\n",
    "# araby.strip_shadda(text) # Remove shadda\n",
    "# araby.autocorrect(text) # Correct most common errors on word like repetetion of harakats,or tanwin befor alef\n",
    "# araby.arabicrange() # Return a list of Arabic characters\n",
    "\n",
    "# New Functions in PyArabic\n",
    "# araby.vocalized_similarity(word1, word2) # if the two words has the same letters and the same harakats, this function return True. \n",
    "# The two words can be full vocalized, or partial vocalized\n",
    "\n",
    "# araby.vocalizedlike(word1, word2) Same as vocalized_similarity but return True and False\n",
    "\n",
    "# araby.joint(word1, word2) # joint the letters with the marks the length ot letters and marks must be equal return word\n",
    "\n",
    "\n",
    "\n",
    "# Return the text, its tashkeel and shadda if extract_shadda is True\n",
    "# text, marks, shada = araby.separate(text,extract_shadda=True) # Separate diacritics from the text\n",
    "# print (text)\n",
    "# for m in marks:\n",
    "#     print (araby.name(m))\n",
    "\n",
    "# for s in shada:\n",
    "#     print (araby.name(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n"
     ]
    }
   ],
   "source": [
    "# read the tokenized input file\n",
    "with open('./generatedFiles/tokenized_input.txt', 'r', encoding='utf-8') as file:\n",
    "    tokenized_input = file.readlines()\n",
    "    print(len(tokenized_input))\n",
    "    # Remove '\\n' from each line\n",
    "    tokenized_input = [line.strip() for line in tokenized_input]\n",
    "    # Put the tokenized input of length 1 in tokenized_input list \n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "\n",
    "print(tokenized_input[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Core Word (CW) Diacritization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Characters: \n",
    "Here we extract each character from all tokenized words and create a vector of size 50 for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_char = Tokenizer(char_level=True)\n",
    "tokenizer_char.fit_on_texts(tokenized_input)\n",
    "sequences_char = tokenizer_char.texts_to_sequences(tokenized_input)\n",
    "char_features = pad_sequences(sequences_char)   # padding the sequences to have the same length as the longest sequence (word)\n",
    "char_embeddings = np.random.rand(len(tokenizer_char.word_index) + 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2104308, 13)\n",
      "(38, 50)\n"
     ]
    }
   ],
   "source": [
    "print(char_features.shape) # (number of words, max length of word in the dataset)\n",
    "\n",
    "\n",
    "print(char_embeddings.shape)\n",
    "\n",
    "# 38 rows: 37 unique characters identified by the tokenizer, 1 row for handling characters not seen in the training data\n",
    "# 50 columns: Each character is encoded as a 50-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0 13  5  1  7]\n"
     ]
    }
   ],
   "source": [
    "print(char_features[0]) \n",
    "# the number of non zero elements corresponds to the length of the word \n",
    "# and the value of each element corresponds to the index of the character in the tokenizer\n",
    "# which means that every character now is encoded as a number and this number is the index of the character in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      " 4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      " 3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      " 9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      " 7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      " 6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      " 3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      " 5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      " 1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      " 4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      " 6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      " 8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      " 5.88926337e-02 2.75899389e-01]\n"
     ]
    }
   ],
   "source": [
    "print(char_embeddings[0])\n",
    "# this is the embedding of each character in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 50)\n",
      "[[5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      "  4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      "  3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      "  9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      "  7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      "  6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      "  3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      "  5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      "  1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      "  4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      "  6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      "  8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      "  5.88926337e-02 2.75899389e-01]\n",
      " [5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      "  4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      "  3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      "  9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      "  7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      "  6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      "  3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      "  5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      "  1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      "  4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      "  6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      "  8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      "  5.88926337e-02 2.75899389e-01]\n",
      " [5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      "  4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      "  3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      "  9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      "  7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      "  6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      "  3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      "  5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      "  1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      "  4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      "  6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      "  8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      "  5.88926337e-02 2.75899389e-01]\n",
      " [5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      "  4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      "  3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      "  9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      "  7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      "  6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      "  3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      "  5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      "  1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      "  4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      "  6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      "  8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      "  5.88926337e-02 2.75899389e-01]\n",
      " [5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      "  4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      "  3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      "  9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      "  7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      "  6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      "  3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      "  5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      "  1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      "  4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      "  6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      "  8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      "  5.88926337e-02 2.75899389e-01]\n",
      " [5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      "  4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      "  3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      "  9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      "  7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      "  6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      "  3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      "  5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      "  1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      "  4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      "  6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      "  8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      "  5.88926337e-02 2.75899389e-01]\n",
      " [5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      "  4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      "  3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      "  9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      "  7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      "  6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      "  3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      "  5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      "  1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      "  4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      "  6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      "  8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      "  5.88926337e-02 2.75899389e-01]\n",
      " [5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      "  4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      "  3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      "  9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      "  7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      "  6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      "  3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      "  5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      "  1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      "  4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      "  6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      "  8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      "  5.88926337e-02 2.75899389e-01]\n",
      " [5.82098405e-03 4.52170943e-01 2.26118577e-01 4.52734281e-01\n",
      "  4.71986498e-01 8.77569283e-01 2.99677815e-01 1.55726624e-02\n",
      "  3.91478983e-02 8.24427352e-01 7.56550101e-01 7.05479687e-01\n",
      "  9.33947820e-01 6.46740160e-01 5.92744401e-01 5.52453906e-01\n",
      "  7.23129971e-01 4.25153965e-01 1.24329615e-01 9.44744530e-01\n",
      "  6.15098857e-01 4.98246613e-01 7.18669207e-01 8.58796914e-02\n",
      "  3.24955328e-01 4.43034449e-01 6.01822368e-01 1.41558367e-01\n",
      "  5.80331513e-01 5.88397080e-01 6.45024165e-01 1.22411292e-02\n",
      "  1.08545581e-01 7.86578797e-01 4.30717596e-01 9.97969209e-01\n",
      "  4.61185776e-01 6.16297552e-01 2.62678067e-04 1.74403967e-01\n",
      "  6.12556529e-01 2.86671947e-01 4.35586596e-01 2.13315200e-01\n",
      "  8.54312559e-01 3.13761512e-01 8.46542087e-01 6.52132553e-01\n",
      "  5.88926337e-02 2.75899389e-01]\n",
      " [3.48861561e-01 1.63562483e-01 2.97182216e-01 8.06423318e-01\n",
      "  2.85398895e-01 7.21040176e-01 6.24659579e-01 9.79838279e-01\n",
      "  5.24432900e-01 3.90879895e-01 1.23620057e-01 5.03501702e-01\n",
      "  3.13726296e-01 3.98173501e-01 2.84070172e-01 6.80847027e-01\n",
      "  8.32553837e-01 7.84290800e-02 8.61252600e-01 5.65008367e-01\n",
      "  6.87116212e-01 8.14659562e-01 3.47213630e-01 3.74738582e-01\n",
      "  2.10813133e-03 3.65646620e-01 5.86578890e-01 6.20064374e-01\n",
      "  9.76372259e-01 3.39190561e-01 2.52993747e-01 4.87888572e-01\n",
      "  6.11856923e-01 7.13645643e-01 2.33167011e-02 4.43177752e-01\n",
      "  4.04543972e-01 5.63599182e-02 6.98103931e-01 7.70947210e-01\n",
      "  9.14830842e-01 8.10454207e-01 3.88421180e-02 5.79988214e-01\n",
      "  3.16831634e-01 4.29105802e-01 9.61974800e-01 8.82567996e-01\n",
      "  4.64405944e-01 5.67250298e-01]\n",
      " [9.05105039e-01 5.76778872e-01 4.68364371e-01 2.67756774e-01\n",
      "  7.84557809e-01 9.68613542e-01 5.98159244e-01 2.52629335e-01\n",
      "  9.28316637e-01 4.12650093e-01 7.57494003e-01 9.02647168e-01\n",
      "  1.41397227e-01 6.03643565e-01 6.67874278e-01 6.19189543e-01\n",
      "  2.05643510e-03 9.60797383e-01 2.66674929e-01 4.40751534e-01\n",
      "  1.11858613e-01 2.40506458e-01 8.77681563e-02 9.17052858e-01\n",
      "  1.54781610e-01 3.57381250e-02 3.65252086e-01 4.19477095e-01\n",
      "  5.94607361e-02 9.24083356e-01 9.92882212e-01 7.40287112e-01\n",
      "  3.80227458e-01 1.22172731e-01 5.62718224e-01 7.00590193e-01\n",
      "  5.04128822e-01 9.34043684e-01 3.58792083e-01 6.11548136e-01\n",
      "  2.95182026e-01 4.08236521e-02 2.39569023e-01 2.17792578e-03\n",
      "  1.24942993e-01 9.30191209e-01 8.09965924e-01 1.41655953e-01\n",
      "  7.38941630e-01 9.01780363e-01]\n",
      " [7.76188009e-01 7.20947864e-02 6.22311908e-01 9.78505399e-01\n",
      "  7.15675370e-01 4.81514869e-01 5.33351361e-02 5.37998816e-01\n",
      "  1.92510816e-01 6.54627332e-01 8.12733494e-01 7.31943143e-01\n",
      "  4.13136859e-01 3.13311601e-01 3.21253202e-01 8.75376493e-02\n",
      "  7.69021823e-01 2.94901077e-01 9.31067777e-01 7.88734085e-02\n",
      "  2.45053876e-01 6.36133900e-01 5.46069338e-01 5.98299727e-01\n",
      "  9.96901088e-01 8.90522676e-01 6.51220440e-01 5.10018135e-01\n",
      "  3.05142493e-01 1.48970896e-01 6.85723924e-01 6.25954386e-01\n",
      "  1.28517742e-01 8.85926168e-01 5.35665500e-01 2.75831886e-01\n",
      "  9.39576193e-01 1.98136130e-01 4.39619377e-02 7.61573213e-01\n",
      "  8.29704614e-01 6.58522015e-01 5.21599425e-01 6.37920287e-01\n",
      "  8.29699128e-01 5.82845233e-01 1.48543969e-01 4.32774445e-01\n",
      "  3.90294808e-01 7.41833374e-01]\n",
      " [4.28461915e-01 9.63143457e-01 2.30203980e-01 5.76335703e-01\n",
      "  7.19686971e-01 3.90066153e-01 2.90632162e-01 6.87619874e-01\n",
      "  6.98203723e-01 1.57570890e-01 8.18107005e-01 1.39758123e-01\n",
      "  6.29708380e-01 1.59950052e-01 8.31620645e-01 9.42009148e-01\n",
      "  8.09337969e-01 7.71928071e-01 6.55331453e-01 4.32986162e-01\n",
      "  1.39200526e-02 9.11189920e-01 4.46696919e-01 7.25135759e-01\n",
      "  7.13647928e-01 6.48264539e-02 5.93590324e-01 3.94297887e-01\n",
      "  6.62678118e-01 2.35884509e-01 6.70511957e-02 2.02913866e-01\n",
      "  3.37662929e-01 5.44031521e-01 2.21803229e-01 1.29426881e-01\n",
      "  8.83378833e-01 3.63624565e-01 7.69070337e-01 7.73189892e-01\n",
      "  9.66764225e-01 4.05911384e-02 1.05364780e-03 5.27919287e-01\n",
      "  2.45300114e-01 1.45733940e-01 5.16349348e-01 3.96545877e-01\n",
      "  2.75822212e-01 9.26783192e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(char_embeddings[char_features[0]].shape)\n",
    "# 13 is the word of characters and 50 is the embedding size of each character\n",
    "\n",
    "print(char_embeddings[char_features[0]])\n",
    "# this is the embedding of each character in the first tokenized word, this is the 1st feature and the input of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - The position of the character in a word segment:\n",
    "For example, given the word “wAlktAb” , which is composed of three segments “w+Al+ktAb”. Letters were marked as “B” if they begin a segment, “M” if they are in the middle of a segment, “E” if they end a segment, and “S” if they are single letter segments. So for “w+Al+ktAb”, the corresponding character positions are “S+BE+BMME.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-30 03:34:06,901 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "segmenter = FarasaSegmenter(interactive=True) # The default behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented word: ['ك', 'قلم', 'ه']\n",
      "SEG tags: ['S', 'B', 'M', 'E', 'S']\n"
     ]
    }
   ],
   "source": [
    "def get_seg_tags(word):                 # word = \"wAlktAb\"\n",
    "    segments = segmenter.segment(word)  # segments will be a list: [\"w\", \"Al\", \"ktAb\"]\n",
    "    segments = segments.split('+')\n",
    "    seg_tags = []\n",
    "    for segment in segments:\n",
    "        if len(segment) == 1:\n",
    "            seg_tags.append(\"S\")\n",
    "        else:\n",
    "            seg_tags.append(\"B\")  # First letter\n",
    "            seg_tags.extend(\"M\" * (len(segment) - 2))  # Middle letters\n",
    "            seg_tags.append(\"E\")  # Last letter\n",
    "    return segments, seg_tags\n",
    "\n",
    "word = \"كقلمه\"\n",
    "segments, seg_tags = get_seg_tags(word)\n",
    "print(\"Segmented word:\", segments)\n",
    "print(\"SEG tags:\", seg_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DON'T RUN THIS CODE AGAIN, THIS CELL TOOK 25m 4.5s TO RUN\n",
    "# # The Output of this code is the input_segments.txt file\n",
    "\n",
    "# for i in range(len(tokenized_input)):\n",
    "#     segments, seg_tags = get_seg_tags(tokenized_input[i])\n",
    "#     # Write and append on the tokenized input to a file\n",
    "#     with open('./generatedFiles/input_segments.txt', 'a', encoding='utf-8') as file:\n",
    "#         for tag in seg_tags:\n",
    "#             file.write(tag)\n",
    "#         file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['BMES', 'BE', 'BME', 'BEBME', 'BES', 'BME', 'BME', 'BEBMMME', 'BME', 'BMES']\n"
     ]
    }
   ],
   "source": [
    "# Read the input_segments file\n",
    "with open('./generatedFiles/input_segments.txt', 'r', encoding='utf-8') as file:\n",
    "    input_segments = file.readlines()\n",
    "    print(len(input_segments))\n",
    "    # Remove '\\n' from each line\n",
    "    input_segments = [line.strip() for line in input_segments]\n",
    "    # Put the tokenized input of length 1 in the tokenized_input list\n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "\n",
    "print(input_segments[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tags = Tokenizer(char_level=True)\n",
    "tokenizer_tags.fit_on_texts(input_segments)\n",
    "sequences_tags = tokenizer_tags.texts_to_sequences(input_segments)\n",
    "tags_features = pad_sequences(sequences_tags)   \n",
    "tags_embeddings = np.random.rand(len(tokenizer_tags.word_index) + 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2104308, 13)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "print(tags_features.shape) \n",
    "print(tags_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - PRIOR: \n",
    "diacritics seen in the training set per segment. Since we used a character-level model, this feature informed the model with word-level information. For example, the word “ktAb”  was observed to have two diacritized forms in the training set, namely “kitaAb” ( – book) and “kut∼aAb” ( – writers). The first letter in the word (“k”) accepted the diacritics “i” and “u.” Thus, given a binary vector representing whether a character is allowed to assume any of the eight primitive Arabic diacritic marks (a, i, u, o, K, N, F, and ∼ in order), the first letter would be given the following vector “01100000.” If a word segment was never observed during training, then the vector for all letters therein would be set to 11111111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قَوْلُهُ', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', 'قَالَ', 'الزَّرْكَشِيُّ', 'ابْنُ', 'عَرَفَةَ']\n"
     ]
    }
   ],
   "source": [
    "# read the gold_output file\n",
    "with open('./generatedFiles/gold_output.txt', 'r', encoding='utf-8') as file:\n",
    "    gold_output = file.readlines()\n",
    "    print(len(gold_output))\n",
    "    # remove '\\n' from each line\n",
    "    gold_output = [line.strip() for line in gold_output]\n",
    "    # put in tokenized_input list the tokenized input of length 1\n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "    # get the inde\n",
    "\n",
    "print(gold_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_input))\n",
    "print(tokenized_input[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each diacrtics to its unicode\n",
    "diacritics_mapping = {\n",
    "    'FATHA': '\\u064E',\n",
    "    'DAMMA': '\\u064F',\n",
    "    'KASRA': '\\u0650',\n",
    "    'SHADDA': '\\u0651',\n",
    "    'SUKUN': '\\u0652',\n",
    "    'FATHATAN': '\\u064B',\n",
    "    'DAMMATAN': '\\u064C',\n",
    "    'KASRATAN': '\\u064D'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract diacritics by returning a list containing a tuple of 3 elements: (letter, tashkeel, shadda)\n",
    "# def extract_arabic_diacritics(word):\n",
    "#     diacritics_list = []\n",
    "#     extracted_word, tashkeel, shadda = araby.separate(word, extract_shadda=True)\n",
    "#     for i in range(len(extracted_word)):\n",
    "#         print(f'{araby.name(extracted_word[i])} {araby.name(tashkeel[i])} {araby.name(shadda[i])}')\n",
    "#         diacritics_list.append((extracted_word[i], (tashkeel[i].encode(\"utf8\")).decode(), (shadda[i].encode(\"utf8\")).decode()))\n",
    "#     return diacritics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # firstly, initialize an empty dictionary for the 'prior' feature\n",
    "# # the value will be the 8 arabic marks (FATHA, DAMMA, KASRA, FATHATAN, DAMMATAN, KASRATAN, SUKUN, SHADDA) as a binary vector\n",
    "\n",
    "# # then, loop over the tokenized input and check if the each character and word pair is not in the dictionary, get the indices of this word and its duplicates in the tokenized input array\n",
    "# def get_prior(tokenized_input, gold_output):\n",
    "#     prior = {} # this dictionary will hold a key of tuple of 3 elements (word, character, index of character in the word) and the value will be the 8 arabic marks\n",
    "#     for i in range(len(tokenized_input)):\n",
    "#         if (tokenized_input[i], tokenized_input[i][0], 0) not in prior:\n",
    "#             # get the indices of the word in the tokenized input array\n",
    "#             indices = [j for j, x in enumerate(tokenized_input) if x == tokenized_input[i]]\n",
    "#             print(indices)\n",
    "#             # get the words in the gold_output array with the same indices\n",
    "#             words = [gold_output[j] for j in indices]\n",
    "#             extracted_diac_all_words = []\n",
    "#             for word in words:\n",
    "#                 extracted_diac_all_words.append(extract_arabic_diacritics(word))\n",
    "#             for indx, charac in enumerate(tokenized_input[i]):\n",
    "#                 for extracted_diac_per_word in extracted_diac_all_words:\n",
    "#                     # extract the diacritics of word[indx]\n",
    "#                     prior[(tokenized_input[i], charac, indx)] = [0, 0, 0, 0, 0, 0, 0, 0] # initialize the value of the key with zeros\n",
    "#                     if diacritics_mapping['SHADDA'] in extracted_diac_per_word[indx]:\n",
    "#                         prior[(tokenized_input[i], charac, indx)][4] = 1 if diacritics_mapping['FATHA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac, indx)][5] = 1 if diacritics_mapping['DAMMA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac, indx)][6] = 1 if diacritics_mapping['KASRA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac, indx)][7] = 1 if not  diacritics_mapping['FATHA'] in extracted_diac_per_word[indx] and not diacritics_mapping['DAMMA'] in word[indx: indx+2]  and not diacritics_mapping['KASRA'] in word[indx: indx+2] else 0\n",
    "#                     else:\n",
    "#                         prior[(tokenized_input[i], charac,indx)][0] = 1 if diacritics_mapping['FATHA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac,indx)][1] = 1 if diacritics_mapping['DAMMA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac,indx)][2] = 1 if diacritics_mapping['KASRA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac,indx)][3] = 1 if diacritics_mapping['SUKUN'] in extracted_diac_per_word[indx] else 0\n",
    "#     return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FATHA in tashkeel:  True\n",
      "DAMMA in tashkeel:  False\n",
      "KASRA in tashkeel:  False\n",
      "SUKUN in tashkeel:  True\n",
      "FATHATAN in tashkeel:  False\n",
      "DAMMATAN in tashkeel:  False\n",
      "KASRATAN in tashkeel:  False\n",
      "SHADDA in tashkeel:  False\n",
      "=============================\n",
      "FATHA in shadda:  False\n",
      "DAMMA in shadda:  False\n",
      "KASRA in shadda:  False\n",
      "SUKUN in shadda:  False\n",
      "FATHATAN in shadda:  False\n",
      "DAMMATAN in shadda:  False\n",
      "KASRATAN in shadda:  False\n",
      "SHADDA in shadda:  True\n",
      "testt False\n",
      "yarab False\n"
     ]
    }
   ],
   "source": [
    "letter, tashkeel, shadda = araby.separate('زَّ', extract_shadda=True)   # SHADDA + FATHA Example\n",
    "# letter, tashkeel, shadda = araby.separate('وَ', extract_shadda=True)   # FATHA Example\n",
    "# letter, tashkeel, shadda = araby.separate('مً', extract_shadda=True)   # FATHATAN Example\n",
    "# letter, tashkeel, shadda = araby.separate('عٌ', extract_shadda=True)   # DAMMATAN Example\n",
    "# letter, tashkeel, shadda = araby.separate('يُّ', extract_shadda=True)   # SHADDA + DAMMA Example\n",
    "# letter, tashkeel, shadda = araby.separate('ذْ', extract_shadda=True)   # SUKUN Example\n",
    "enkar = 'كَإِنْكَارِ'\n",
    "# print(enkar[4:6])\n",
    "# print( diacritics_mapping['FATHA'] in enkar[0:1])\n",
    "# print( diacritics_mapping['SHADDA'] in 'زَّ')\n",
    "# print( diacritics_mapping['DAMMA'] in 'زَّ')\n",
    "\n",
    "print('FATHA in tashkeel: ', diacritics_mapping['FATHA'] in tashkeel)\n",
    "print('DAMMA in tashkeel: ', diacritics_mapping['DAMMA'] in tashkeel)\n",
    "print('KASRA in tashkeel: ', diacritics_mapping['KASRA'] in tashkeel)\n",
    "print('SUKUN in tashkeel: ', diacritics_mapping['SUKUN'] in tashkeel)\n",
    "print('FATHATAN in tashkeel: ', diacritics_mapping['FATHATAN'] in tashkeel)\n",
    "print('DAMMATAN in tashkeel: ', diacritics_mapping['DAMMATAN'] in tashkeel)\n",
    "print('KASRATAN in tashkeel: ', diacritics_mapping['KASRATAN'] in tashkeel)\n",
    "print('SHADDA in tashkeel: ', diacritics_mapping['SHADDA'] in tashkeel)\n",
    "print('=============================')\n",
    "print('FATHA in shadda: ', diacritics_mapping['FATHA'] in shadda)\n",
    "print('DAMMA in shadda: ', diacritics_mapping['DAMMA'] in shadda)\n",
    "print('KASRA in shadda: ', diacritics_mapping['KASRA'] in shadda)\n",
    "print('SUKUN in shadda: ', diacritics_mapping['SUKUN'] in shadda)\n",
    "print('FATHATAN in shadda: ', diacritics_mapping['FATHATAN'] in shadda)\n",
    "print('DAMMATAN in shadda: ', diacritics_mapping['DAMMATAN'] in shadda)\n",
    "print('KASRATAN in shadda: ', diacritics_mapping['KASRATAN'] in shadda)\n",
    "print('SHADDA in shadda: ', diacritics_mapping['SHADDA'] in shadda)\n",
    "\n",
    "print('testt', (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['FATHA'] not in tashkeel and diacritics_mapping['DAMMA'] not in tashkeel and diacritics_mapping['KASRA'] not in tashkeel))\n",
    "print('yarab', (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['SHADDA'] not in shadda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstly, initialize an empty dictionary for the 'prior' feature\n",
    "# the value will be the 8 arabic marks (FATHA, DAMMA, KASRA, FATHATAN, DAMMATAN, KASRATAN, SUKUN, SHADDA) as a binary vector\n",
    "\n",
    "# then, loop over the tokenized input and check if the each character and word pair is not in the dictionary, get the indices of this word and its duplicates in the tokenized input array\n",
    "def get_prior(tokenized_input, gold_output):\n",
    "    prior = {}  # this dictionary will hold a key of tuple of 3 elements (word, character, index of character in the word) and the value will be the 8 arabic marks\n",
    "    for i in range(len(tokenized_input)):\n",
    "        if (tokenized_input[i], tokenized_input[i][0], 0) not in prior:\n",
    "            # get the indices of the word in the tokenized input array\n",
    "            indices = [j for j, x in enumerate(tokenized_input) if x == tokenized_input[i]]\n",
    "            print(indices)\n",
    "            # get the words in the gold_output array with the same indices\n",
    "            words = []\n",
    "            maxi_len = 0\n",
    "            for j in indices:\n",
    "                if gold_output[j] not in words:\n",
    "                    words.append(gold_output[j])\n",
    "                    maxi_len = max(maxi_len, len(gold_output[j]))\n",
    "\n",
    "            for t in range(len(tokenized_input[i])):\n",
    "                prior[(tokenized_input[i], tokenized_input[i][t], t)] = [0, 0, 0, 0, 0, 0, 0, 0] # initialize the value of the key with zeros\n",
    "            \n",
    "            indx2 = 0\n",
    "            for word in words:\n",
    "                indx = 0\n",
    "                while indx < maxi_len:\n",
    "                    # extract the diacritics of word[indx]\n",
    "                    for iter in range(indx+1, len(word)):\n",
    "                        if is_not_arabic_diacritic(word[iter]):\n",
    "                            # print(iter)\n",
    "                            letter, tashkeel, shadda = araby.separate(word[indx: iter], extract_shadda=True) \n",
    "                            if diacritics_mapping['FATHA'] in tashkeel:         prior[(tokenized_input[i], word[indx], indx2)][0] = 1 \n",
    "                            if diacritics_mapping['DAMMA'] in tashkeel:         prior[(tokenized_input[i], word[indx], indx2)][1] = 1\n",
    "                            if diacritics_mapping['KASRA'] in tashkeel:         prior[(tokenized_input[i], word[indx], indx2)][2] = 1\n",
    "                            if diacritics_mapping['FATHATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], indx2)][3] = 1\n",
    "                            if diacritics_mapping['DAMMATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], indx2)][4] = 1\n",
    "                            if diacritics_mapping['KASRATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], indx2)][5] = 1\n",
    "                            if (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['SHADDA'] not in shadda):  \n",
    "                                prior[(tokenized_input[i], word[indx], indx2)][6] = 1 # if the letter has SHADDA, araby.separate() will return SUKUN in tashkeel and SHADDA in shadda, so to avoid this mislabeling we check if SHADDA not in shadda and if SUKUN in tashkeel, then this is a true SUKUN\n",
    "                            if diacritics_mapping['SHADDA'] in shadda:          prior[(tokenized_input[i], word[indx], indx2)][7] = 1\n",
    "                            indx = iter - 1\n",
    "                            indx2 += 1\n",
    "                            break \n",
    "                    indx += 1\n",
    "                indx2 = 0\n",
    "\n",
    "\n",
    "                indx = len(word) - 1    # my assumption is that the last character in the not a diacritic\n",
    "                if (not is_not_arabic_diacritic(word[len(word) - 1]) and is_not_arabic_diacritic(word[len(word) - 2])):  # if the last character is a diacritic and the one before it is not, then the index of the last character is len(word) - 2\n",
    "                    indx = len(word) - 2\n",
    "                elif (not is_not_arabic_diacritic(word[len(word) - 1]) and not is_not_arabic_diacritic(word[len(word) - 2])):  # if the last character is a diacritic and the one before it is also a diacritic (in shadda case), then the index of the last character is len(word) - 3\n",
    "                    indx = len(word) - 3\n",
    "\n",
    "\n",
    "                if (tokenized_input[i], word[indx], indx) not in prior:\n",
    "                    letter, tashkeel, shadda = araby.separate(word[indx: len(word)], extract_shadda=True) \n",
    "                    if diacritics_mapping['FATHA'] in tashkeel:         prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][0] = 1\n",
    "                    if diacritics_mapping['DAMMA'] in tashkeel:         prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][1] = 1\n",
    "                    if diacritics_mapping['KASRA'] in tashkeel:         prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][2] = 1 \n",
    "                    if diacritics_mapping['FATHATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][3] = 1 \n",
    "                    if diacritics_mapping['DAMMATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][4] = 1 \n",
    "                    if diacritics_mapping['KASRATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][5] = 1\n",
    "                    if (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['SHADDA'] not in shadda):\n",
    "                        prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][6] = 1  # if the letter has SHADDA, araby.separate() will return SUKUN in tashkeel and SHADDA in shadda, so to avoid this mislabeling we check if SHADDA not in shadda and if SUKUN in tashkeel, then this is a true SUKUN\n",
    "                    if diacritics_mapping['SHADDA'] in shadda:          prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][7] = 1\n",
    "                    \n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[2, 3]\n",
      "[4, 5]\n",
      "[6]\n",
      "{('كإنكار', 'ك', 0): [1, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'إ', 1): [0, 0, 1, 0, 0, 0, 0, 0], ('كإنكار', 'ن', 2): [0, 0, 0, 0, 0, 0, 1, 0], ('كإنكار', 'ك', 3): [1, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'ا', 4): [0, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'ر', 5): [0, 0, 1, 0, 0, 1, 0, 0], ('بقذر', 'ب', 0): [0, 0, 1, 0, 0, 0, 0, 0], ('بقذر', 'ق', 1): [1, 0, 0, 0, 0, 0, 0, 0], ('بقذر', 'ذ', 2): [1, 0, 0, 0, 0, 0, 0, 0], ('بقذر', 'ر', 3): [0, 0, 0, 0, 0, 1, 0, 0], ('أكثر', 'أ', 0): [1, 0, 0, 0, 0, 0, 0, 0], ('أكثر', 'ك', 1): [0, 0, 0, 0, 0, 0, 1, 0], ('أكثر', 'ث', 2): [1, 0, 0, 0, 0, 0, 0, 0], ('أكثر', 'ر', 3): [1, 1, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ا', 0): [0, 0, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ل', 1): [0, 0, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ز', 2): [1, 0, 0, 0, 0, 0, 0, 1], ('الزركشي', 'ر', 3): [0, 0, 0, 0, 0, 0, 1, 0], ('الزركشي', 'ك', 4): [1, 0, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ش', 5): [0, 0, 1, 0, 0, 0, 0, 0], ('الزركشي', 'ي', 6): [0, 1, 0, 0, 0, 0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "test_tokenized_input = ['كإنكار', 'كإنكار', 'بقذر','بقذر', 'أكثر', 'أكثر', 'الزركشي']\n",
    "test_gold_output = ['كَإِنْكَارِ','كَإِنْكَارٍ', 'بِقَذَر', 'بِقَذَرٍ','أكْثَرَ', 'أَكْثَرُ', 'الزَّرْكَشِيُّ']\n",
    "print (get_prior(test_tokenized_input, test_gold_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
