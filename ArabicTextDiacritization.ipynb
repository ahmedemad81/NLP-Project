{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hlaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import farasa\n",
    "from farasa.segmenter import FarasaSegmenter \n",
    "import unicodedata\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run on GPU\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# print (device)\n",
    "# # print the cpu or gpu\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# # print the number of gpus you have\n",
    "# print(torch.cuda.device_count())\n",
    "# # print current gpu\n",
    "# print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    \"\"\"\n",
    "    Read the contents of the file located at file_path \n",
    "    and append each line to the list data\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "        # remove '\\n' from each line\n",
    "        data = [line.strip() for line in data]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "data_before_preprocessing = read_data(\"./dataset/train.txt\")\n",
    "print(len(data_before_preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the contents of the pickle file located at file_path \n",
    "    and append each line to the list data\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_words_in_file(path, words, permission='w'):\n",
    "    \"\"\"\n",
    "    Save the words in the file located at path \n",
    "    \"\"\"\n",
    "    with open(path, permission, encoding='utf-8') as file:\n",
    "            for word in words:\n",
    "                file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "{'ج', 'ؤ', 'ح', 'ة', 'ذ', 'ر', 'ش', 'ض', 'ف', 'غ', 'ب', 'س', 'ث', 'ظ', 'ئ', 'ت', 'ع', 'م', 'آ', 'ا', 'ص', 'ط', 'أ', 'ن', 'ل', 'ز', 'و', 'د', 'ق', 'ء', 'ى', 'ي', 'خ', 'ك', 'ه', 'إ'}\n"
     ]
    }
   ],
   "source": [
    "# set for arabic letters\n",
    "arabic_letters = set(read_pickle_file(\"./Delivery/arabic_letters.pickle\"))\n",
    "\n",
    "print(len(arabic_letters))\n",
    "print(arabic_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "{'ِ', 'ٌ', 'ُ', 'ً', 'ْ', 'ّ', 'َ', 'ٍ'}\n"
     ]
    }
   ],
   "source": [
    "# set for arabic letters\n",
    "diacritics = set(read_pickle_file(\"./Delivery/diacritics.pickle\"))\n",
    "\n",
    "print(len(diacritics))\n",
    "print(diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove diacritics\n",
    "def remove_diacritics(text):\n",
    "    text = araby.strip_tashkeel(text)\n",
    "    return text\n",
    "\n",
    "# Remove any letters not found in set arabic_letters and not found in set diacritics\n",
    "def remove_non_arabic(text):\n",
    "    text = re.sub(r'[^\\s' + ''.join(arabic_letters) + ''.join(diacritics) + ']', '', text)\n",
    "    return text\n",
    "\n",
    "def input_preprocessing_text(text):\n",
    "    # Correct most common errors on word like repetetion of harakats, or tanween before alef\n",
    "    text = araby.autocorrect(text)\n",
    "\n",
    "    # Remove any non-Arabic letters\n",
    "    text = remove_non_arabic(text)\n",
    "\n",
    "    # Remove diacritics\n",
    "    text = remove_diacritics(text)\n",
    "\n",
    "    # Tokenize\n",
    "    text = araby.tokenize(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def save_tokenized_input(text,path=\"./generatedFiles/training/tokenized_input.txt\", permission='w'):\n",
    "    words = input_preprocessing_text(text)\n",
    "    save_words_in_file(path, words, permission)\n",
    "    \n",
    "\n",
    "def save_gold_output(text,path=\"./generatedFiles/training/gold_output.txt\", permission='w'):\n",
    "    # Remove any non-Arabic letters and extra spaces\n",
    "    text = remove_non_arabic(text)\n",
    "\n",
    "    # Tokenize\n",
    "    text = araby.tokenize(text)\n",
    "\n",
    "    save_words_in_file(path, text, permission)\n",
    "\n",
    "\n",
    "def is_not_arabic_diacritic(char):\n",
    "   category = unicodedata.category(char)\n",
    "   return not (category == 'Mn' or category == 'Mc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character is an Arabic diacritic.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "character = 'ذْ'\n",
    "if is_not_arabic_diacritic(character[1]):\n",
    "   print(\"The character is not an Arabic diacritic.\")\n",
    "else:\n",
    "   print(\"The character is an Arabic diacritic.\")\n",
    "\n",
    "\n",
    "# Testing of is_not_arabic_diacritic() function with gettting the index of the first non diacritic character in the word\n",
    "word = 'زَّراع'\n",
    " \n",
    "for i in range(1, len(word)): # start from 1 because the first character is not a diacritic\n",
    "    if is_not_arabic_diacritic(word[i]):\n",
    "        print(i)\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN ONE TIME ONLY THIS CODE AGAIN \n",
    "# # Generate Gold Input file\n",
    "for i in range(len(data_before_preprocessing)):\n",
    "    save_tokenized_input(data_before_preprocessing[i], permission='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN ONE TIME ONLY THIS CODE AGAIN\n",
    "# # Generate Gold Output file\n",
    "for i in range(len(data_before_preprocessing)):\n",
    "    save_gold_output(data_before_preprocessing[i], permission='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important functions in PyArabic\n",
    "\n",
    "# araby.tokenize(text) # Tokenize the sentence text into words\n",
    "# araby.is_arabicrange(text) # Check if the text is Arabic\n",
    "# araby.sentence_tokenize(text) # Tokenize the text into sentences\n",
    "# araby.strip_tashkeel(text) # Remove diacritics (FATHA, DAMMA, KASRA, SUKUN, SHADDA, FATHATAN, DAMMATAN, KASRATAN)\n",
    "# araby.strip_diacritics(text) # Remove diacritics (Small Alef الألف الخنجرية, Harakat + Shadda, Quranic marks)\n",
    "# araby.strip_tatweel(text) # Remove tatweel\n",
    "# araby.strip_shadda(text) # Remove shadda\n",
    "# araby.autocorrect(text) # Correct most common errors on word like repetetion of harakats,or tanwin befor alef\n",
    "# araby.arabicrange() # Return a list of Arabic characters\n",
    "\n",
    "# New Functions in PyArabic\n",
    "# araby.vocalized_similarity(word1, word2) # if the two words has the same letters and the same harakats, this function return True. \n",
    "# The two words can be full vocalized, or partial vocalized\n",
    "\n",
    "# araby.vocalizedlike(word1, word2) Same as vocalized_similarity but return True and False\n",
    "\n",
    "# araby.joint(word1, word2) # joint the letters with the marks the length ot letters and marks must be equal return word\n",
    "\n",
    "\n",
    "\n",
    "# Return the text, its tashkeel and shadda if extract_shadda is True\n",
    "# text, marks, shada = araby.separate(text,extract_shadda=True) # Separate diacritics from the text\n",
    "# print (text)\n",
    "# for m in marks:\n",
    "#     print (araby.name(m))\n",
    "\n",
    "# for s in shada:\n",
    "#     print (araby.name(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n",
      "2101983\n"
     ]
    }
   ],
   "source": [
    "# Read tokenized_input file\n",
    "tokenized_input = read_data(\"./generatedFiles/training/tokenized_input.txt\")\n",
    "print(tokenized_input[:10])\n",
    "print(len(tokenized_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Core Word (CW) Diacritization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Characters: \n",
    "Here we extract each character from all tokenized words and create a vector of size 50 for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_char = Tokenizer(char_level=True)\n",
    "tokenizer_char.fit_on_texts(tokenized_input)\n",
    "sequences_char = tokenizer_char.texts_to_sequences(tokenized_input)\n",
    "char_features = pad_sequences(sequences_char)   # padding the sequences to have the same length as the longest sequence (word)\n",
    "char_embeddings = np.random.rand(len(tokenizer_char.word_index) + 1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2101983, 13)\n",
      "(37, 20)\n"
     ]
    }
   ],
   "source": [
    "print(char_features.shape) # (number of words, max length of word in the dataset)\n",
    "\n",
    "\n",
    "print(char_embeddings.shape)\n",
    "\n",
    "# 38 rows: 37 unique characters identified by the tokenizer, 1 row for handling characters not seen in the training data\n",
    "# 50 columns: Each character is encoded as a 50-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0 13  5  1  7]\n"
     ]
    }
   ],
   "source": [
    "print(char_features[0]) \n",
    "# the number of non zero elements corresponds to the length of the word \n",
    "# and the value of each element corresponds to the index of the character in the tokenizer\n",
    "# which means that every character now is encoded as a number and this number is the index of the character in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      " 0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      " 0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      " 0.54126672 0.89214072]\n"
     ]
    }
   ],
   "source": [
    "print(char_embeddings[0])\n",
    "# this is the embedding of each character in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 20)\n",
      "[[0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      "  0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      "  0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      "  0.54126672 0.89214072]\n",
      " [0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      "  0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      "  0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      "  0.54126672 0.89214072]\n",
      " [0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      "  0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      "  0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      "  0.54126672 0.89214072]\n",
      " [0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      "  0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      "  0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      "  0.54126672 0.89214072]\n",
      " [0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      "  0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      "  0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      "  0.54126672 0.89214072]\n",
      " [0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      "  0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      "  0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      "  0.54126672 0.89214072]\n",
      " [0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      "  0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      "  0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      "  0.54126672 0.89214072]\n",
      " [0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      "  0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      "  0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      "  0.54126672 0.89214072]\n",
      " [0.24331295 0.36023869 0.44320409 0.97995645 0.64139997 0.09802749\n",
      "  0.02166102 0.05060848 0.45108359 0.9962569  0.53666939 0.62983648\n",
      "  0.32140253 0.26110958 0.19620536 0.86482574 0.32375746 0.49414959\n",
      "  0.54126672 0.89214072]\n",
      " [0.84590661 0.14286382 0.36662366 0.65998295 0.6552849  0.11255097\n",
      "  0.76383735 0.88310913 0.72474443 0.55146199 0.48142269 0.99457569\n",
      "  0.27927859 0.45919595 0.19263906 0.0530254  0.94194767 0.67285972\n",
      "  0.80287228 0.55279982]\n",
      " [0.11257829 0.34814034 0.72005913 0.20465867 0.17181682 0.26056601\n",
      "  0.34033302 0.62990072 0.51013862 0.744088   0.20139161 0.34074223\n",
      "  0.56763505 0.2090345  0.32549431 0.90793541 0.34129149 0.10882777\n",
      "  0.29117328 0.90097432]\n",
      " [0.7121431  0.10461415 0.32727701 0.10829456 0.77780613 0.12137812\n",
      "  0.18169213 0.99618697 0.75537217 0.44148167 0.60523731 0.5675774\n",
      "  0.56779786 0.38561765 0.40919963 0.95095064 0.00352431 0.0486627\n",
      "  0.98241187 0.73144909]\n",
      " [0.24391439 0.49554503 0.98260124 0.82226203 0.96968255 0.52095906\n",
      "  0.2551672  0.48392057 0.131161   0.62827687 0.00280979 0.65693281\n",
      "  0.95963056 0.59160473 0.89915243 0.00928778 0.94100887 0.80637621\n",
      "  0.22325058 0.85959773]]\n"
     ]
    }
   ],
   "source": [
    "print(char_embeddings[char_features[0]].shape)\n",
    "# 13 is the word of characters and 50 is the embedding size of each character\n",
    "\n",
    "print(char_embeddings[char_features[0]])\n",
    "# this is the embedding of each character in the first tokenized word, this is the 1st feature and the input of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - The position of the character in a word segment:\n",
    "For example, given the word “wAlktAb” , which is composed of three segments “w+Al+ktAb”. Letters were marked as “B” if they begin a segment, “M” if they are in the middle of a segment, “E” if they end a segment, and “S” if they are single letter segments. So for “w+Al+ktAb”, the corresponding character positions are “S+BE+BMME.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-01 15:27:02,290 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "segmenter = FarasaSegmenter(interactive=True) # The default behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented word: ['ك', 'قلم', 'ه']\n",
      "SEG tags: ['S', 'B', 'M', 'E', 'S']\n"
     ]
    }
   ],
   "source": [
    "def get_seg_tags(word):                 # word = \"wAlktAb\"\n",
    "    segments = segmenter.segment(word)  # segments will be a list: [\"w\", \"Al\", \"ktAb\"]\n",
    "    segments = segments.split('+')\n",
    "    seg_tags = []\n",
    "    for segment in segments:\n",
    "        if len(segment) == 1:\n",
    "            seg_tags.append(\"S\")\n",
    "        else:\n",
    "            seg_tags.append(\"B\")  # First letter\n",
    "            seg_tags.extend(\"M\" * (len(segment) - 2))  # Middle letters\n",
    "            seg_tags.append(\"E\")  # Last letter\n",
    "    return segments, seg_tags\n",
    "\n",
    "word = \"كقلمه\"\n",
    "segments, seg_tags = get_seg_tags(word)\n",
    "print(\"Segmented word:\", segments)\n",
    "print(\"SEG tags:\", seg_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DON'T RUN THIS CODE AGAIN, THIS CELL TOOK 25m 4.5s TO RUN\n",
    "# # The Output of this code is the input_segments.txt file\n",
    "\n",
    "for i in range(len(tokenized_input)):\n",
    "    segments, seg_tags = get_seg_tags(tokenized_input[i])\n",
    "    # Write and append on the tokenized input to a file\n",
    "    with open('./generatedFiles/training/input_segments.txt', 'a', encoding='utf-8') as file:\n",
    "        for tag in seg_tags:\n",
    "            file.write(tag)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['BMES', 'BE', 'BME', 'BEBME', 'BES', 'BME', 'BME', 'BEBMMME', 'BME', 'BMES']\n"
     ]
    }
   ],
   "source": [
    "# Read the input_segments file\n",
    "with open('./generatedFiles/input_segments.txt', 'r', encoding='utf-8') as file:\n",
    "    input_segments = file.readlines()\n",
    "    print(len(input_segments))\n",
    "    # Remove '\\n' from each line\n",
    "    input_segments = [line.strip() for line in input_segments]\n",
    "    # Put the tokenized input of length 1 in the tokenized_input list\n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "\n",
    "print(input_segments[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tags = Tokenizer(char_level=True)\n",
    "tokenizer_tags.fit_on_texts(input_segments)\n",
    "sequences_tags = tokenizer_tags.texts_to_sequences(input_segments)\n",
    "tags_features = pad_sequences(sequences_tags)   \n",
    "tags_embeddings = np.random.rand(len(tokenizer_tags.word_index) + 1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2104308, 13)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "print(tags_features.shape) \n",
    "print(tags_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 2, 4], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.66677583e-01, 8.86454123e-01, 1.13832792e-02, 3.70831116e-01,\n",
       "       3.31769904e-01, 2.90807994e-01, 4.11001010e-01, 6.76911686e-02,\n",
       "       4.15257775e-01, 7.36223802e-02, 3.28225494e-03, 4.32367067e-01,\n",
       "       6.67660097e-01, 1.80724637e-01, 3.30664200e-01, 7.22262214e-02,\n",
       "       5.22613413e-01, 1.06997034e-01, 5.82355531e-01, 6.31027943e-01,\n",
       "       8.57333603e-01, 4.44430890e-01, 1.91273319e-01, 3.19766491e-01,\n",
       "       9.03579279e-01, 3.59271829e-01, 4.04055775e-01, 5.40305776e-02,\n",
       "       4.18897447e-01, 3.00678619e-01, 4.36432881e-01, 1.08191547e-01,\n",
       "       9.46181705e-01, 2.49636459e-01, 5.60390077e-01, 3.31735707e-01,\n",
       "       7.98341826e-01, 7.80804634e-01, 3.05748368e-01, 1.67359678e-01,\n",
       "       4.21394462e-01, 1.25127367e-01, 7.01076676e-01, 8.34157794e-01,\n",
       "       2.38780978e-01, 6.28024653e-01, 2.96880331e-01, 6.74303259e-04,\n",
       "       8.45280411e-01, 9.92803073e-01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_embeddings[tags_features[0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - PRIOR: \n",
    "diacritics seen in the training set per segment. Since we used a character-level model, this feature informed the model with word-level information. For example, the word “ktAb”  was observed to have two diacritized forms in the training set, namely “kitaAb” ( – book) and “kut∼aAb” ( – writers). The first letter in the word (“k”) accepted the diacritics “i” and “u.” Thus, given a binary vector representing whether a character is allowed to assume any of the eight primitive Arabic diacritic marks (a, i, u, o, K, N, F, and ∼ in order), the first letter would be given the following vector “01100000.” If a word segment was never observed during training, then the vector for all letters therein would be set to 11111111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قَوْلُهُ', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', 'قَالَ', 'الزَّرْكَشِيُّ', 'ابْنُ', 'عَرَفَةَ']\n"
     ]
    }
   ],
   "source": [
    "# read the gold_output file\n",
    "with open('./generatedFiles/gold_output.txt', 'r', encoding='utf-8') as file:\n",
    "    gold_output = file.readlines()\n",
    "    print(len(gold_output))\n",
    "    # remove '\\n' from each line\n",
    "    gold_output = [line.strip() for line in gold_output]\n",
    "    # put in tokenized_input list the tokenized input of length 1\n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "    # get the inde\n",
    "\n",
    "print(gold_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_input))\n",
    "print(tokenized_input[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each diacrtics to its unicode\n",
    "diacritics_mapping = {\n",
    "    'FATHA': '\\u064E',\n",
    "    'DAMMA': '\\u064F',\n",
    "    'KASRA': '\\u0650',\n",
    "    'SHADDA': '\\u0651',\n",
    "    'SUKUN': '\\u0652',\n",
    "    'FATHATAN': '\\u064B',\n",
    "    'DAMMATAN': '\\u064C',\n",
    "    'KASRATAN': '\\u064D'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract diacritics by returning a list containing a tuple of 3 elements: (letter, tashkeel, shadda)\n",
    "# def extract_arabic_diacritics(word):\n",
    "#     diacritics_list = []\n",
    "#     extracted_word, tashkeel, shadda = araby.separate(word, extract_shadda=True)\n",
    "#     for i in range(len(extracted_word)):\n",
    "#         print(f'{araby.name(extracted_word[i])} {araby.name(tashkeel[i])} {araby.name(shadda[i])}')\n",
    "#         diacritics_list.append((extracted_word[i], (tashkeel[i].encode(\"utf8\")).decode(), (shadda[i].encode(\"utf8\")).decode()))\n",
    "#     return diacritics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # firstly, initialize an empty dictionary for the 'prior' feature\n",
    "# # the value will be the 8 arabic marks (FATHA, DAMMA, KASRA, FATHATAN, DAMMATAN, KASRATAN, SUKUN, SHADDA) as a binary vector\n",
    "\n",
    "# # then, loop over the tokenized input and check if the each character and word pair is not in the dictionary, get the indices of this word and its duplicates in the tokenized input array\n",
    "# def get_prior(tokenized_input, gold_output):\n",
    "#     prior = {} # this dictionary will hold a key of tuple of 3 elements (word, character, index of character in the word) and the value will be the 8 arabic marks\n",
    "#     for i in range(len(tokenized_input)):\n",
    "#         if (tokenized_input[i], tokenized_input[i][0], 0) not in prior:\n",
    "#             # get the indices of the word in the tokenized input array\n",
    "#             indices = [j for j, x in enumerate(tokenized_input) if x == tokenized_input[i]]\n",
    "#             print(indices)\n",
    "#             # get the words in the gold_output array with the same indices\n",
    "#             words = [gold_output[j] for j in indices]\n",
    "#             extracted_diac_all_words = []\n",
    "#             for word in words:\n",
    "#                 extracted_diac_all_words.append(extract_arabic_diacritics(word))\n",
    "#             for indx, charac in enumerate(tokenized_input[i]):\n",
    "#                 for extracted_diac_per_word in extracted_diac_all_words:\n",
    "#                     # extract the diacritics of word[indx]\n",
    "#                     prior[(tokenized_input[i], charac, indx)] = [0, 0, 0, 0, 0, 0, 0, 0] # initialize the value of the key with zeros\n",
    "#                     if diacritics_mapping['SHADDA'] in extracted_diac_per_word[indx]:\n",
    "#                         prior[(tokenized_input[i], charac, indx)][4] = 1 if diacritics_mapping['FATHA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac, indx)][5] = 1 if diacritics_mapping['DAMMA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac, indx)][6] = 1 if diacritics_mapping['KASRA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac, indx)][7] = 1 if not  diacritics_mapping['FATHA'] in extracted_diac_per_word[indx] and not diacritics_mapping['DAMMA'] in word[indx: indx+2]  and not diacritics_mapping['KASRA'] in word[indx: indx+2] else 0\n",
    "#                     else:\n",
    "#                         prior[(tokenized_input[i], charac,indx)][0] = 1 if diacritics_mapping['FATHA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac,indx)][1] = 1 if diacritics_mapping['DAMMA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac,indx)][2] = 1 if diacritics_mapping['KASRA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac,indx)][3] = 1 if diacritics_mapping['SUKUN'] in extracted_diac_per_word[indx] else 0\n",
    "#     return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FATHA in tashkeel:  True\n",
      "DAMMA in tashkeel:  False\n",
      "KASRA in tashkeel:  False\n",
      "SUKUN in tashkeel:  True\n",
      "FATHATAN in tashkeel:  False\n",
      "DAMMATAN in tashkeel:  False\n",
      "KASRATAN in tashkeel:  False\n",
      "SHADDA in tashkeel:  False\n",
      "=============================\n",
      "FATHA in shadda:  False\n",
      "DAMMA in shadda:  False\n",
      "KASRA in shadda:  False\n",
      "SUKUN in shadda:  False\n",
      "FATHATAN in shadda:  False\n",
      "DAMMATAN in shadda:  False\n",
      "KASRATAN in shadda:  False\n",
      "SHADDA in shadda:  True\n",
      "testt False\n",
      "yarab False\n"
     ]
    }
   ],
   "source": [
    "letter, tashkeel, shadda = araby.separate('زَّ', extract_shadda=True)   # SHADDA + FATHA Example\n",
    "# letter, tashkeel, shadda = araby.separate('وَ', extract_shadda=True)   # FATHA Example\n",
    "# letter, tashkeel, shadda = araby.separate('مً', extract_shadda=True)   # FATHATAN Example\n",
    "# letter, tashkeel, shadda = araby.separate('عٌ', extract_shadda=True)   # DAMMATAN Example\n",
    "# letter, tashkeel, shadda = araby.separate('يُّ', extract_shadda=True)   # SHADDA + DAMMA Example\n",
    "# letter, tashkeel, shadda = araby.separate('ذْ', extract_shadda=True)   # SUKUN Example\n",
    "enkar = 'كَإِنْكَارِ'\n",
    "# print(enkar[4:6])\n",
    "# print( diacritics_mapping['FATHA'] in enkar[0:1])\n",
    "# print( diacritics_mapping['SHADDA'] in 'زَّ')\n",
    "# print( diacritics_mapping['DAMMA'] in 'زَّ')\n",
    "\n",
    "print('FATHA in tashkeel: ', diacritics_mapping['FATHA'] in tashkeel)\n",
    "print('DAMMA in tashkeel: ', diacritics_mapping['DAMMA'] in tashkeel)\n",
    "print('KASRA in tashkeel: ', diacritics_mapping['KASRA'] in tashkeel)\n",
    "print('SUKUN in tashkeel: ', diacritics_mapping['SUKUN'] in tashkeel)\n",
    "print('FATHATAN in tashkeel: ', diacritics_mapping['FATHATAN'] in tashkeel)\n",
    "print('DAMMATAN in tashkeel: ', diacritics_mapping['DAMMATAN'] in tashkeel)\n",
    "print('KASRATAN in tashkeel: ', diacritics_mapping['KASRATAN'] in tashkeel)\n",
    "print('SHADDA in tashkeel: ', diacritics_mapping['SHADDA'] in tashkeel)\n",
    "print('=============================')\n",
    "print('FATHA in shadda: ', diacritics_mapping['FATHA'] in shadda)\n",
    "print('DAMMA in shadda: ', diacritics_mapping['DAMMA'] in shadda)\n",
    "print('KASRA in shadda: ', diacritics_mapping['KASRA'] in shadda)\n",
    "print('SUKUN in shadda: ', diacritics_mapping['SUKUN'] in shadda)\n",
    "print('FATHATAN in shadda: ', diacritics_mapping['FATHATAN'] in shadda)\n",
    "print('DAMMATAN in shadda: ', diacritics_mapping['DAMMATAN'] in shadda)\n",
    "print('KASRATAN in shadda: ', diacritics_mapping['KASRATAN'] in shadda)\n",
    "print('SHADDA in shadda: ', diacritics_mapping['SHADDA'] in shadda)\n",
    "\n",
    "print('testt', (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['FATHA'] not in tashkeel and diacritics_mapping['DAMMA'] not in tashkeel and diacritics_mapping['KASRA'] not in tashkeel))\n",
    "print('yarab', (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['SHADDA'] not in shadda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstly, initialize an empty dictionary for the 'prior' feature\n",
    "# the value will be the 8 arabic marks (FATHA, DAMMA, KASRA, FATHATAN, DAMMATAN, KASRATAN, SUKUN, SHADDA) as a binary vector\n",
    "\n",
    "# then, loop over the tokenized input and check if the each character and word pair is not in the dictionary, get the indices of this word and its duplicates in the tokenized input array\n",
    "def get_prior(tokenized_input, gold_output):\n",
    "    prior = {}  # this dictionary will hold a key of tuple of 3 elements (word, character, index of character in the word) and the value will be the 8 arabic marks\n",
    "    for i in range(len(tokenized_input)):\n",
    "        if (tokenized_input[i], tokenized_input[i][0], 0) not in prior:\n",
    "            # get the indices of the word in the tokenized input array\n",
    "            indices = [j for j, x in enumerate(tokenized_input) if x == tokenized_input[i]]\n",
    "            # print(indices)\n",
    "            # get the words in the gold_output array with the same indices\n",
    "            words = []\n",
    "            maxi_len = 0\n",
    "            for j in indices:\n",
    "                if gold_output[j] not in words:\n",
    "                    words.append(gold_output[j])\n",
    "                    maxi_len = max(maxi_len, len(gold_output[j]))\n",
    "\n",
    "            for t in range(len(tokenized_input[i])):\n",
    "                prior[(tokenized_input[i], tokenized_input[i][t], t)] = [0, 0, 0, 0, 0, 0, 0, 0] # initialize the value of the key with zeros\n",
    "            \n",
    "            indx2 = 0\n",
    "            for word in words:\n",
    "                indx = 0\n",
    "                while indx < maxi_len:\n",
    "                    # extract the diacritics of word[indx]\n",
    "                    for iter in range(indx+1, len(word)):\n",
    "                        if is_not_arabic_diacritic(word[iter]):\n",
    "                            # print(iter)\n",
    "                            letter, tashkeel, shadda = araby.separate(word[indx: iter], extract_shadda=True) \n",
    "                            if diacritics_mapping['FATHA'] in tashkeel:         prior[(tokenized_input[i], word[indx], indx2)][0] = 1 \n",
    "                            if diacritics_mapping['DAMMA'] in tashkeel:         prior[(tokenized_input[i], word[indx], indx2)][1] = 1\n",
    "                            if diacritics_mapping['KASRA'] in tashkeel:         prior[(tokenized_input[i], word[indx], indx2)][2] = 1\n",
    "                            if diacritics_mapping['FATHATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], indx2)][3] = 1\n",
    "                            if diacritics_mapping['DAMMATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], indx2)][4] = 1\n",
    "                            if diacritics_mapping['KASRATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], indx2)][5] = 1\n",
    "                            if (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['SHADDA'] not in shadda):  \n",
    "                                prior[(tokenized_input[i], word[indx], indx2)][6] = 1 # if the letter has SHADDA, araby.separate() will return SUKUN in tashkeel and SHADDA in shadda, so to avoid this mislabeling we check if SHADDA not in shadda and if SUKUN in tashkeel, then this is a true SUKUN\n",
    "                            if diacritics_mapping['SHADDA'] in shadda:          prior[(tokenized_input[i], word[indx], indx2)][7] = 1\n",
    "                            indx = iter - 1\n",
    "                            indx2 += 1\n",
    "                            break \n",
    "                    indx += 1\n",
    "                indx2 = 0\n",
    "\n",
    "\n",
    "                indx = len(word) - 1    # my assumption is that the last character in the not a diacritic\n",
    "                if (not is_not_arabic_diacritic(word[len(word) - 1]) and is_not_arabic_diacritic(word[len(word) - 2])):  # if the last character is a diacritic and the one before it is not, then the index of the last character is len(word) - 2\n",
    "                    indx = len(word) - 2\n",
    "                elif (not is_not_arabic_diacritic(word[len(word) - 1]) and not is_not_arabic_diacritic(word[len(word) - 2])):  # if the last character is a diacritic and the one before it is also a diacritic (in shadda case), then the index of the last character is len(word) - 3\n",
    "                    indx = len(word) - 3\n",
    "\n",
    "\n",
    "                if (tokenized_input[i], word[indx], indx) not in prior:\n",
    "                    letter, tashkeel, shadda = araby.separate(word[indx: len(word)], extract_shadda=True) \n",
    "                    if diacritics_mapping['FATHA'] in tashkeel:         prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][0] = 1\n",
    "                    if diacritics_mapping['DAMMA'] in tashkeel:         prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][1] = 1\n",
    "                    if diacritics_mapping['KASRA'] in tashkeel:         prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][2] = 1 \n",
    "                    if diacritics_mapping['FATHATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][3] = 1 \n",
    "                    if diacritics_mapping['DAMMATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][4] = 1 \n",
    "                    if diacritics_mapping['KASRATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][5] = 1\n",
    "                    if (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['SHADDA'] not in shadda):\n",
    "                        prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][6] = 1  # if the letter has SHADDA, araby.separate() will return SUKUN in tashkeel and SHADDA in shadda, so to avoid this mislabeling we check if SHADDA not in shadda and if SUKUN in tashkeel, then this is a true SUKUN\n",
    "                    if diacritics_mapping['SHADDA'] in shadda:          prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][7] = 1\n",
    "                    \n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('كإنكار', 'ك', 0): [1, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'إ', 1): [0, 0, 1, 0, 0, 0, 0, 0], ('كإنكار', 'ن', 2): [0, 0, 0, 0, 0, 0, 1, 0], ('كإنكار', 'ك', 3): [1, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'ا', 4): [0, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'ر', 5): [0, 0, 1, 0, 0, 1, 0, 0], ('بقذر', 'ب', 0): [0, 0, 1, 0, 0, 0, 0, 0], ('بقذر', 'ق', 1): [1, 0, 0, 0, 0, 0, 0, 0], ('بقذر', 'ذ', 2): [1, 0, 0, 0, 0, 0, 0, 0], ('بقذر', 'ر', 3): [0, 0, 0, 0, 0, 1, 0, 0], ('أكثر', 'أ', 0): [1, 0, 0, 0, 0, 0, 0, 0], ('أكثر', 'ك', 1): [0, 0, 0, 0, 0, 0, 1, 0], ('أكثر', 'ث', 2): [1, 0, 0, 0, 0, 0, 0, 0], ('أكثر', 'ر', 3): [1, 1, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ا', 0): [0, 0, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ل', 1): [0, 0, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ز', 2): [1, 0, 0, 0, 0, 0, 0, 1], ('الزركشي', 'ر', 3): [0, 0, 0, 0, 0, 0, 1, 0], ('الزركشي', 'ك', 4): [1, 0, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ش', 5): [0, 0, 1, 0, 0, 0, 0, 0], ('الزركشي', 'ي', 6): [0, 1, 0, 0, 0, 0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "test_tokenized_input = ['كإنكار', 'كإنكار', 'بقذر','بقذر', 'أكثر', 'أكثر', 'الزركشي']\n",
    "test_gold_output = ['كَإِنْكَارِ','كَإِنْكَارٍ', 'بِقَذَر', 'بِقَذَرٍ','أكْثَرَ', 'أَكْثَرُ', 'الزَّرْكَشِيُّ']\n",
    "print (get_prior(test_tokenized_input, test_gold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DON'T RUN THIS CODE AGAIN, THIS CELL TOOK 276 minutes TO RUN\n",
    "# # write in a file the prior feature\n",
    "# prior_feature = get_prior(tokenized_input, gold_output)\n",
    "# with open('./generatedFiles/prior_feature.txt', 'w', encoding='utf-8') as file:\n",
    "#     for key, value in prior_feature.items():\n",
    "#         file.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# read the prior feature file in a dictionary called prior_feature \n",
    "prior_feature = {}\n",
    "with open('./generatedFiles/prior_feature.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(':')\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        key = key[1:-1].split(',')\n",
    "        value = value[1:-1].split(',')\n",
    "        key = (key[0][1:-1], key[1][2:-1], int(key[2]))\n",
    "        value = [int(i) for i in value]\n",
    "        prior_feature[key] = value\n",
    "\n",
    "print(prior_feature[('قوله', 'ق', 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - CASE Feature: \n",
    "whether the letter expects a core word diacritic or a case ending. Case endings are placed on only one letter in a word, which may or may not be the last letter in the word. This is a binary feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-01 00:47:22,675 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: الكتابة باللغة العربية\n",
      "Stemmed text: كتابة لغة عربي\n"
     ]
    }
   ],
   "source": [
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "def arabic_stemmer(text):\n",
    "    stemmer = FarasaStemmer(interactive=True)  # Set interactive to True for better performance\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmed_text = stemmer.stem(text)\n",
    "\n",
    "    return stemmed_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"الكتابة باللغة العربية\"\n",
    "stemmed_text = arabic_stemmer(input_text)\n",
    "print(\"Original text:\", input_text)\n",
    "print(\"Stemmed text:\", stemmed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     stemmed_text = arabic_stemmer(tokenized_input[i])\n",
    "#     print(\"Original text:\", tokenized_input[i])\n",
    "#     print(\"Stemmed text:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(tokenized_input)):\n",
    "#     stemmed_text = arabic_stemmer(tokenized_input[i])\n",
    "#     # Write and append on the tokenized input to a file\n",
    "#     with open('./generatedFiles/stemmed_input.txt', 'a', encoding='utf-8') as file:\n",
    "#         file.write(stemmed_text + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed_text = stemmed_text.split(' ')\n",
    "# # write in a file the stemmed input\n",
    "# with open('./generatedFiles/stemmed_input.txt', 'w', encoding='utf-8') as file:\n",
    "#     for word in stemmed_text:\n",
    "#         file.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8353805, 150)\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL TOOK 12 minutes TO RUN\n",
    "# input layer\n",
    "char_features_vector=[]\n",
    "tag_features_vector=[]\n",
    "prior_features_vector=[]\n",
    "embeddings = []\n",
    "# with open('./generatedFiles/i_j.txt', 'a', encoding='utf-8') as file:\n",
    "for i in range(len(tokenized_input)):\n",
    "    for j in range(len(tokenized_input[i])):    \n",
    "        # write i and j in a file\n",
    "        # file.write(f'{i} {j}\\n')\n",
    "        char_index = tokenizer_char.word_index.get(tokenized_input[i][j])\n",
    "        char_features_vector= char_embeddings[char_index]\n",
    "        if (len(tokenized_input[i]) != len(input_segments[i])):\n",
    "            input_segments[i] = \"S\" * (len(tokenized_input[i]) - len(input_segments[i])) + input_segments[i]\n",
    "        tag_index = tokenizer_tags.word_index.get(input_segments[i][j].lower())\n",
    "        tag_features_vector= tags_embeddings[tag_index]\n",
    "        prior_features_vector= prior_feature[(tokenized_input[i], tokenized_input[i][j], j)]\n",
    "        # pad the prior feature vector with zeros to have the same length as the other features\n",
    "        prior_features_vector = np.pad(prior_features_vector, (0, 42), 'constant')\n",
    "        # concatenate the 3 features vectors to have a matrix of 3 columns\n",
    "        # embeddings.append(np.vstack((char_features_vector, tag_features_vector, prior_features_vector)))\n",
    "        embeddings.append(np.concatenate((char_features_vector, tag_features_vector, prior_features_vector)))\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# print(char_features_vector)\n",
    "# print(tag_features_vector)\n",
    "# print(prior_features_vector)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embeddings in a pickle file\n",
    "with open('./generatedFiles/embeddings.pickle', 'wb') as file:\n",
    "    pickle.dump(embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the embeddings from the pickle file\n",
    "with open('./generatedFiles/embeddings.pickle', 'rb') as file:\n",
    "    embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_map = {\n",
    "    (1, 0, 0, 0, 0, 0, 0, 0) : 0, # FATHA\n",
    "    (0, 0, 0, 1, 0, 0, 0, 0) : 1, # FATHATAN\n",
    "    (0, 0, 1, 0, 0, 0, 0, 0) : 2, # KASRA\n",
    "    (0, 0, 0, 0, 0, 1, 0, 0) : 3, # KASRATAN\n",
    "    (0, 1, 0, 0, 0, 0, 0, 0) : 4, # DAMMA\n",
    "    (0, 0, 0, 0, 1, 0, 0, 0) : 5, # DAMMATAN\n",
    "    (0, 0, 0, 0, 0, 0, 1, 0) : 6, # SUKUN\n",
    "    (0, 0, 0, 0, 0, 0, 0, 1) : 7,  # SHADDA\n",
    "    (1, 0, 0, 0, 0, 0, 0, 1) : 8, # SHADDA FATHA\n",
    "    (0, 0, 0, 1, 0, 0, 0, 1) : 9, # SHADDA FATHATAN\n",
    "    (0, 0, 1, 0, 0, 0, 0, 1) : 10, # SHADDA KASRA\n",
    "    (0, 0, 0, 0, 0, 1, 0, 1) : 11, # SHADDA KASRATAN\n",
    "    (0, 1, 0, 0, 0, 0, 0, 1) : 12, # SHADDA DAMMA\n",
    "    (0, 0, 0, 0, 1, 0, 0, 1) : 13, # SHADDA DAMMATAN\n",
    "    (0, 0, 0, 0, 0, 0, 0, 0) : 14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_prior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./generatedFiles/gold_output_dict.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gold_output):\n\u001b[0;32m----> 4\u001b[0m         gold_diacritics \u001b[38;5;241m=\u001b[39m \u001b[43mget_prior\u001b[49m([tokenized_input[idx]], [word])\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m gold_diacritics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m             key \u001b[38;5;241m=\u001b[39m key \u001b[38;5;241m+\u001b[39m (idx,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_prior' is not defined"
     ]
    }
   ],
   "source": [
    "# gold labels\n",
    "with open('./generatedFiles/gold_output_dict.txt', 'w', encoding='utf-8') as file:\n",
    "    for idx, word in enumerate(gold_output):\n",
    "        gold_diacritics = get_prior([tokenized_input[idx]], [word])\n",
    "        for key, value in gold_diacritics.items():\n",
    "            key = key + (idx,)\n",
    "            file.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prior feature file in a dictionary called prior_feature \n",
    "gold_output_dict = {}\n",
    "with open('./generatedFiles/gold_output_dict.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(':')\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        key = key[1:-1].split(',')\n",
    "        value = value[1:-1].split(',')\n",
    "        key = (key[0][1:-1], key[1][2:-1], int(key[2]), int(key[3]))\n",
    "        value = [int(i) for i in value]\n",
    "        gold_output_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('قوله', 'ق', 0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgold_output_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mقوله\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mق\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('قوله', 'ق', 0, 0)"
     ]
    }
   ],
   "source": [
    "gold_output_dict[('قوله', 'ق', 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change gold_output_dict.values() to a list of tuples\n",
    "for key, value in gold_output_dict.items():\n",
    "    gold_output_dict[key] = tuple(value)\n",
    "    \n",
    "gold_output_dict_values = list(gold_output_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./generatedFiles/gold_output_id.txt', 'w', encoding='utf-8') as file:\n",
    "    for value in gold_output_dict_values:\n",
    "        file.write(f'{output_map[value]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the gold_output_id file\n",
    "with open('./generatedFiles/gold_output_id.txt', 'r', encoding='utf-8') as file:\n",
    "    gold_output_id = file.readlines()\n",
    "    gold_output_id = [line.strip() for line in gold_output_id]\n",
    "\n",
    "gold_output_id = np.array(gold_output_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8353805,)\n",
      "['0' '6' '4' '4' '0' '6' '0' '0' '0' '14']\n"
     ]
    }
   ],
   "source": [
    "print(gold_output_id.shape)\n",
    "print(gold_output_id[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8353805\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate emdeddings to have the 8353000\n",
    "embeddings_reshape = embeddings[:4500000]\n",
    "gold_output_id = gold_output_id[:4500000]\n",
    "\n",
    "# Make it np array \n",
    "embeddings_reshape = np.array(embeddings_reshape)\n",
    "gold_output_id = np.array(gold_output_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500000, 150)\n",
      "(4500000,)\n"
     ]
    }
   ],
   "source": [
    "print (embeddings_reshape.shape)\n",
    "print (gold_output_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape embeddings to have 3 dimensions \n",
    "embeddings_reshape = embeddings_reshape.reshape((-1, 1000, 150))\n",
    "gold_output_id_reshape = gold_output_id.reshape(-1, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 1000, 150)\n",
      "(4500, 1000, 1)\n",
      "[['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['8']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['12']\n",
      " ['14']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['3']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['3']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['4']\n",
      " ['2']\n",
      " ['6']\n",
      " ['14']\n",
      " ['14']\n",
      " ['10']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['1']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['3']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['3']\n",
      " ['0']\n",
      " ['0']\n",
      " ['10']\n",
      " ['4']\n",
      " ['8']\n",
      " ['14']\n",
      " ['3']\n",
      " ['14']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['3']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['6']\n",
      " ['3']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['12']\n",
      " ['8']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['8']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['12']\n",
      " ['4']\n",
      " ['14']\n",
      " ['2']\n",
      " ['2']\n",
      " ['14']\n",
      " ['8']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['3']\n",
      " ['4']\n",
      " ['0']\n",
      " ['8']\n",
      " ['5']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['3']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['8']\n",
      " ['14']\n",
      " ['14']\n",
      " ['8']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['5']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['8']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['5']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['10']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['10']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['8']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['8']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['3']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['1']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['8']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['5']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['8']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['8']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['14']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['8']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['2']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['14']\n",
      " ['12']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['4']\n",
      " ['6']\n",
      " ['5']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['3']\n",
      " ['4']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['5']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['8']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['1']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['8']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['5']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['3']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['5']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['2']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['4']\n",
      " ['2']\n",
      " ['10']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['2']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['3']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['4']\n",
      " ['2']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['8']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['5']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['10']\n",
      " ['14']\n",
      " ['6']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['10']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['8']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['12']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['12']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['12']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['1']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['4']\n",
      " ['0']\n",
      " ['10']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Print Shap\n",
    "#e\n",
    "print(embeddings_reshape.shape)\n",
    "print(gold_output_id_reshape.shape)\n",
    "\n",
    "# print the first 10 rows of the embeddings\n",
    "# print(embeddings[:10])\n",
    "\n",
    "# print the first 10 rows of the gold_output_id\n",
    "print(gold_output_id_reshape[:][0])\n",
    "\n",
    "# print the first 10 columns of the gold_output_id\n",
    "print(tf.keras.utils.to_categorical(gold_output_id_reshape[:][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirection  (None, 1000, 100)         80400     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 1000, 100)         10100     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 1000, 15)          1515      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 92015 (359.43 KB)\n",
      "Trainable params: 92015 (359.43 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build a training model, first we need input layer that take matrix \"embeddings\" as an input with dropout of 10%\n",
    "# then we need a bidirectional LSTM layer with 100 units\n",
    "# then we need a dense layer with 100 units and relu activation function\n",
    "# then we need an output layer with 14 units and softmax activation function\n",
    "# use early stopping with patience of five epochs, a learning rate of 0.001, a batch size of 256, and an Adamax optimizer\n",
    "\n",
    "# define the model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.activations import relu,linear\n",
    "\n",
    "input_shape = (1000, 150)\n",
    "    # tf.keras.layers.Input(shape=150),\n",
    "    # Dropout(0.1),\n",
    "    # Bidirectional(LSTM(100)),\n",
    "    # Dense(100, activation='relu'),\n",
    "    # Dense(14, activation='softmax')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True), input_shape=input_shape))\n",
    "model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "model.add(TimeDistributed(Dense(15, activation='softmax')))\n",
    "\n",
    "# model = Sequential()\n",
    "# forward_layer = LSTM(50)\n",
    "# backward_layer = LSTM(50, activation='relu', go_backwards=True)\n",
    "# model.add(Bidirectional(forward_layer, backward_layer=backward_layer, input_shape=(1,150)))\n",
    "# # model.add(Dropout(0.1))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(14, activation='softmax'))\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.build(reshaped_matrix.shape)\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(gold_output_id_reshape)):\n",
    "    for j in range(1000):\n",
    "        labels.append( tf.keras.utils.to_categorical(gold_output_id_reshape[i][j], num_classes=15))\n",
    "        \n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 1000, 15)\n"
     ]
    }
   ],
   "source": [
    "labels = labels.reshape(-1, 1000, 15)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 25s 1s/step - loss: 2.2714 - accuracy: 0.3096\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8047 - accuracy: 0.3507\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.6972 - accuracy: 0.3507\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.6122 - accuracy: 0.3925\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.5010 - accuracy: 0.4886\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.3659 - accuracy: 0.6000\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.2242 - accuracy: 0.6574\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.0878 - accuracy: 0.7066\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.9597 - accuracy: 0.7376\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.8491 - accuracy: 0.7586\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.7577 - accuracy: 0.7759\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6817 - accuracy: 0.7934\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6184 - accuracy: 0.8070\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.5659 - accuracy: 0.8175\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.5224 - accuracy: 0.8280\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.4865 - accuracy: 0.8376\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.4568 - accuracy: 0.8451\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.4319 - accuracy: 0.8512\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.4108 - accuracy: 0.8562\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3932 - accuracy: 0.8606\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3780 - accuracy: 0.8636\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3648 - accuracy: 0.8664\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3535 - accuracy: 0.8689\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3435 - accuracy: 0.8717\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3349 - accuracy: 0.8741\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3275 - accuracy: 0.8764\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3207 - accuracy: 0.8789\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3146 - accuracy: 0.8810\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3091 - accuracy: 0.8828\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.3042 - accuracy: 0.8844\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2994 - accuracy: 0.8859\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2952 - accuracy: 0.8873\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2912 - accuracy: 0.8886\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2875 - accuracy: 0.8898\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2839 - accuracy: 0.8911\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.2804 - accuracy: 0.8923\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2772 - accuracy: 0.8935\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2741 - accuracy: 0.8946\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.2713 - accuracy: 0.8957\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2685 - accuracy: 0.8967\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2658 - accuracy: 0.8978\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2633 - accuracy: 0.8988\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2609 - accuracy: 0.8997\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.2586 - accuracy: 0.9006\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.2563 - accuracy: 0.9015\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2541 - accuracy: 0.9023\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2523 - accuracy: 0.9030\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2502 - accuracy: 0.9038\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2483 - accuracy: 0.9045\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 23s 1s/step - loss: 0.2465 - accuracy: 0.9052\n"
     ]
    }
   ],
   "source": [
    "# fit the model on the training dataset and evaluate it on the validation dataset,\n",
    "# use early stopping with patience of five epochs, a learning rate of 0.001, a batch size of 256\n",
    "# before that, configure the model to use GPU\n",
    "\n",
    "# fit the model with gpu\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(embeddings_reshape, labels, epochs=50, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model.save('./generatedFiles/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Specify the file path\n",
    "file_path = \"./test/test_no_diacritics.txt\"\n",
    "\n",
    "# Read the contents of the file located at file_path \n",
    "# and append each line to the list data_before_preprocessing\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    val_data_before_preprocessing = file.readlines()\n",
    "    # remove '\\n' from each line\n",
    "    val_data_before_preprocessing = [line.strip() for line in val_data_before_preprocessing]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_data_before_preprocessing)):\n",
    "    save_tokenized_input(val_data_before_preprocessing[i],\"test_tokenized_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_data_before_preprocessing)):\n",
    "    save_test_tokenized_input(val_data_before_preprocessing[i], i ,\"test_line_numb_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104898\n"
     ]
    }
   ],
   "source": [
    "# Read the tokenized input file\n",
    "# read the tokenized input file\n",
    "with open('./generatedFiles/test_tokenized_input.txt', 'r', encoding='utf-8') as file:\n",
    "    val_tokenized_input = file.readlines()\n",
    "    # Remove '\\n' from each line\n",
    "    val_tokenized_input = [line.strip() for line in val_tokenized_input]\n",
    "    \n",
    "print(len(val_tokenized_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_data_before_preprocessing)):\n",
    "    save_gold_output(val_data_before_preprocessing[i],\"val_gold_output\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>line_number</th>\n",
       "      <th>letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>س</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>ل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>ل</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  line_number letter\n",
       "0   0            0      ل\n",
       "1   1            0      ي\n",
       "2   2            0      س\n",
       "3   3            0      ل\n",
       "4   4            0      ل"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read test_set_without_labels.csv file \n",
    "test_set_without_labels = pd.read_csv('./test/test_set_without_labels.csv', encoding='utf-8')\n",
    "test_set_without_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the gold_output file\n",
    "with open('./generatedFiles/val_gold_output.txt', 'r', encoding='utf-8') as file:\n",
    "    val_gold_output = file.readlines()\n",
    "    # remove '\\n' from each line\n",
    "    val_gold_output = [line.strip() for line in val_gold_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_tokenized_input)):\n",
    "    val_segments, val_seg_tags = get_seg_tags(val_tokenized_input[i])\n",
    "    # Write and append on the tokenized input to a file\n",
    "    with open('./generatedFiles/test_input_segments.txt', 'a', encoding='utf-8') as file:\n",
    "        for tag in val_seg_tags:\n",
    "            file.write(tag)\n",
    "        file.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the val input_segments file\n",
    "with open('./generatedFiles/test_input_segments.txt', 'r', encoding='utf-8') as file:\n",
    "    val_input_segments = file.readlines()\n",
    "    val_input_segments = [line.strip() for line in val_input_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417470, 60)\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL TOOK 12 minutes TO RUN\n",
    "# input layer\n",
    "val_char_features_vector=[]\n",
    "val_tag_features_vector=[]\n",
    "val_prior_features_vector=[]\n",
    "val_embeddings = []\n",
    "# with open('./generatedFiles/i_j.txt', 'a', encoding='utf-8') as file:\n",
    "for i in range(len(val_tokenized_input)):\n",
    "    for j in range(len(val_tokenized_input[i])):    \n",
    "        # write i and j in a file\n",
    "        # file.write(f'{i} {j}\\n')\n",
    "        char_index = tokenizer_char.word_index.get(val_tokenized_input[i][j])\n",
    "        char_features_vector= char_embeddings[char_index]\n",
    "        if (len(val_tokenized_input[i]) != len(val_input_segments[i])):\n",
    "            val_input_segments[i] = \"S\" * (len(val_tokenized_input[i]) - len(val_input_segments[i])) + val_input_segments[i]\n",
    "        tag_index = tokenizer_tags.word_index.get(val_input_segments[i][j].lower())\n",
    "        tag_features_vector= tags_embeddings[tag_index]\n",
    "        prior_features_vector= (prior_feature[(val_tokenized_input[i], val_tokenized_input[i][j], j)]) if (val_tokenized_input[i], val_tokenized_input[i][j], j) in prior_feature else [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        # pad the prior feature vector with zeros to have the same length as the other features\n",
    "        prior_features_vector = np.pad(prior_features_vector, (0, 12), 'constant')\n",
    "        # concatenate the 3 features vectors to have a matrix of 3 columns\n",
    "        # embeddings.append(np.vstack((char_features_vector, tag_features_vector, prior_features_vector)))\n",
    "        val_embeddings.append(np.concatenate((char_features_vector, tag_features_vector, prior_features_vector)))\n",
    "\n",
    "val_embeddings = np.array(val_embeddings)\n",
    "\n",
    "# print(char_features_vector)\n",
    "# print(tag_features_vector)\n",
    "# print(prior_features_vector)\n",
    "print(val_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad the embeddings to be of size 418000\n",
    "val_embeddings = np.pad(val_embeddings, ((0, 418000 - val_embeddings.shape[0]), (0, 0)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418000, 60)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 1000, 60)\n"
     ]
    }
   ],
   "source": [
    "val_embeddings_reshape = val_embeddings[:418000]\n",
    "val_embeddings_reshape = val_embeddings_reshape.reshape((-1, 1000, 60))\n",
    "val_embeddings_reshape = np.array(val_embeddings_reshape)\n",
    "print(val_embeddings_reshape.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قَوْلُهُ', 'وَلَا', 'تُكْرَهُ', 'ضِيَافَتُهُ', 'الْفَرْقُ', 'الثَّالِثُ', 'وَالثَّلَاثُونَ', 'بَيْنَ', 'قَاعِدَةِ', 'تَقَدُّمِ']\n"
     ]
    }
   ],
   "source": [
    "print (val_gold_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold labels\n",
    "with open('./generatedFiles/val_gold_output_dict.txt', 'w', encoding='utf-8') as file:\n",
    "    for idx, word in enumerate(val_gold_output):\n",
    "        gold_diacritics = get_prior([val_tokenized_input[idx]], [word])\n",
    "        for key, value in gold_diacritics.items():\n",
    "            key = key + (idx,)\n",
    "            file.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prior feature file in a dictionary called prior_feature \n",
    "val_gold_output_dict = {}\n",
    "with open('./generatedFiles/val_gold_output_dict.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(':')\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        key = key[1:-1].split(',')\n",
    "        value = value[1:-1].split(',')\n",
    "        key = (key[0][1:-1], key[1][2:-1], int(key[2]), int(key[3]))\n",
    "        value = [int(i) for i in value]\n",
    "        val_gold_output_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change gold_output_dict.values() to a list of tuples\n",
    "for key, value in val_gold_output_dict.items():\n",
    "    val_gold_output_dict[key] = tuple(value)\n",
    "    \n",
    "val_gold_output_dict_values = list(val_gold_output_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./generatedFiles/val_gold_output_id.txt', 'w', encoding='utf-8') as file:\n",
    "    for value in val_gold_output_dict_values:\n",
    "        file.write(f'{output_map[value]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the gold_output_id file\n",
    "with open('./generatedFiles/val_gold_output_id.txt', 'r', encoding='utf-8') as file:\n",
    "    val_gold_output_id = file.readlines()\n",
    "    val_gold_output_id = [line.strip() for line in val_gold_output_id]\n",
    "\n",
    "val_gold_output_id = np.array(val_gold_output_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gold_output_id = val_gold_output_id[:421000]\n",
    "val_gold_output_id = val_gold_output_id.reshape(-1, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4210, 100, 15)\n"
     ]
    }
   ],
   "source": [
    "val_labels = []\n",
    "for i in range(len(val_gold_output_id)):\n",
    "    for j in range(1000):\n",
    "        val_labels.append( tf.keras.utils.to_categorical(val_gold_output_id[i][j], num_classes=15))\n",
    "        \n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "val_labels = val_labels.reshape(-1, 1000, 15)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.312587\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the validation dataset\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "loss, accuracy = model.evaluate(val_embeddings_reshape, val_labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = tf.keras.models.load_model('./generatedFiles/model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 5s 386ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict the test dataset\n",
    "predictions = model.predict(val_embeddings_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418000, 15)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.reshape(-1, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.13796008e-01 1.33784284e-04 1.02331594e-03 1.20260862e-04\n",
      "  7.52487744e-04 4.73528817e-05 2.49509321e-05 3.98757329e-05\n",
      "  9.89871114e-05 2.71059946e-07 4.40470359e-07 3.43715897e-07\n",
      "  1.15467415e-07 8.10991594e-07 8.39609057e-02]\n",
      " [1.68595999e-03 1.73978582e-10 6.16314574e-05 9.12179075e-06\n",
      "  1.12038877e-04 3.35399818e-06 9.93750870e-01 3.53652990e-06\n",
      "  6.19534194e-06 2.08193220e-11 1.22610118e-07 1.68911754e-07\n",
      "  1.14870993e-07 6.30322063e-07 4.36629448e-03]\n",
      " [9.99693036e-01 1.03811519e-08 1.69698205e-06 2.95501906e-07\n",
      "  5.01330715e-06 1.07018074e-07 3.29081340e-05 2.69331167e-07\n",
      "  2.50303583e-06 8.56664652e-12 2.37258491e-10 2.17200924e-10\n",
      "  1.12204739e-10 1.41583922e-09 2.64183967e-04]\n",
      " [4.60601914e-05 4.77857647e-08 9.98641670e-01 4.12220015e-06\n",
      "  1.33615918e-04 7.17239814e-08 9.38295273e-07 5.87541820e-08\n",
      "  6.54907506e-09 4.81892637e-10 3.57363729e-06 1.03452180e-09\n",
      "  2.59281818e-09 1.78892859e-10 1.16989145e-03]\n",
      " [1.51824730e-04 2.25686331e-10 1.39221243e-04 4.33684448e-07\n",
      "  2.15607259e-04 3.30812554e-07 8.97353608e-03 2.17788411e-07\n",
      "  5.77655612e-08 1.61536257e-12 7.58030083e-09 6.81497470e-10\n",
      "  3.21671223e-09 5.26689981e-09 9.90518868e-01]\n",
      " [9.99772489e-01 4.15430534e-09 1.93802975e-06 4.53768308e-07\n",
      "  3.30996204e-06 2.25528865e-07 1.87011537e-05 5.82606425e-08\n",
      "  6.93084360e-07 3.74341481e-13 2.74295725e-11 4.37816172e-11\n",
      "  1.93772012e-11 2.98223363e-10 2.02104246e-04]\n",
      " [6.47165652e-05 7.38432959e-08 9.99589264e-01 1.03830447e-04\n",
      "  2.20648973e-04 1.15199771e-06 1.01549723e-07 3.67003739e-09\n",
      "  9.24455956e-10 2.82577746e-11 3.30795672e-07 1.29628053e-09\n",
      "  8.27511282e-10 1.04138351e-10 1.98358284e-05]\n",
      " [1.50492162e-01 3.61145467e-05 3.47047142e-04 1.01038497e-02\n",
      "  2.04627938e-03 3.22876847e-03 7.63481294e-05 1.79367435e-05\n",
      "  1.79041199e-05 1.21284227e-08 3.50119308e-07 6.73998238e-06\n",
      "  5.87213094e-07 7.72901967e-06 8.33618164e-01]\n",
      " [1.33380105e-04 1.37646077e-06 9.79349256e-01 6.37613237e-03\n",
      "  5.43073547e-05 7.44849895e-05 2.75214074e-06 3.83072262e-07\n",
      "  4.30340741e-08 2.13224216e-09 1.14439063e-05 1.45361594e-06\n",
      "  9.59247970e-09 5.60829214e-08 1.39949489e-02]\n",
      " [1.09580775e-04 2.83899304e-08 9.94125724e-01 1.06955552e-03\n",
      "  3.73395073e-04 1.75901823e-05 2.50643370e-05 5.89796088e-08\n",
      "  7.01399827e-09 2.87511213e-11 1.48983372e-06 6.15314377e-08\n",
      "  4.89060525e-09 3.58255003e-09 4.27734293e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[:10])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the maximum value in each row\n",
    "predictions = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418000,)\n",
      "[ 0  6  0  2 14  0  2 14  2  2]\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to map the predictions to the corresponding unicode \n",
    "\n",
    "predictions_map = {\n",
    "    0 : diacritics_mapping['FATHA'],\n",
    "    1 : diacritics_mapping['FATHATAN'],\n",
    "    2 : diacritics_mapping['KASRA'],\n",
    "    3 : diacritics_mapping['KASRATAN'],\n",
    "    4 : diacritics_mapping['DAMMA'],\n",
    "    5 : diacritics_mapping['DAMMATAN'],\n",
    "    6 : diacritics_mapping['SUKUN'],\n",
    "    7 : diacritics_mapping['SHADDA'],\n",
    "    8 : diacritics_mapping['SHADDA'] + diacritics_mapping['FATHA'],\n",
    "    9 : diacritics_mapping['SHADDA'] + diacritics_mapping['FATHATAN'],\n",
    "    10 : diacritics_mapping['SHADDA'] + diacritics_mapping['KASRA'],\n",
    "    11 : diacritics_mapping['SHADDA'] + diacritics_mapping['KASRATAN'],\n",
    "    12 : diacritics_mapping['SHADDA'] + diacritics_mapping['DAMMA'],\n",
    "    13 : diacritics_mapping['SHADDA'] + diacritics_mapping['DAMMATAN'],\n",
    "    14 : ''\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the predictions to 417470\n",
    "predictions = predictions[:417470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417470\n",
      "['لَ', 'يْ', 'سَ', 'لِ', 'ل', 'وَ', 'كِ', 'ي', 'لِ', 'بِ']\n"
     ]
    }
   ],
   "source": [
    "# loop over the letters and concatenate it with the corresponding prediction\n",
    "predicted_diacritized_text = []\n",
    "count = 0\n",
    "for i in range(len(val_tokenized_input)):\n",
    "    for j in range(len(val_tokenized_input[i])):\n",
    "        predicted_diacritized_text.append(val_tokenized_input[i][j] + predictions_map[predictions[count]])\n",
    "        count += 1\n",
    "        \n",
    "print(len(predicted_diacritized_text))\n",
    "print(predicted_diacritized_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test_lines_numb_input file\n",
    "with open('./generatedFiles/test_line_numb_input.txt', 'r', encoding='utf-8') as file:\n",
    "    test_line_numb_input = file.readlines()\n",
    "    test_line_numb_input = [line.strip() for line in test_line_numb_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_arabic_letter(letter):\n",
    "    # Unicode code points for Alef and Yeh in Arabic script\n",
    "    alef_code_point = ord('ا')\n",
    "    yeh_code_point = ord('ي')\n",
    "\n",
    "    # Unicode code point for the given letter\n",
    "    letter_code_point = ord(letter)\n",
    "\n",
    "    # Check if the letter is an Arabic letter between Alef and Yeh\n",
    "    return alef_code_point <= letter_code_point <= yeh_code_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the predictions in a csv file \n",
    "# the file will have 3 columns, the first column is an incremented id from 0 and the second column is the line number and the third column is the diacritized letter\n",
    "with open('./generatedFiles/predictions.csv', 'w', encoding='utf-8') as file:\n",
    "    file.write('ID,line_number,letter\\n')\n",
    "    id = 0\n",
    "    id2 = 0\n",
    "    for i in range(len(val_tokenized_input)):\n",
    "        for j in range(len(val_tokenized_input[i])):\n",
    "            # check if val_tokenized_input[i][j] is an arabic letter\n",
    "            if val_tokenized_input[i][j] in arabic_letters:\n",
    "                file.write(f'{id2},{test_line_numb_input[i]},{predicted_diacritized_text[id]}\\n')\n",
    "                id2 += 1\n",
    "            id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./generatedFiles/predictions.csv', 'w', encoding='utf-8') as file:\n",
    "    file.write('ID,label\\n')\n",
    "    id = 0\n",
    "    id2 = 0\n",
    "    for i in range(len(val_tokenized_input)):\n",
    "        for j in range(len(val_tokenized_input[i])):\n",
    "            # check if val_tokenized_input[i][j] is an arabic letter\n",
    "            if val_tokenized_input[i][j] in arabic_letters:\n",
    "                file.write(f'{id2},{predictions[id]}\\n')\n",
    "                id2 += 1\n",
    "            id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
