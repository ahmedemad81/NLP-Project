{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 03:33:59.810508: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-01 03:33:59.813063: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-01 03:33:59.862026: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-01 03:33:59.863285: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-01 03:34:00.599016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import farasa\n",
    "from farasa.segmenter import FarasaSegmenter \n",
    "import unicodedata\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print the cpu or gpu\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print the number of gpus you have\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:419\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:449\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Run on GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print (device)\n",
    "# print the cpu or gpu\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# print the number of gpus you have\n",
    "print(torch.cuda.device_count())\n",
    "# print current gpu\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Specify the file path\n",
    "file_path = \"./dataset/train.txt\"\n",
    "\n",
    "# Read the contents of the file located at file_path \n",
    "# and append each line to the list data_before_preprocessing\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data_before_preprocessing = file.readlines()\n",
    "    # remove '\\n' from each line\n",
    "    data_before_preprocessing = [line.strip() for line in data_before_preprocessing]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove diacritics\n",
    "def remove_diacritics(text):\n",
    "    text = araby.strip_tashkeel(text)\n",
    "    return text\n",
    "\n",
    "# Remove any non-Arabic letters\n",
    "def remove_non_arabic(text):\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]|،|؛', '', text)\n",
    "    return text\n",
    "\n",
    "def input_preprocessing_text(text):\n",
    "    # Correct most common errors on word like repetetion of harakats, or tanween before alef\n",
    "    text = araby.autocorrect(text)\n",
    "\n",
    "    # Remove any non-Arabic letters\n",
    "    text = remove_non_arabic(text)\n",
    "\n",
    "    # Remove diacritics\n",
    "    text = remove_diacritics(text)\n",
    "\n",
    "    # Tokenize\n",
    "    text = araby.tokenize(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def save_tokenized_input(text,path):\n",
    "    words = input_preprocessing_text(text)\n",
    "    # Write and append on the tokenized input to a file\n",
    "    with open(f'./generatedFiles/{path}.txt', 'a', encoding='utf-8') as file:\n",
    "        for word in words:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "def save_gold_output(text,path):\n",
    "    # Remove any non-Arabic letters and extra spaces\n",
    "    text = remove_non_arabic(text)\n",
    "\n",
    "    # Tokenize\n",
    "    text = araby.tokenize(text)\n",
    "\n",
    "    # Write and append on the gold output to a file\n",
    "    with open(f'./generatedFiles/{path}.txt', 'a', encoding='utf-8') as file:\n",
    "        for word in text:\n",
    "            # if last word in the text don't add '\\n'\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "\n",
    "def is_not_arabic_diacritic(char):\n",
    "   category = unicodedata.category(char)\n",
    "   return not (category == 'Mn' or category == 'Mc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character is an Arabic diacritic.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "character = 'ذْ'\n",
    "if is_not_arabic_diacritic(character[1]):\n",
    "   print(\"The character is not an Arabic diacritic.\")\n",
    "else:\n",
    "   print(\"The character is an Arabic diacritic.\")\n",
    "\n",
    "\n",
    "# Testing of is_not_arabic_diacritic() function with gettting the index of the first non diacritic character in the word\n",
    "word = 'زَّراع'\n",
    " \n",
    "for i in range(1, len(word)): # start from 1 because the first character is not a diacritic\n",
    "    if is_not_arabic_diacritic(word[i]):\n",
    "        print(i)\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN ONE TIME ONLY THIS CODE AGAIN \n",
    "# # Generate Gold Input file\n",
    "# for i in range(len(data_before_preprocessing)):\n",
    "#     save_tokenized_input(data_before_preprocessing[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RUN ONE TIME ONLY THIS CODE AGAIN\n",
    "# # Generate Gold Output file\n",
    "# for i in range(len(data_before_preprocessing)):\n",
    "#     test = data_before_preprocessing[i]\n",
    "#     text1 = save_gold_output(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قال', 'ابن', 'القاسم', 'قال', 'مالك', 'في', 'مكي', 'أحرم', 'بحجة', 'من', 'الحرم', 'ثم', 'أحصر', 'أنه', 'يخرج', 'إلى', 'الحل', 'فيلبي', 'من', 'هناك', 'لأنه', 'أمر', 'من', 'فاته', 'الحج', 'وقد', 'أحرم', 'من', 'مكة', 'أن', 'يخرج', 'إلى', 'الحل', 'فيعمل', 'فيما', 'بقي', 'عليه', 'ما', 'يعمل', 'المعتمر', 'ويحل']\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "test = \"قَالَ ابْنُ الْقَاسِمِ : قَالَ مَالِكٌ فِي مَكِّيٍّ أَحْرَمَ بِحَجَّةٍ مِنْ الْحَرَمِ ثُمَّ أُحْصِرَ ، أَنَّهُ يَخْرُجُ إلَى الْحِلِّ فَيُلَبِّي مِنْ هُنَاكَ لِأَنَّهُ أَمَرَ مَنْ فَاتَهُ الْحَجُّ وَقَدْ أَحْرَمَ مِنْ مَكَّةَ ، أَنْ يَخْرُجَ إلَى الْحِلِّ فَيَعْمَلَ فِيمَا بَقِيَ عَلَيْهِ مَا يَعْمَلُ الْمُعْتَمِرُ وَيُحِلُّ .( 2 / 437 ) \"\n",
    "text2 = input_preprocessing_text(test)\n",
    "print(text2)\n",
    "text3 = remove_non_arabic(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important functions in PyArabic\n",
    "\n",
    "# araby.tokenize(text) # Tokenize the sentence text into words\n",
    "# araby.is_arabicrange(text) # Check if the text is Arabic\n",
    "# araby.sentence_tokenize(text) # Tokenize the text into sentences\n",
    "# araby.strip_tashkeel(text) # Remove diacritics (FATHA, DAMMA, KASRA, SUKUN, SHADDA, FATHATAN, DAMMATAN, KASRATAN)\n",
    "# araby.strip_diacritics(text) # Remove diacritics (Small Alef الألف الخنجرية, Harakat + Shadda, Quranic marks)\n",
    "# araby.strip_tatweel(text) # Remove tatweel\n",
    "# araby.strip_shadda(text) # Remove shadda\n",
    "# araby.autocorrect(text) # Correct most common errors on word like repetetion of harakats,or tanwin befor alef\n",
    "# araby.arabicrange() # Return a list of Arabic characters\n",
    "\n",
    "# New Functions in PyArabic\n",
    "# araby.vocalized_similarity(word1, word2) # if the two words has the same letters and the same harakats, this function return True. \n",
    "# The two words can be full vocalized, or partial vocalized\n",
    "\n",
    "# araby.vocalizedlike(word1, word2) Same as vocalized_similarity but return True and False\n",
    "\n",
    "# araby.joint(word1, word2) # joint the letters with the marks the length ot letters and marks must be equal return word\n",
    "\n",
    "\n",
    "\n",
    "# Return the text, its tashkeel and shadda if extract_shadda is True\n",
    "# text, marks, shada = araby.separate(text,extract_shadda=True) # Separate diacritics from the text\n",
    "# print (text)\n",
    "# for m in marks:\n",
    "#     print (araby.name(m))\n",
    "\n",
    "# for s in shada:\n",
    "#     print (araby.name(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n"
     ]
    }
   ],
   "source": [
    "# read the tokenized input file\n",
    "with open('./generatedFiles/tokenized_input.txt', 'r', encoding='utf-8') as file:\n",
    "    tokenized_input = file.readlines()\n",
    "    print(len(tokenized_input))\n",
    "    # Remove '\\n' from each line\n",
    "    tokenized_input = [line.strip() for line in tokenized_input]\n",
    "    # Put the tokenized input of length 1 in tokenized_input list \n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "\n",
    "print(tokenized_input[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Core Word (CW) Diacritization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Characters: \n",
    "Here we extract each character from all tokenized words and create a vector of size 50 for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_char = Tokenizer(char_level=True)\n",
    "tokenizer_char.fit_on_texts(tokenized_input)\n",
    "sequences_char = tokenizer_char.texts_to_sequences(tokenized_input)\n",
    "char_features = pad_sequences(sequences_char)   # padding the sequences to have the same length as the longest sequence (word)\n",
    "char_embeddings = np.random.rand(len(tokenizer_char.word_index) + 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2104308, 13)\n",
      "(38, 50)\n"
     ]
    }
   ],
   "source": [
    "print(char_features.shape) # (number of words, max length of word in the dataset)\n",
    "\n",
    "\n",
    "print(char_embeddings.shape)\n",
    "\n",
    "# 38 rows: 37 unique characters identified by the tokenizer, 1 row for handling characters not seen in the training data\n",
    "# 50 columns: Each character is encoded as a 50-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0 13  5  1  7]\n"
     ]
    }
   ],
   "source": [
    "print(char_features[0]) \n",
    "# the number of non zero elements corresponds to the length of the word \n",
    "# and the value of each element corresponds to the index of the character in the tokenizer\n",
    "# which means that every character now is encoded as a number and this number is the index of the character in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      " 0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      " 0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      " 0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      " 0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      " 0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      " 0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      " 0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      " 0.35316103 0.60159834]\n"
     ]
    }
   ],
   "source": [
    "print(char_embeddings[0])\n",
    "# this is the embedding of each character in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 50)\n",
      "[[0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      "  0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      "  0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      "  0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      "  0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      "  0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      "  0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      "  0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      "  0.35316103 0.60159834]\n",
      " [0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      "  0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      "  0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      "  0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      "  0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      "  0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      "  0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      "  0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      "  0.35316103 0.60159834]\n",
      " [0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      "  0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      "  0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      "  0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      "  0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      "  0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      "  0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      "  0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      "  0.35316103 0.60159834]\n",
      " [0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      "  0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      "  0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      "  0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      "  0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      "  0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      "  0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      "  0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      "  0.35316103 0.60159834]\n",
      " [0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      "  0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      "  0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      "  0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      "  0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      "  0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      "  0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      "  0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      "  0.35316103 0.60159834]\n",
      " [0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      "  0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      "  0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      "  0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      "  0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      "  0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      "  0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      "  0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      "  0.35316103 0.60159834]\n",
      " [0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      "  0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      "  0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      "  0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      "  0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      "  0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      "  0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      "  0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      "  0.35316103 0.60159834]\n",
      " [0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      "  0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      "  0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      "  0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      "  0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      "  0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      "  0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      "  0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      "  0.35316103 0.60159834]\n",
      " [0.82775715 0.14407463 0.20135952 0.77541977 0.69178919 0.22198801\n",
      "  0.61390601 0.20377658 0.33884922 0.73752823 0.59720915 0.0043791\n",
      "  0.31430793 0.44029642 0.30902776 0.37853242 0.28455223 0.32269753\n",
      "  0.03608024 0.51503891 0.04991179 0.08641144 0.75597148 0.87585047\n",
      "  0.37427598 0.47991583 0.11946686 0.31026765 0.19423183 0.24198316\n",
      "  0.20052713 0.38847992 0.50487335 0.37485987 0.05907555 0.99735723\n",
      "  0.0471801  0.95735755 0.86742232 0.30184236 0.51258831 0.18192415\n",
      "  0.74410411 0.92625454 0.08531106 0.17199506 0.94759159 0.50132945\n",
      "  0.35316103 0.60159834]\n",
      " [0.74810475 0.3384157  0.26040341 0.13859089 0.0935149  0.81583579\n",
      "  0.50353602 0.19965956 0.3317361  0.352817   0.13193759 0.56226167\n",
      "  0.08154547 0.93111311 0.33456174 0.31224133 0.0719607  0.71821806\n",
      "  0.71092309 0.83482354 0.93181746 0.36961122 0.76813534 0.46072228\n",
      "  0.69877942 0.78484195 0.40820099 0.00300907 0.50316952 0.76031765\n",
      "  0.04769544 0.62663204 0.49084595 0.73825884 0.61843503 0.80872305\n",
      "  0.86233909 0.94887345 0.26207364 0.48583408 0.64900208 0.83869171\n",
      "  0.66796429 0.55528643 0.16221587 0.74040906 0.73244705 0.97993122\n",
      "  0.81013397 0.34205142]\n",
      " [0.13048637 0.29965516 0.55019337 0.90441558 0.70317311 0.2883663\n",
      "  0.43779735 0.95940802 0.44941969 0.75116143 0.04489208 0.90592077\n",
      "  0.58299411 0.39414056 0.55248743 0.92086123 0.29609771 0.4846856\n",
      "  0.4052012  0.61773683 0.55984941 0.93210977 0.00712693 0.91050468\n",
      "  0.73296044 0.33752241 0.06233547 0.01661644 0.01476328 0.76381086\n",
      "  0.09244813 0.15376747 0.71842303 0.34885666 0.12763746 0.32670926\n",
      "  0.38180606 0.74322225 0.59677264 0.53867306 0.73475618 0.80428127\n",
      "  0.15610045 0.21615348 0.00625615 0.32849314 0.78032654 0.95816759\n",
      "  0.30456645 0.92193483]\n",
      " [0.83140793 0.79681553 0.13540798 0.29838352 0.26736657 0.39650282\n",
      "  0.15778785 0.83662451 0.4272     0.09355131 0.61322498 0.80687635\n",
      "  0.60895938 0.65996015 0.63103148 0.09257834 0.960388   0.13292135\n",
      "  0.32629399 0.09820074 0.34799615 0.86325617 0.3888433  0.16934789\n",
      "  0.64432991 0.42655319 0.84646463 0.2507509  0.57634929 0.93882048\n",
      "  0.31833025 0.13569718 0.57155546 0.82295227 0.00404757 0.97251622\n",
      "  0.74721655 0.74640581 0.93643764 0.81310158 0.18943266 0.42714025\n",
      "  0.71875869 0.05036084 0.99823549 0.0835623  0.58620881 0.28428319\n",
      "  0.73797121 0.24500093]\n",
      " [0.7673527  0.28388029 0.55142477 0.70095307 0.31076839 0.32758765\n",
      "  0.85617535 0.60348159 0.63436673 0.57696071 0.88313422 0.97024441\n",
      "  0.46387864 0.73046767 0.54657417 0.83232685 0.84525912 0.94846152\n",
      "  0.83684005 0.6620143  0.1215779  0.65215723 0.99356433 0.13953961\n",
      "  0.47693231 0.21820279 0.09117146 0.56320983 0.10753155 0.69712428\n",
      "  0.02402631 0.36336479 0.25770054 0.30673171 0.76440045 0.33595628\n",
      "  0.43397897 0.36480852 0.7409655  0.71292384 0.60656695 0.51767539\n",
      "  0.61674991 0.56050678 0.91943508 0.99045653 0.70615915 0.5491114\n",
      "  0.87559569 0.65319759]]\n"
     ]
    }
   ],
   "source": [
    "print(char_embeddings[char_features[0]].shape)\n",
    "# 13 is the word of characters and 50 is the embedding size of each character\n",
    "\n",
    "print(char_embeddings[char_features[0]])\n",
    "# this is the embedding of each character in the first tokenized word, this is the 1st feature and the input of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - The position of the character in a word segment:\n",
    "For example, given the word “wAlktAb” , which is composed of three segments “w+Al+ktAb”. Letters were marked as “B” if they begin a segment, “M” if they are in the middle of a segment, “E” if they end a segment, and “S” if they are single letter segments. So for “w+Al+ktAb”, the corresponding character positions are “S+BE+BMME.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'farasa-api.qcri.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241M/241M [01:55<00:00, 2.08MiB/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-01 04:16:03,180 - farasapy_logger - ERROR]: an error occured\n",
      "[2024-01-01 04:16:03,181 - farasapy_logger - ERROR]: [Errno 13] Permission denied: '/usr/local/lib/python3.8/dist-packages/farasa/farasa_bin'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/usr/local/lib/python3.8/dist-packages/farasa/tmp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m segmenter \u001b[38;5;241m=\u001b[39m \u001b[43mFarasaSegmenter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minteractive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# The default behaviour\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/farasa/__base.py:44\u001b[0m, in \u001b[0;36mFarasaBase.__init__\u001b[0;34m(self, interactive, logging_level)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck toolkit binaries...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_toolkit_binaries()\n\u001b[0;32m---> 44\u001b[0m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__base_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/tmp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDependencies seem to be satisfied..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interactive:\n",
      "File \u001b[0;32m/usr/lib/python3.8/pathlib.py:1288\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_closed()\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1288\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/usr/local/lib/python3.8/dist-packages/farasa/tmp'"
     ]
    }
   ],
   "source": [
    "segmenter = FarasaSegmenter(interactive=True) # The default behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'segmenter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m segments, seg_tags\n\u001b[1;32m     14\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mكقلمه\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m segments, seg_tags \u001b[38;5;241m=\u001b[39m \u001b[43mget_seg_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmented word:\u001b[39m\u001b[38;5;124m\"\u001b[39m, segments)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEG tags:\u001b[39m\u001b[38;5;124m\"\u001b[39m, seg_tags)\n",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m, in \u001b[0;36mget_seg_tags\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_seg_tags\u001b[39m(word):                 \u001b[38;5;66;03m# word = \"wAlktAb\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     segments \u001b[38;5;241m=\u001b[39m \u001b[43msegmenter\u001b[49m\u001b[38;5;241m.\u001b[39msegment(word)  \u001b[38;5;66;03m# segments will be a list: [\"w\", \"Al\", \"ktAb\"]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     segments \u001b[38;5;241m=\u001b[39m segments\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     seg_tags \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'segmenter' is not defined"
     ]
    }
   ],
   "source": [
    "def get_seg_tags(word):                 # word = \"wAlktAb\"\n",
    "    segments = segmenter.segment(word)  # segments will be a list: [\"w\", \"Al\", \"ktAb\"]\n",
    "    segments = segments.split('+')\n",
    "    seg_tags = []\n",
    "    for segment in segments:\n",
    "        if len(segment) == 1:\n",
    "            seg_tags.append(\"S\")\n",
    "        else:\n",
    "            seg_tags.append(\"B\")  # First letter\n",
    "            seg_tags.extend(\"M\" * (len(segment) - 2))  # Middle letters\n",
    "            seg_tags.append(\"E\")  # Last letter\n",
    "    return segments, seg_tags\n",
    "\n",
    "word = \"كقلمه\"\n",
    "segments, seg_tags = get_seg_tags(word)\n",
    "print(\"Segmented word:\", segments)\n",
    "print(\"SEG tags:\", seg_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DON'T RUN THIS CODE AGAIN, THIS CELL TOOK 25m 4.5s TO RUN\n",
    "# # The Output of this code is the input_segments.txt file\n",
    "\n",
    "# for i in range(len(tokenized_input)):\n",
    "#     segments, seg_tags = get_seg_tags(tokenized_input[i])\n",
    "#     # Write and append on the tokenized input to a file\n",
    "#     with open('./generatedFiles/input_segments.txt', 'a', encoding='utf-8') as file:\n",
    "#         for tag in seg_tags:\n",
    "#             file.write(tag)\n",
    "#         file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['BMES', 'BE', 'BME', 'BEBME', 'BES', 'BME', 'BME', 'BEBMMME', 'BME', 'BMES']\n"
     ]
    }
   ],
   "source": [
    "# Read the input_segments file\n",
    "with open('./generatedFiles/input_segments.txt', 'r', encoding='utf-8') as file:\n",
    "    input_segments = file.readlines()\n",
    "    print(len(input_segments))\n",
    "    # Remove '\\n' from each line\n",
    "    input_segments = [line.strip() for line in input_segments]\n",
    "    # Put the tokenized input of length 1 in the tokenized_input list\n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "\n",
    "print(input_segments[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tags = Tokenizer(char_level=True)\n",
    "tokenizer_tags.fit_on_texts(input_segments)\n",
    "sequences_tags = tokenizer_tags.texts_to_sequences(input_segments)\n",
    "tags_features = pad_sequences(sequences_tags)   \n",
    "tags_embeddings = np.random.rand(len(tokenizer_tags.word_index) + 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2104308, 13)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "print(tags_features.shape) \n",
    "print(tags_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 2, 4], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.66677583e-01, 8.86454123e-01, 1.13832792e-02, 3.70831116e-01,\n",
       "       3.31769904e-01, 2.90807994e-01, 4.11001010e-01, 6.76911686e-02,\n",
       "       4.15257775e-01, 7.36223802e-02, 3.28225494e-03, 4.32367067e-01,\n",
       "       6.67660097e-01, 1.80724637e-01, 3.30664200e-01, 7.22262214e-02,\n",
       "       5.22613413e-01, 1.06997034e-01, 5.82355531e-01, 6.31027943e-01,\n",
       "       8.57333603e-01, 4.44430890e-01, 1.91273319e-01, 3.19766491e-01,\n",
       "       9.03579279e-01, 3.59271829e-01, 4.04055775e-01, 5.40305776e-02,\n",
       "       4.18897447e-01, 3.00678619e-01, 4.36432881e-01, 1.08191547e-01,\n",
       "       9.46181705e-01, 2.49636459e-01, 5.60390077e-01, 3.31735707e-01,\n",
       "       7.98341826e-01, 7.80804634e-01, 3.05748368e-01, 1.67359678e-01,\n",
       "       4.21394462e-01, 1.25127367e-01, 7.01076676e-01, 8.34157794e-01,\n",
       "       2.38780978e-01, 6.28024653e-01, 2.96880331e-01, 6.74303259e-04,\n",
       "       8.45280411e-01, 9.92803073e-01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_embeddings[tags_features[0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - PRIOR: \n",
    "diacritics seen in the training set per segment. Since we used a character-level model, this feature informed the model with word-level information. For example, the word “ktAb”  was observed to have two diacritized forms in the training set, namely “kitaAb” ( – book) and “kut∼aAb” ( – writers). The first letter in the word (“k”) accepted the diacritics “i” and “u.” Thus, given a binary vector representing whether a character is allowed to assume any of the eight primitive Arabic diacritic marks (a, i, u, o, K, N, F, and ∼ in order), the first letter would be given the following vector “01100000.” If a word segment was never observed during training, then the vector for all letters therein would be set to 11111111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قَوْلُهُ', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', 'قَالَ', 'الزَّرْكَشِيُّ', 'ابْنُ', 'عَرَفَةَ']\n"
     ]
    }
   ],
   "source": [
    "# read the gold_output file\n",
    "with open('./generatedFiles/gold_output.txt', 'r', encoding='utf-8') as file:\n",
    "    gold_output = file.readlines()\n",
    "    print(len(gold_output))\n",
    "    # remove '\\n' from each line\n",
    "    gold_output = [line.strip() for line in gold_output]\n",
    "    # put in tokenized_input list the tokenized input of length 1\n",
    "    # tokenized_input = [(line.strip(), i) for i,line in enumerate(tokenized_input) if (len(line.strip())== 1 and line.strip() != '؟')]\n",
    "    # get the inde\n",
    "\n",
    "print(gold_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104308\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_input))\n",
    "print(tokenized_input[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each diacrtics to its unicode\n",
    "diacritics_mapping = {\n",
    "    'FATHA': '\\u064E',\n",
    "    'DAMMA': '\\u064F',\n",
    "    'KASRA': '\\u0650',\n",
    "    'SHADDA': '\\u0651',\n",
    "    'SUKUN': '\\u0652',\n",
    "    'FATHATAN': '\\u064B',\n",
    "    'DAMMATAN': '\\u064C',\n",
    "    'KASRATAN': '\\u064D'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract diacritics by returning a list containing a tuple of 3 elements: (letter, tashkeel, shadda)\n",
    "# def extract_arabic_diacritics(word):\n",
    "#     diacritics_list = []\n",
    "#     extracted_word, tashkeel, shadda = araby.separate(word, extract_shadda=True)\n",
    "#     for i in range(len(extracted_word)):\n",
    "#         print(f'{araby.name(extracted_word[i])} {araby.name(tashkeel[i])} {araby.name(shadda[i])}')\n",
    "#         diacritics_list.append((extracted_word[i], (tashkeel[i].encode(\"utf8\")).decode(), (shadda[i].encode(\"utf8\")).decode()))\n",
    "#     return diacritics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # firstly, initialize an empty dictionary for the 'prior' feature\n",
    "# # the value will be the 8 arabic marks (FATHA, DAMMA, KASRA, FATHATAN, DAMMATAN, KASRATAN, SUKUN, SHADDA) as a binary vector\n",
    "\n",
    "# # then, loop over the tokenized input and check if the each character and word pair is not in the dictionary, get the indices of this word and its duplicates in the tokenized input array\n",
    "# def get_prior(tokenized_input, gold_output):\n",
    "#     prior = {} # this dictionary will hold a key of tuple of 3 elements (word, character, index of character in the word) and the value will be the 8 arabic marks\n",
    "#     for i in range(len(tokenized_input)):\n",
    "#         if (tokenized_input[i], tokenized_input[i][0], 0) not in prior:\n",
    "#             # get the indices of the word in the tokenized input array\n",
    "#             indices = [j for j, x in enumerate(tokenized_input) if x == tokenized_input[i]]\n",
    "#             print(indices)\n",
    "#             # get the words in the gold_output array with the same indices\n",
    "#             words = [gold_output[j] for j in indices]\n",
    "#             extracted_diac_all_words = []\n",
    "#             for word in words:\n",
    "#                 extracted_diac_all_words.append(extract_arabic_diacritics(word))\n",
    "#             for indx, charac in enumerate(tokenized_input[i]):\n",
    "#                 for extracted_diac_per_word in extracted_diac_all_words:\n",
    "#                     # extract the diacritics of word[indx]\n",
    "#                     prior[(tokenized_input[i], charac, indx)] = [0, 0, 0, 0, 0, 0, 0, 0] # initialize the value of the key with zeros\n",
    "#                     if diacritics_mapping['SHADDA'] in extracted_diac_per_word[indx]:\n",
    "#                         prior[(tokenized_input[i], charac, indx)][4] = 1 if diacritics_mapping['FATHA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac, indx)][5] = 1 if diacritics_mapping['DAMMA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac, indx)][6] = 1 if diacritics_mapping['KASRA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac, indx)][7] = 1 if not  diacritics_mapping['FATHA'] in extracted_diac_per_word[indx] and not diacritics_mapping['DAMMA'] in word[indx: indx+2]  and not diacritics_mapping['KASRA'] in word[indx: indx+2] else 0\n",
    "#                     else:\n",
    "#                         prior[(tokenized_input[i], charac,indx)][0] = 1 if diacritics_mapping['FATHA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac,indx)][1] = 1 if diacritics_mapping['DAMMA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac,indx)][2] = 1 if diacritics_mapping['KASRA'] in extracted_diac_per_word[indx] else 0\n",
    "#                         prior[(tokenized_input[i], charac,indx)][3] = 1 if diacritics_mapping['SUKUN'] in extracted_diac_per_word[indx] else 0\n",
    "#     return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FATHA in tashkeel:  True\n",
      "DAMMA in tashkeel:  False\n",
      "KASRA in tashkeel:  False\n",
      "SUKUN in tashkeel:  True\n",
      "FATHATAN in tashkeel:  False\n",
      "DAMMATAN in tashkeel:  False\n",
      "KASRATAN in tashkeel:  False\n",
      "SHADDA in tashkeel:  False\n",
      "=============================\n",
      "FATHA in shadda:  False\n",
      "DAMMA in shadda:  False\n",
      "KASRA in shadda:  False\n",
      "SUKUN in shadda:  False\n",
      "FATHATAN in shadda:  False\n",
      "DAMMATAN in shadda:  False\n",
      "KASRATAN in shadda:  False\n",
      "SHADDA in shadda:  True\n",
      "testt False\n",
      "yarab False\n"
     ]
    }
   ],
   "source": [
    "letter, tashkeel, shadda = araby.separate('زَّ', extract_shadda=True)   # SHADDA + FATHA Example\n",
    "# letter, tashkeel, shadda = araby.separate('وَ', extract_shadda=True)   # FATHA Example\n",
    "# letter, tashkeel, shadda = araby.separate('مً', extract_shadda=True)   # FATHATAN Example\n",
    "# letter, tashkeel, shadda = araby.separate('عٌ', extract_shadda=True)   # DAMMATAN Example\n",
    "# letter, tashkeel, shadda = araby.separate('يُّ', extract_shadda=True)   # SHADDA + DAMMA Example\n",
    "# letter, tashkeel, shadda = araby.separate('ذْ', extract_shadda=True)   # SUKUN Example\n",
    "enkar = 'كَإِنْكَارِ'\n",
    "# print(enkar[4:6])\n",
    "# print( diacritics_mapping['FATHA'] in enkar[0:1])\n",
    "# print( diacritics_mapping['SHADDA'] in 'زَّ')\n",
    "# print( diacritics_mapping['DAMMA'] in 'زَّ')\n",
    "\n",
    "print('FATHA in tashkeel: ', diacritics_mapping['FATHA'] in tashkeel)\n",
    "print('DAMMA in tashkeel: ', diacritics_mapping['DAMMA'] in tashkeel)\n",
    "print('KASRA in tashkeel: ', diacritics_mapping['KASRA'] in tashkeel)\n",
    "print('SUKUN in tashkeel: ', diacritics_mapping['SUKUN'] in tashkeel)\n",
    "print('FATHATAN in tashkeel: ', diacritics_mapping['FATHATAN'] in tashkeel)\n",
    "print('DAMMATAN in tashkeel: ', diacritics_mapping['DAMMATAN'] in tashkeel)\n",
    "print('KASRATAN in tashkeel: ', diacritics_mapping['KASRATAN'] in tashkeel)\n",
    "print('SHADDA in tashkeel: ', diacritics_mapping['SHADDA'] in tashkeel)\n",
    "print('=============================')\n",
    "print('FATHA in shadda: ', diacritics_mapping['FATHA'] in shadda)\n",
    "print('DAMMA in shadda: ', diacritics_mapping['DAMMA'] in shadda)\n",
    "print('KASRA in shadda: ', diacritics_mapping['KASRA'] in shadda)\n",
    "print('SUKUN in shadda: ', diacritics_mapping['SUKUN'] in shadda)\n",
    "print('FATHATAN in shadda: ', diacritics_mapping['FATHATAN'] in shadda)\n",
    "print('DAMMATAN in shadda: ', diacritics_mapping['DAMMATAN'] in shadda)\n",
    "print('KASRATAN in shadda: ', diacritics_mapping['KASRATAN'] in shadda)\n",
    "print('SHADDA in shadda: ', diacritics_mapping['SHADDA'] in shadda)\n",
    "\n",
    "print('testt', (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['FATHA'] not in tashkeel and diacritics_mapping['DAMMA'] not in tashkeel and diacritics_mapping['KASRA'] not in tashkeel))\n",
    "print('yarab', (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['SHADDA'] not in shadda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstly, initialize an empty dictionary for the 'prior' feature\n",
    "# the value will be the 8 arabic marks (FATHA, DAMMA, KASRA, FATHATAN, DAMMATAN, KASRATAN, SUKUN, SHADDA) as a binary vector\n",
    "\n",
    "# then, loop over the tokenized input and check if the each character and word pair is not in the dictionary, get the indices of this word and its duplicates in the tokenized input array\n",
    "def get_prior(tokenized_input, gold_output):\n",
    "    prior = {}  # this dictionary will hold a key of tuple of 3 elements (word, character, index of character in the word) and the value will be the 8 arabic marks\n",
    "    for i in range(len(tokenized_input)):\n",
    "        if (tokenized_input[i], tokenized_input[i][0], 0) not in prior:\n",
    "            # get the indices of the word in the tokenized input array\n",
    "            indices = [j for j, x in enumerate(tokenized_input) if x == tokenized_input[i]]\n",
    "            # print(indices)\n",
    "            # get the words in the gold_output array with the same indices\n",
    "            words = []\n",
    "            maxi_len = 0\n",
    "            for j in indices:\n",
    "                if gold_output[j] not in words:\n",
    "                    words.append(gold_output[j])\n",
    "                    maxi_len = max(maxi_len, len(gold_output[j]))\n",
    "\n",
    "            for t in range(len(tokenized_input[i])):\n",
    "                prior[(tokenized_input[i], tokenized_input[i][t], t)] = [0, 0, 0, 0, 0, 0, 0, 0] # initialize the value of the key with zeros\n",
    "            \n",
    "            indx2 = 0\n",
    "            for word in words:\n",
    "                indx = 0\n",
    "                while indx < maxi_len:\n",
    "                    # extract the diacritics of word[indx]\n",
    "                    for iter in range(indx+1, len(word)):\n",
    "                        if is_not_arabic_diacritic(word[iter]):\n",
    "                            # print(iter)\n",
    "                            letter, tashkeel, shadda = araby.separate(word[indx: iter], extract_shadda=True) \n",
    "                            if diacritics_mapping['FATHA'] in tashkeel:         prior[(tokenized_input[i], word[indx], indx2)][0] = 1 \n",
    "                            if diacritics_mapping['DAMMA'] in tashkeel:         prior[(tokenized_input[i], word[indx], indx2)][1] = 1\n",
    "                            if diacritics_mapping['KASRA'] in tashkeel:         prior[(tokenized_input[i], word[indx], indx2)][2] = 1\n",
    "                            if diacritics_mapping['FATHATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], indx2)][3] = 1\n",
    "                            if diacritics_mapping['DAMMATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], indx2)][4] = 1\n",
    "                            if diacritics_mapping['KASRATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], indx2)][5] = 1\n",
    "                            if (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['SHADDA'] not in shadda):  \n",
    "                                prior[(tokenized_input[i], word[indx], indx2)][6] = 1 # if the letter has SHADDA, araby.separate() will return SUKUN in tashkeel and SHADDA in shadda, so to avoid this mislabeling we check if SHADDA not in shadda and if SUKUN in tashkeel, then this is a true SUKUN\n",
    "                            if diacritics_mapping['SHADDA'] in shadda:          prior[(tokenized_input[i], word[indx], indx2)][7] = 1\n",
    "                            indx = iter - 1\n",
    "                            indx2 += 1\n",
    "                            break \n",
    "                    indx += 1\n",
    "                indx2 = 0\n",
    "\n",
    "\n",
    "                indx = len(word) - 1    # my assumption is that the last character in the not a diacritic\n",
    "                if (not is_not_arabic_diacritic(word[len(word) - 1]) and is_not_arabic_diacritic(word[len(word) - 2])):  # if the last character is a diacritic and the one before it is not, then the index of the last character is len(word) - 2\n",
    "                    indx = len(word) - 2\n",
    "                elif (not is_not_arabic_diacritic(word[len(word) - 1]) and not is_not_arabic_diacritic(word[len(word) - 2])):  # if the last character is a diacritic and the one before it is also a diacritic (in shadda case), then the index of the last character is len(word) - 3\n",
    "                    indx = len(word) - 3\n",
    "\n",
    "\n",
    "                if (tokenized_input[i], word[indx], indx) not in prior:\n",
    "                    letter, tashkeel, shadda = araby.separate(word[indx: len(word)], extract_shadda=True) \n",
    "                    if diacritics_mapping['FATHA'] in tashkeel:         prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][0] = 1\n",
    "                    if diacritics_mapping['DAMMA'] in tashkeel:         prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][1] = 1\n",
    "                    if diacritics_mapping['KASRA'] in tashkeel:         prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][2] = 1 \n",
    "                    if diacritics_mapping['FATHATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][3] = 1 \n",
    "                    if diacritics_mapping['DAMMATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][4] = 1 \n",
    "                    if diacritics_mapping['KASRATAN'] in tashkeel:      prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][5] = 1\n",
    "                    if (diacritics_mapping['SUKUN'] in tashkeel and diacritics_mapping['SHADDA'] not in shadda):\n",
    "                        prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][6] = 1  # if the letter has SHADDA, araby.separate() will return SUKUN in tashkeel and SHADDA in shadda, so to avoid this mislabeling we check if SHADDA not in shadda and if SUKUN in tashkeel, then this is a true SUKUN\n",
    "                    if diacritics_mapping['SHADDA'] in shadda:          prior[(tokenized_input[i], word[indx], len(tokenized_input[i])-1)][7] = 1\n",
    "                    \n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('كإنكار', 'ك', 0): [1, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'إ', 1): [0, 0, 1, 0, 0, 0, 0, 0], ('كإنكار', 'ن', 2): [0, 0, 0, 0, 0, 0, 1, 0], ('كإنكار', 'ك', 3): [1, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'ا', 4): [0, 0, 0, 0, 0, 0, 0, 0], ('كإنكار', 'ر', 5): [0, 0, 1, 0, 0, 1, 0, 0], ('بقذر', 'ب', 0): [0, 0, 1, 0, 0, 0, 0, 0], ('بقذر', 'ق', 1): [1, 0, 0, 0, 0, 0, 0, 0], ('بقذر', 'ذ', 2): [1, 0, 0, 0, 0, 0, 0, 0], ('بقذر', 'ر', 3): [0, 0, 0, 0, 0, 1, 0, 0], ('أكثر', 'أ', 0): [1, 0, 0, 0, 0, 0, 0, 0], ('أكثر', 'ك', 1): [0, 0, 0, 0, 0, 0, 1, 0], ('أكثر', 'ث', 2): [1, 0, 0, 0, 0, 0, 0, 0], ('أكثر', 'ر', 3): [1, 1, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ا', 0): [0, 0, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ل', 1): [0, 0, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ز', 2): [1, 0, 0, 0, 0, 0, 0, 1], ('الزركشي', 'ر', 3): [0, 0, 0, 0, 0, 0, 1, 0], ('الزركشي', 'ك', 4): [1, 0, 0, 0, 0, 0, 0, 0], ('الزركشي', 'ش', 5): [0, 0, 1, 0, 0, 0, 0, 0], ('الزركشي', 'ي', 6): [0, 1, 0, 0, 0, 0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "test_tokenized_input = ['كإنكار', 'كإنكار', 'بقذر','بقذر', 'أكثر', 'أكثر', 'الزركشي']\n",
    "test_gold_output = ['كَإِنْكَارِ','كَإِنْكَارٍ', 'بِقَذَر', 'بِقَذَرٍ','أكْثَرَ', 'أَكْثَرُ', 'الزَّرْكَشِيُّ']\n",
    "print (get_prior(test_tokenized_input, test_gold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DON'T RUN THIS CODE AGAIN, THIS CELL TOOK 276 minutes TO RUN\n",
    "# # write in a file the prior feature\n",
    "# prior_feature = get_prior(tokenized_input, gold_output)\n",
    "# with open('./generatedFiles/prior_feature.txt', 'w', encoding='utf-8') as file:\n",
    "#     for key, value in prior_feature.items():\n",
    "#         file.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# read the prior feature file in a dictionary called prior_feature \n",
    "prior_feature = {}\n",
    "with open('./generatedFiles/prior_feature.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(':')\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        key = key[1:-1].split(',')\n",
    "        value = value[1:-1].split(',')\n",
    "        key = (key[0][1:-1], key[1][2:-1], int(key[2]))\n",
    "        value = [int(i) for i in value]\n",
    "        prior_feature[key] = value\n",
    "\n",
    "print(prior_feature[('قوله', 'ق', 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - CASE Feature: \n",
    "whether the letter expects a core word diacritic or a case ending. Case endings are placed on only one letter in a word, which may or may not be the last letter in the word. This is a binary feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-01 00:47:22,675 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: الكتابة باللغة العربية\n",
      "Stemmed text: كتابة لغة عربي\n"
     ]
    }
   ],
   "source": [
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "def arabic_stemmer(text):\n",
    "    stemmer = FarasaStemmer(interactive=True)  # Set interactive to True for better performance\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmed_text = stemmer.stem(text)\n",
    "\n",
    "    return stemmed_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"الكتابة باللغة العربية\"\n",
    "stemmed_text = arabic_stemmer(input_text)\n",
    "print(\"Original text:\", input_text)\n",
    "print(\"Stemmed text:\", stemmed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     stemmed_text = arabic_stemmer(tokenized_input[i])\n",
    "#     print(\"Original text:\", tokenized_input[i])\n",
    "#     print(\"Stemmed text:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(tokenized_input)):\n",
    "#     stemmed_text = arabic_stemmer(tokenized_input[i])\n",
    "#     # Write and append on the tokenized input to a file\n",
    "#     with open('./generatedFiles/stemmed_input.txt', 'a', encoding='utf-8') as file:\n",
    "#         file.write(stemmed_text + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed_text = stemmed_text.split(' ')\n",
    "# # write in a file the stemmed input\n",
    "# with open('./generatedFiles/stemmed_input.txt', 'w', encoding='utf-8') as file:\n",
    "#     for word in stemmed_text:\n",
    "#         file.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8353805, 150)\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL TOOK 12 minutes TO RUN\n",
    "# input layer\n",
    "char_features_vector=[]\n",
    "tag_features_vector=[]\n",
    "prior_features_vector=[]\n",
    "embeddings = []\n",
    "# with open('./generatedFiles/i_j.txt', 'a', encoding='utf-8') as file:\n",
    "for i in range(len(tokenized_input)):\n",
    "    for j in range(len(tokenized_input[i])):    \n",
    "        # write i and j in a file\n",
    "        # file.write(f'{i} {j}\\n')\n",
    "        char_index = tokenizer_char.word_index.get(tokenized_input[i][j])\n",
    "        char_features_vector= char_embeddings[char_index]\n",
    "        if (len(tokenized_input[i]) != len(input_segments[i])):\n",
    "            input_segments[i] = \"S\" * (len(tokenized_input[i]) - len(input_segments[i])) + input_segments[i]\n",
    "        tag_index = tokenizer_tags.word_index.get(input_segments[i][j].lower())\n",
    "        tag_features_vector= tags_embeddings[tag_index]\n",
    "        prior_features_vector= prior_feature[(tokenized_input[i], tokenized_input[i][j], j)]\n",
    "        # pad the prior feature vector with zeros to have the same length as the other features\n",
    "        prior_features_vector = np.pad(prior_features_vector, (0, 42), 'constant')\n",
    "        # concatenate the 3 features vectors to have a matrix of 3 columns\n",
    "        # embeddings.append(np.vstack((char_features_vector, tag_features_vector, prior_features_vector)))\n",
    "        embeddings.append(np.concatenate((char_features_vector, tag_features_vector, prior_features_vector)))\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# print(char_features_vector)\n",
    "# print(tag_features_vector)\n",
    "# print(prior_features_vector)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embeddings in a pickle file\n",
    "with open('./generatedFiles/embeddings.pickle', 'wb') as file:\n",
    "    pickle.dump(embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the embeddings from the pickle file\n",
    "with open('./generatedFiles/embeddings.pickle', 'rb') as file:\n",
    "    embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_map = {\n",
    "    (1, 0, 0, 0, 0, 0, 0, 0) : 0, # FATHA\n",
    "    (0, 0, 0, 1, 0, 0, 0, 0) : 1, # FATHATAN\n",
    "    (0, 0, 1, 0, 0, 0, 0, 0) : 2, # KASRA\n",
    "    (0, 0, 0, 0, 0, 1, 0, 0) : 3, # KASRATAN\n",
    "    (0, 1, 0, 0, 0, 0, 0, 0) : 4, # DAMMA\n",
    "    (0, 0, 0, 0, 1, 0, 0, 0) : 5, # DAMMATAN\n",
    "    (0, 0, 0, 0, 0, 0, 1, 0) : 6, # SUKUN\n",
    "    (0, 0, 0, 0, 0, 0, 0, 1) : 7,  # SHADDA\n",
    "    (1, 0, 0, 0, 0, 0, 0, 1) : 8, # SHADDA FATHA\n",
    "    (0, 0, 0, 1, 0, 0, 0, 1) : 9, # SHADDA FATHATAN\n",
    "    (0, 0, 1, 0, 0, 0, 0, 1) : 10, # SHADDA KASRA\n",
    "    (0, 0, 0, 0, 0, 1, 0, 1) : 11, # SHADDA KASRATAN\n",
    "    (0, 1, 0, 0, 0, 0, 0, 1) : 12, # SHADDA DAMMA\n",
    "    (0, 0, 0, 0, 1, 0, 0, 1) : 13, # SHADDA DAMMATAN\n",
    "    (0, 0, 0, 0, 0, 0, 0, 0) : 14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_prior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./generatedFiles/gold_output_dict.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gold_output):\n\u001b[0;32m----> 4\u001b[0m         gold_diacritics \u001b[38;5;241m=\u001b[39m \u001b[43mget_prior\u001b[49m([tokenized_input[idx]], [word])\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m gold_diacritics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m             key \u001b[38;5;241m=\u001b[39m key \u001b[38;5;241m+\u001b[39m (idx,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_prior' is not defined"
     ]
    }
   ],
   "source": [
    "# gold labels\n",
    "with open('./generatedFiles/gold_output_dict.txt', 'w', encoding='utf-8') as file:\n",
    "    for idx, word in enumerate(gold_output):\n",
    "        gold_diacritics = get_prior([tokenized_input[idx]], [word])\n",
    "        for key, value in gold_diacritics.items():\n",
    "            key = key + (idx,)\n",
    "            file.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prior feature file in a dictionary called prior_feature \n",
    "gold_output_dict = {}\n",
    "with open('./generatedFiles/gold_output_dict.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(':')\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        key = key[1:-1].split(',')\n",
    "        value = value[1:-1].split(',')\n",
    "        key = (key[0][1:-1], key[1][2:-1], int(key[2]), int(key[3]))\n",
    "        value = [int(i) for i in value]\n",
    "        gold_output_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('قوله', 'ق', 0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgold_output_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mقوله\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mق\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('قوله', 'ق', 0, 0)"
     ]
    }
   ],
   "source": [
    "gold_output_dict[('قوله', 'ق', 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change gold_output_dict.values() to a list of tuples\n",
    "for key, value in gold_output_dict.items():\n",
    "    gold_output_dict[key] = tuple(value)\n",
    "    \n",
    "gold_output_dict_values = list(gold_output_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./generatedFiles/gold_output_id.txt', 'w', encoding='utf-8') as file:\n",
    "    for value in gold_output_dict_values:\n",
    "        file.write(f'{output_map[value]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the gold_output_id file\n",
    "with open('./generatedFiles/gold_output_id.txt', 'r', encoding='utf-8') as file:\n",
    "    gold_output_id = file.readlines()\n",
    "    gold_output_id = [line.strip() for line in gold_output_id]\n",
    "\n",
    "gold_output_id = np.array(gold_output_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8353805,)\n",
      "['0' '6' '4' '4' '0' '6' '0' '0' '0' '14']\n"
     ]
    }
   ],
   "source": [
    "print(gold_output_id.shape)\n",
    "print(gold_output_id[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8353805\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate emdeddings to have the 8353000\n",
    "embeddings_reshape = embeddings[:4500000]\n",
    "gold_output_id = gold_output_id[:4500000]\n",
    "\n",
    "# Make it np array \n",
    "embeddings_reshape = np.array(embeddings_reshape)\n",
    "gold_output_id = np.array(gold_output_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500000, 150)\n",
      "(4500000,)\n"
     ]
    }
   ],
   "source": [
    "print (embeddings_reshape.shape)\n",
    "print (gold_output_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape embeddings to have 3 dimensions \n",
    "embeddings_reshape = embeddings_reshape.reshape((-1, 1000, 150))\n",
    "gold_output_id_reshape = gold_output_id.reshape(-1, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 1000, 150)\n",
      "(4500, 1000, 1)\n",
      "[['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['8']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['12']\n",
      " ['14']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['3']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['3']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['4']\n",
      " ['2']\n",
      " ['6']\n",
      " ['14']\n",
      " ['14']\n",
      " ['10']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['1']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['3']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['3']\n",
      " ['0']\n",
      " ['0']\n",
      " ['10']\n",
      " ['4']\n",
      " ['8']\n",
      " ['14']\n",
      " ['3']\n",
      " ['14']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['3']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['6']\n",
      " ['3']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['12']\n",
      " ['8']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['8']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['12']\n",
      " ['4']\n",
      " ['14']\n",
      " ['2']\n",
      " ['2']\n",
      " ['14']\n",
      " ['8']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['3']\n",
      " ['4']\n",
      " ['0']\n",
      " ['8']\n",
      " ['5']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['3']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['8']\n",
      " ['14']\n",
      " ['14']\n",
      " ['8']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['5']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['8']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['5']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['10']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['10']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['8']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['8']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['3']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['1']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['8']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['5']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['8']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['8']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['14']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['8']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['2']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['14']\n",
      " ['12']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['4']\n",
      " ['6']\n",
      " ['5']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['3']\n",
      " ['4']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['5']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['8']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['8']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['1']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['4']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['1']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['8']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['5']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['3']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['5']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['6']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['6']\n",
      " ['2']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['4']\n",
      " ['2']\n",
      " ['10']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['2']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['3']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['14']\n",
      " ['4']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['4']\n",
      " ['2']\n",
      " ['14']\n",
      " ['4']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['2']\n",
      " ['0']\n",
      " ['8']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['4']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['14']\n",
      " ['5']\n",
      " ['2']\n",
      " ['14']\n",
      " ['0']\n",
      " ['10']\n",
      " ['14']\n",
      " ['6']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['10']\n",
      " ['0']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['14']\n",
      " ['14']\n",
      " ['0']\n",
      " ['8']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['4']\n",
      " ['4']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['0']\n",
      " ['2']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['12']\n",
      " ['0']\n",
      " ['14']\n",
      " ['14']\n",
      " ['12']\n",
      " ['14']\n",
      " ['0']\n",
      " ['14']\n",
      " ['2']\n",
      " ['12']\n",
      " ['0']\n",
      " ['6']\n",
      " ['2']\n",
      " ['14']\n",
      " ['1']\n",
      " ['14']\n",
      " ['0']\n",
      " ['0']\n",
      " ['14']\n",
      " ['0']\n",
      " ['2']\n",
      " ['0']\n",
      " ['4']\n",
      " ['0']\n",
      " ['10']\n",
      " ['4']\n",
      " ['4']\n",
      " ['14']\n",
      " ['6']\n",
      " ['0']\n",
      " ['6']\n",
      " ['0']\n",
      " ['0']\n",
      " ['2']]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Print Shap\n",
    "#e\n",
    "print(embeddings_reshape.shape)\n",
    "print(gold_output_id_reshape.shape)\n",
    "\n",
    "# print the first 10 rows of the embeddings\n",
    "# print(embeddings[:10])\n",
    "\n",
    "# print the first 10 rows of the gold_output_id\n",
    "print(gold_output_id_reshape[:][0])\n",
    "\n",
    "# print the first 10 columns of the gold_output_id\n",
    "print(tf.keras.utils.to_categorical(gold_output_id_reshape[:][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirection  (None, 1000, 100)         80400     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 1000, 100)         10100     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 1000, 15)          1515      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 92015 (359.43 KB)\n",
      "Trainable params: 92015 (359.43 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build a training model, first we need input layer that take matrix \"embeddings\" as an input with dropout of 10%\n",
    "# then we need a bidirectional LSTM layer with 100 units\n",
    "# then we need a dense layer with 100 units and relu activation function\n",
    "# then we need an output layer with 14 units and softmax activation function\n",
    "# use early stopping with patience of five epochs, a learning rate of 0.001, a batch size of 256, and an Adamax optimizer\n",
    "\n",
    "# define the model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.activations import relu,linear\n",
    "\n",
    "input_shape = (1000, 150)\n",
    "    # tf.keras.layers.Input(shape=150),\n",
    "    # Dropout(0.1),\n",
    "    # Bidirectional(LSTM(100)),\n",
    "    # Dense(100, activation='relu'),\n",
    "    # Dense(14, activation='softmax')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True), input_shape=input_shape))\n",
    "model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "model.add(TimeDistributed(Dense(15, activation='softmax')))\n",
    "\n",
    "# model = Sequential()\n",
    "# forward_layer = LSTM(50)\n",
    "# backward_layer = LSTM(50, activation='relu', go_backwards=True)\n",
    "# model.add(Bidirectional(forward_layer, backward_layer=backward_layer, input_shape=(1,150)))\n",
    "# # model.add(Dropout(0.1))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(14, activation='softmax'))\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.build(reshaped_matrix.shape)\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(gold_output_id_reshape)):\n",
    "    for j in range(1000):\n",
    "        labels.append( tf.keras.utils.to_categorical(gold_output_id_reshape[i][j], num_classes=15))\n",
    "        \n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 1000, 15)\n"
     ]
    }
   ],
   "source": [
    "labels = labels.reshape(-1, 1000, 15)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 25s 1s/step - loss: 2.2714 - accuracy: 0.3096\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8047 - accuracy: 0.3507\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.6972 - accuracy: 0.3507\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.6122 - accuracy: 0.3925\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.5010 - accuracy: 0.4886\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.3659 - accuracy: 0.6000\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.2242 - accuracy: 0.6574\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.0878 - accuracy: 0.7066\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.9597 - accuracy: 0.7376\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.8491 - accuracy: 0.7586\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.7577 - accuracy: 0.7759\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6817 - accuracy: 0.7934\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6184 - accuracy: 0.8070\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.5659 - accuracy: 0.8175\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.5224 - accuracy: 0.8280\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.4865 - accuracy: 0.8376\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.4568 - accuracy: 0.8451\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.4319 - accuracy: 0.8512\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.4108 - accuracy: 0.8562\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3932 - accuracy: 0.8606\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3780 - accuracy: 0.8636\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3648 - accuracy: 0.8664\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3535 - accuracy: 0.8689\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3435 - accuracy: 0.8717\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3349 - accuracy: 0.8741\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3275 - accuracy: 0.8764\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3207 - accuracy: 0.8789\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3146 - accuracy: 0.8810\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.3091 - accuracy: 0.8828\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.3042 - accuracy: 0.8844\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2994 - accuracy: 0.8859\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2952 - accuracy: 0.8873\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2912 - accuracy: 0.8886\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2875 - accuracy: 0.8898\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2839 - accuracy: 0.8911\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.2804 - accuracy: 0.8923\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2772 - accuracy: 0.8935\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2741 - accuracy: 0.8946\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.2713 - accuracy: 0.8957\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2685 - accuracy: 0.8967\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2658 - accuracy: 0.8978\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2633 - accuracy: 0.8988\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2609 - accuracy: 0.8997\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.2586 - accuracy: 0.9006\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.2563 - accuracy: 0.9015\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2541 - accuracy: 0.9023\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2523 - accuracy: 0.9030\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2502 - accuracy: 0.9038\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.2483 - accuracy: 0.9045\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 23s 1s/step - loss: 0.2465 - accuracy: 0.9052\n"
     ]
    }
   ],
   "source": [
    "# fit the model on the training dataset and evaluate it on the validation dataset,\n",
    "# use early stopping with patience of five epochs, a learning rate of 0.001, a batch size of 256\n",
    "# before that, configure the model to use GPU\n",
    "\n",
    "# fit the model with gpu\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(embeddings_reshape, labels, epochs=50, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model.save('./generatedFiles/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Specify the file path\n",
    "file_path = \"./dataset/val.txt\"\n",
    "\n",
    "# Read the contents of the file located at file_path \n",
    "# and append each line to the list data_before_preprocessing\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    val_data_before_preprocessing = file.readlines()\n",
    "    # remove '\\n' from each line\n",
    "    val_data_before_preprocessing = [line.strip() for line in val_data_before_preprocessing]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_data_before_preprocessing)):\n",
    "    save_tokenized_input(val_data_before_preprocessing[i],\"val_tokenized_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212368\n"
     ]
    }
   ],
   "source": [
    "# Read the tokenized input file\n",
    "# read the tokenized input file\n",
    "with open('./generatedFiles/val_tokenized_input.txt', 'r', encoding='utf-8') as file:\n",
    "    val_tokenized_input = file.readlines()\n",
    "    # Remove '\\n' from each line\n",
    "    val_tokenized_input = [line.strip() for line in val_tokenized_input]\n",
    "    \n",
    "print(len(val_tokenized_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_data_before_preprocessing)):\n",
    "    save_gold_output(val_data_before_preprocessing[i],\"val_gold_output\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the gold_output file\n",
    "with open('./generatedFiles/val_gold_output.txt', 'r', encoding='utf-8') as file:\n",
    "    val_gold_output = file.readlines()\n",
    "    # remove '\\n' from each line\n",
    "    val_gold_output = [line.strip() for line in val_gold_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_seg_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val_tokenized_input)):\n\u001b[0;32m----> 2\u001b[0m     val_segments, val_seg_tags \u001b[38;5;241m=\u001b[39m \u001b[43mget_seg_tags\u001b[49m(val_tokenized_input[i])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Write and append on the tokenized input to a file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./generatedFiles/val_input_segments.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_seg_tags' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_tokenized_input)):\n",
    "    val_segments, val_seg_tags = get_seg_tags(val_tokenized_input[i])\n",
    "    # Write and append on the tokenized input to a file\n",
    "    with open('./generatedFiles/val_input_segments.txt', 'a', encoding='utf-8') as file:\n",
    "        for tag in val_seg_tags:\n",
    "            file.write(tag)\n",
    "        file.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the val input_segments file\n",
    "with open('./generatedFiles/val_input_segments.txt', 'r', encoding='utf-8') as file:\n",
    "    val_input_segments = file.readlines()\n",
    "    val_input_segments = [line.strip() for line in val_input_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421224, 150)\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL TOOK 12 minutes TO RUN\n",
    "# input layer\n",
    "val_char_features_vector=[]\n",
    "val_tag_features_vector=[]\n",
    "val_prior_features_vector=[]\n",
    "val_embeddings = []\n",
    "# with open('./generatedFiles/i_j.txt', 'a', encoding='utf-8') as file:\n",
    "for i in range(len(val_tokenized_input)):\n",
    "    for j in range(len(val_tokenized_input[i])):    \n",
    "        # write i and j in a file\n",
    "        # file.write(f'{i} {j}\\n')\n",
    "        char_index = tokenizer_char.word_index.get(val_tokenized_input[i][j])\n",
    "        char_features_vector= char_embeddings[char_index]\n",
    "        if (len(val_tokenized_input[i]) != len(val_input_segments[i])):\n",
    "            val_input_segments[i] = \"S\" * (len(val_tokenized_input[i]) - len(val_input_segments[i])) + val_input_segments[i]\n",
    "        tag_index = tokenizer_tags.word_index.get(val_input_segments[i][j].lower())\n",
    "        tag_features_vector= tags_embeddings[tag_index]\n",
    "        prior_features_vector= (prior_feature[(val_tokenized_input[i], val_tokenized_input[i][j], j)]) if (val_tokenized_input[i], val_tokenized_input[i][j], j) in prior_feature else [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        # pad the prior feature vector with zeros to have the same length as the other features\n",
    "        prior_features_vector = np.pad(prior_features_vector, (0, 42), 'constant')\n",
    "        # concatenate the 3 features vectors to have a matrix of 3 columns\n",
    "        # embeddings.append(np.vstack((char_features_vector, tag_features_vector, prior_features_vector)))\n",
    "        val_embeddings.append(np.concatenate((char_features_vector, tag_features_vector, prior_features_vector)))\n",
    "\n",
    "val_embeddings = np.array(val_embeddings)\n",
    "\n",
    "# print(char_features_vector)\n",
    "# print(tag_features_vector)\n",
    "# print(prior_features_vector)\n",
    "print(val_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4210, 100, 150)\n"
     ]
    }
   ],
   "source": [
    "val_embeddings_reshape = val_embeddings[:421000]\n",
    "val_embeddings_reshape = val_embeddings_reshape.reshape((-1, 1000, 150))\n",
    "val_embeddings_reshape = np.array(val_embeddings_reshape)\n",
    "print(val_embeddings_reshape.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قَوْلُهُ', 'وَلَا', 'تُكْرَهُ', 'ضِيَافَتُهُ', 'الْفَرْقُ', 'الثَّالِثُ', 'وَالثَّلَاثُونَ', 'بَيْنَ', 'قَاعِدَةِ', 'تَقَدُّمِ']\n"
     ]
    }
   ],
   "source": [
    "print (val_gold_output[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold labels\n",
    "with open('./generatedFiles/val_gold_output_dict.txt', 'w', encoding='utf-8') as file:\n",
    "    for idx, word in enumerate(val_gold_output):\n",
    "        gold_diacritics = get_prior([val_tokenized_input[idx]], [word])\n",
    "        for key, value in gold_diacritics.items():\n",
    "            key = key + (idx,)\n",
    "            file.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prior feature file in a dictionary called prior_feature \n",
    "val_gold_output_dict = {}\n",
    "with open('./generatedFiles/val_gold_output_dict.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(':')\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        key = key[1:-1].split(',')\n",
    "        value = value[1:-1].split(',')\n",
    "        key = (key[0][1:-1], key[1][2:-1], int(key[2]), int(key[3]))\n",
    "        value = [int(i) for i in value]\n",
    "        val_gold_output_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change gold_output_dict.values() to a list of tuples\n",
    "for key, value in val_gold_output_dict.items():\n",
    "    val_gold_output_dict[key] = tuple(value)\n",
    "    \n",
    "val_gold_output_dict_values = list(val_gold_output_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./generatedFiles/val_gold_output_id.txt', 'w', encoding='utf-8') as file:\n",
    "    for value in val_gold_output_dict_values:\n",
    "        file.write(f'{output_map[value]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the gold_output_id file\n",
    "with open('./generatedFiles/val_gold_output_id.txt', 'r', encoding='utf-8') as file:\n",
    "    val_gold_output_id = file.readlines()\n",
    "    val_gold_output_id = [line.strip() for line in val_gold_output_id]\n",
    "\n",
    "val_gold_output_id = np.array(val_gold_output_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gold_output_id = val_gold_output_id[:421000]\n",
    "val_gold_output_id = val_gold_output_id.reshape(-1, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4210, 100, 15)\n"
     ]
    }
   ],
   "source": [
    "val_labels = []\n",
    "for i in range(len(val_gold_output_id)):\n",
    "    for j in range(1000):\n",
    "        val_labels.append( tf.keras.utils.to_categorical(val_gold_output_id[i][j], num_classes=15))\n",
    "        \n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "val_labels = val_labels.reshape(-1, 1000, 15)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.312587\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the validation dataset\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "loss, accuracy = model.evaluate(val_embeddings_reshape, val_labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
